{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarOvanger/GeoDecepticon/blob/main/training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhDYtsPDKH1Y"
      },
      "source": [
        "# This is where the training goes down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqn3Jq7vKH1Z"
      },
      "source": [
        " We start of by installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "hvzMW1GvKONI",
        "outputId": "28b05543-4da3-4737-aab9-e2f9f36904af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OscarOvanger/GeoDecepticon.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sSxYr07NGmM",
        "outputId": "1cf3368b-8d0c-4614-f511-9346ac6c6ed6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GeoDecepticon'...\n",
            "remote: Enumerating objects: 296, done.\u001b[K\n",
            "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 296 (delta 115), reused 42 (delta 36), pack-reused 100 (from 1)\u001b[K\n",
            "Receiving objects: 100% (296/296), 41.38 MiB | 20.45 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/GeoDecepticon')"
      ],
      "metadata": {
        "id": "C5PNiWOrNTCN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataloader import BinaryImageDataset, preprocess_image\n",
        "from transformer import VisionTransformer\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sample import reconstruct_image_from_patches\n",
        "import matplotlib.colors as mcolors\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from matplotlib import colors\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "uDxei4TeKtU6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1 = np.load(\"data_array_1.npz\")\n",
        "data_2 = np.load(\"data_array_2.npz\")\n",
        "data_3 = np.load(\"data_array_3.npz\")\n",
        "\n",
        "data_array_1 = data_1['data_array']\n",
        "data_array_2 = data_2['data_array']\n",
        "data_array_3 = data_3['data_array']\n",
        "\n",
        "training_data = np.concatenate((data_array_1, data_array_2), axis=0)\n",
        "test_data = data_array_3\n",
        "print(\"training data shape: \", training_data.shape)\n",
        "print(\"test data shape: \", test_data.shape)\n",
        "\n",
        "plt.imshow(training_data[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "q1X8GW0A1vJ2",
        "outputId": "d08eb6d8-c799-4070-dd63-4167d0ddc672"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data shape:  (6000, 64, 64)\n",
            "test data shape:  (3423, 64, 64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding matrix for all 2x2 binary combinations + 1 mask token\n",
        "embedding_matrix = torch.zeros((17, 4))  # Shape: (num_tokens, embed_dim)\n",
        "\n",
        "# Generate all possible 2x2 binary patches\n",
        "patches = torch.tensor([\n",
        "    [a, b, c, d]\n",
        "    for a in range(2)\n",
        "    for b in range(2)\n",
        "    for c in range(2)\n",
        "    for d in range(2)\n",
        "])  # Shape: (16, 4) for 16 combinations of 2x2 patches\n",
        "\n",
        "# Assign each patch's values as its embedding\n",
        "for i, patch in enumerate(patches):\n",
        "    embedding_matrix[i, :] = patch  # Set the embedding to the patch values\n",
        "\n",
        "# Set the last row to all 2s for the masked patch\n",
        "embedding_matrix[-1, :] = 0.5  # Mask token embedding\n",
        "\n",
        "\n",
        "print(\"Embedding Matrix:\\n\", embedding_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVWktkS5DH1S",
        "outputId": "80543043-d465-4040-e29e-f5084cf20280"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Matrix:\n",
            " tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 1.0000, 1.0000],\n",
            "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0000, 0.0000, 1.0000],\n",
            "        [0.0000, 1.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [1.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 1.0000, 0.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 0.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model parameters\n",
        "# Convert to tensors, should be type Long\n",
        "training_images = torch.tensor(training_data, dtype=torch.long)\n",
        "test_images = torch.tensor(test_data, dtype=torch.long)\n",
        "\n",
        "# Parameters\n",
        "batch_size = 60\n",
        "embed_dim = 4\n",
        "num_heads = 2\n",
        "feedforward_dim = 128\n",
        "num_layers = 2\n",
        "num_tokens = 17  # 16 tokens + 1 mask token\n",
        "max_patches = 32 * 32\n",
        "dropout = 0.2\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 1000\n",
        "hidden_dim = 64\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = BinaryImageDataset(training_images)  # Assumes training_images is already loaded\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model\n",
        "model = VisionTransformer(embed_dim, num_heads, feedforward_dim, num_layers, num_tokens, max_patches, dropout, hidden_dim).to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def prob_log_scheduler(x,max_patches,min_patches,num_epochs,rand_size):\n",
        "    value = max_patches*np.log((((np.exp(1)-1)*x)/num_epochs) + 1) + min_patches\n",
        "    i = 0\n",
        "    for val in value:\n",
        "      rand_comp = np.random.randint(-rand_size,rand_size)*(val/num_epochs)\n",
        "      #print(\"random value:\", rand_comp)\n",
        "      val = int(val + rand_comp)\n",
        "      if val > max_patches:\n",
        "          val = max_patches\n",
        "      elif val < min_patches:\n",
        "          val = min_patches\n",
        "      value[i] = val\n",
        "      i += 1\n",
        "    return value"
      ],
      "metadata": {
        "id": "C5CIDqPjAXWh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(1,num_epochs,num_epochs)\n",
        "max_masking = int(max_patches*0.9)\n",
        "min_patches = int(max_patches*0.01)\n",
        "rand_size = 20\n",
        "y = prob_log_scheduler(x,max_masking,min_patches,num_epochs,rand_size)\n",
        "plt.plot(x,y)\n",
        "plt.title(\"Masking schedule\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Number of masked patches\")\n",
        "plt.show()\n",
        "print(y[0])\n",
        "print(y[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "vE7Hqpmkzj2M",
        "outputId": "8fab9ddb-86da-48f2-9240-2edc15100011"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.0\n",
            "921.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# Initialize wandb project\n",
        "wandb.init(\n",
        "    project=\"vision-transformer\",\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"num_heads\": num_heads,\n",
        "        \"feedforward_dim\": feedforward_dim,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"num_tokens\": num_tokens,\n",
        "        \"max_patches\": max_patches,\n",
        "        \"dropout\": dropout,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"hidden_dim\": hidden_dim,\n",
        "    },\n",
        ")\n",
        "\n",
        "reconstructed_images = []\n",
        "original_images = []\n",
        "# Directory to save checkpoints\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "maskings = y\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    current_num_mask = int(maskings[epoch])\n",
        "    for batch_idx, images in enumerate(dataloader):\n",
        "        # Preprocess images\n",
        "        patch_indices = torch.stack([preprocess_image(img) for img in images]).long() #(btach_size,max_patches)\n",
        "        masked_patches = patch_indices.clone() #(btach_size,max_patches)\n",
        "\n",
        "        # Generate random indices for masking\n",
        "        random_indices = torch.rand(batch_size, max_patches).argsort(dim=1)[:, :current_num_mask]\n",
        "        # Create the mask tensor\n",
        "        mask = torch.zeros((batch_size, max_patches), dtype=torch.bool)\n",
        "        mask.scatter_(1, random_indices, True)\n",
        "\n",
        "        #Mask\n",
        "        masked_patches[mask] = num_tokens - 1  # Replace masked patches with the mask token\n",
        "\n",
        "        # Define weighted CrossEntropyLoss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Move to device\n",
        "        masked_patches, patch_indices, mask = (\n",
        "            masked_patches.to(device),\n",
        "            patch_indices.to(device),\n",
        "            mask.to(device),\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(masked_patches) # (batch_size,max_patches,num_tokens - 1)\n",
        "        # Isolate the masked patches\n",
        "        # Get the indices of True values\n",
        "        indices = mask.nonzero(as_tuple=True)  # Returns indices where mask is True\n",
        "        # Extract logits at masked positions while maintaining row-wise structure\n",
        "        masked_logits = logits[indices[0], indices[1]]  # Extracts all True-index logits\n",
        "        masked_patch_indices = patch_indices[indices[0], indices[1]]  # Extracts all True-index patch indices\n",
        "        # Reshape to [batch_size, num_masked_patches, num_classes]\n",
        "        masked_logits = masked_logits.view(batch_size, current_num_mask, -1)  # (batch_size, current_num_mask, num_tokens-1)\n",
        "        masked_patch_indices = masked_patch_indices.view(batch_size, current_num_mask) #(batch_size, current_num_mask)\n",
        "\n",
        "        #Compute logits\n",
        "        loss = criterion(masked_logits.view(-1, num_tokens-1), masked_patch_indices.view(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Log batch metrics\n",
        "        wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "        # Visualization for the first batch in the epoch\n",
        "        if batch_idx == 0:\n",
        "          with torch.no_grad():\n",
        "              # Initialize predicted_indices with original patch indices\n",
        "              predicted_indices = patch_indices.cpu()[0].clone()\n",
        "\n",
        "              # Predict only the masked indices\n",
        "              masked_predictions = torch.argmax(masked_logits, dim=-1).cpu()  # Shape: [batch_size, current_num_mask]\n",
        "              predicted_indices[mask[0]] = masked_predictions[0]  # Update only masked indices\n",
        "\n",
        "              # Reconstruct images\n",
        "              reconstructed_image = reconstruct_image_from_patches(predicted_indices)\n",
        "              reconstructed_images.append(reconstructed_image)\n",
        "              original_images.append(reconstruct_image_from_patches(patch_indices.cpu()[0]))\n",
        "\n",
        "\n",
        "              # Prepare masked image for visualization\n",
        "              visualized_masked_patches = masked_patches.cpu()[0].clone()\n",
        "              visualized_masked_patches[visualized_masked_patches == num_tokens - 1] = -1\n",
        "              masked_image = reconstruct_image_from_patches(visualized_masked_patches)\n",
        "\n",
        "              # Log visualizations to wandb\n",
        "              wandb.log({\n",
        "                  \"Original Image\": wandb.Image(\n",
        "                      reconstruct_image_from_patches(patch_indices.cpu()[0])\n",
        "                  ),\n",
        "                  \"Masked Image\": wandb.Image(masked_image, caption=\"Masked Image\"),\n",
        "                  \"Reconstructed Image\": wandb.Image(\n",
        "                      reconstructed_image, caption=\"Reconstructed Image\"\n",
        "                  ),\n",
        "              })\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Log epoch metrics\n",
        "    wandb.log({\"epoch_loss\": avg_loss, \"masked_patches\": current_num_mask})\n",
        "\n",
        "    # Save checkpoint periodically\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        checkpoint_path = f\"checkpoints/vision_transformer_epoch_{epoch+1}.pth\"\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": avg_loss,\n",
        "            },\n",
        "            checkpoint_path,\n",
        "        )\n",
        "        print(f\"Checkpoint saved for epoch {epoch+1}\")\n",
        "        wandb.save(checkpoint_path)\n",
        "\n",
        "# Save the final model\n",
        "torch.save(model.state_dict(), \"vision_transformer_final_balanced.pth\")\n",
        "wandb.save(\"vision_transformer_final_balanced.pth\")\n",
        "print(\"Final model saved as 'vision_transformer_final_balanced.pth'.\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EG-Wyvra6908",
        "outputId": "bdc246f5-3235-47fb-9984-76d2313a5bea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/GeoDecepticon/wandb/run-20241206_135607-nsyxjjyn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oscars/vision-transformer/runs/nsyxjjyn' target=\"_blank\">solar-shadow-56</a></strong> to <a href='https://wandb.ai/oscars/vision-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/oscars/vision-transformer' target=\"_blank\">https://wandb.ai/oscars/vision-transformer</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/oscars/vision-transformer/runs/nsyxjjyn' target=\"_blank\">https://wandb.ai/oscars/vision-transformer/runs/nsyxjjyn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/GeoDecepticon/dataloader.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(image, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000] completed. Average Loss: 0.3974\n",
            "Epoch [2/1000] completed. Average Loss: 0.3454\n",
            "Epoch [3/1000] completed. Average Loss: 0.3318\n",
            "Epoch [4/1000] completed. Average Loss: 0.3230\n",
            "Epoch [5/1000] completed. Average Loss: 0.3167\n",
            "Epoch [6/1000] completed. Average Loss: 0.3096\n",
            "Epoch [7/1000] completed. Average Loss: 0.3056\n",
            "Epoch [8/1000] completed. Average Loss: 0.2996\n",
            "Epoch [9/1000] completed. Average Loss: 0.2959\n",
            "Epoch [10/1000] completed. Average Loss: 0.2945\n",
            "Epoch [11/1000] completed. Average Loss: 0.2946\n",
            "Epoch [12/1000] completed. Average Loss: 0.2903\n",
            "Epoch [13/1000] completed. Average Loss: 0.2899\n",
            "Epoch [14/1000] completed. Average Loss: 0.2881\n",
            "Epoch [15/1000] completed. Average Loss: 0.2883\n",
            "Epoch [16/1000] completed. Average Loss: 0.2854\n",
            "Epoch [17/1000] completed. Average Loss: 0.2830\n",
            "Epoch [18/1000] completed. Average Loss: 0.2826\n",
            "Epoch [19/1000] completed. Average Loss: 0.2842\n",
            "Epoch [20/1000] completed. Average Loss: 0.2837\n",
            "Epoch [21/1000] completed. Average Loss: 0.2814\n",
            "Epoch [22/1000] completed. Average Loss: 0.2765\n",
            "Epoch [23/1000] completed. Average Loss: 0.2786\n",
            "Epoch [24/1000] completed. Average Loss: 0.2766\n",
            "Epoch [25/1000] completed. Average Loss: 0.2770\n",
            "Epoch [26/1000] completed. Average Loss: 0.2761\n",
            "Epoch [27/1000] completed. Average Loss: 0.2765\n",
            "Epoch [28/1000] completed. Average Loss: 0.2770\n",
            "Epoch [29/1000] completed. Average Loss: 0.2767\n",
            "Epoch [30/1000] completed. Average Loss: 0.2736\n",
            "Epoch [31/1000] completed. Average Loss: 0.2775\n",
            "Epoch [32/1000] completed. Average Loss: 0.2733\n",
            "Epoch [33/1000] completed. Average Loss: 0.2729\n",
            "Epoch [34/1000] completed. Average Loss: 0.2739\n",
            "Epoch [35/1000] completed. Average Loss: 0.2729\n",
            "Epoch [36/1000] completed. Average Loss: 0.2738\n",
            "Epoch [37/1000] completed. Average Loss: 0.2727\n",
            "Epoch [38/1000] completed. Average Loss: 0.2738\n",
            "Epoch [39/1000] completed. Average Loss: 0.2744\n",
            "Epoch [40/1000] completed. Average Loss: 0.2732\n",
            "Epoch [41/1000] completed. Average Loss: 0.2703\n",
            "Epoch [42/1000] completed. Average Loss: 0.2742\n",
            "Epoch [43/1000] completed. Average Loss: 0.2733\n",
            "Epoch [44/1000] completed. Average Loss: 0.2741\n",
            "Epoch [45/1000] completed. Average Loss: 0.2722\n",
            "Epoch [46/1000] completed. Average Loss: 0.2686\n",
            "Epoch [47/1000] completed. Average Loss: 0.2728\n",
            "Epoch [48/1000] completed. Average Loss: 0.2730\n",
            "Epoch [49/1000] completed. Average Loss: 0.2725\n",
            "Epoch [50/1000] completed. Average Loss: 0.2726\n",
            "Checkpoint saved for epoch 50\n",
            "Epoch [51/1000] completed. Average Loss: 0.2732\n",
            "Epoch [52/1000] completed. Average Loss: 0.2731\n",
            "Epoch [53/1000] completed. Average Loss: 0.2716\n",
            "Epoch [54/1000] completed. Average Loss: 0.2723\n",
            "Epoch [55/1000] completed. Average Loss: 0.2717\n",
            "Epoch [56/1000] completed. Average Loss: 0.2713\n",
            "Epoch [57/1000] completed. Average Loss: 0.2724\n",
            "Epoch [58/1000] completed. Average Loss: 0.2709\n",
            "Epoch [59/1000] completed. Average Loss: 0.2743\n",
            "Epoch [60/1000] completed. Average Loss: 0.2722\n",
            "Epoch [61/1000] completed. Average Loss: 0.2713\n",
            "Epoch [62/1000] completed. Average Loss: 0.2723\n",
            "Epoch [63/1000] completed. Average Loss: 0.2725\n",
            "Epoch [64/1000] completed. Average Loss: 0.2726\n",
            "Epoch [65/1000] completed. Average Loss: 0.2719\n",
            "Epoch [66/1000] completed. Average Loss: 0.2714\n",
            "Epoch [67/1000] completed. Average Loss: 0.2727\n",
            "Epoch [68/1000] completed. Average Loss: 0.2711\n",
            "Epoch [69/1000] completed. Average Loss: 0.2715\n",
            "Epoch [70/1000] completed. Average Loss: 0.2700\n",
            "Epoch [71/1000] completed. Average Loss: 0.2708\n",
            "Epoch [72/1000] completed. Average Loss: 0.2727\n",
            "Epoch [73/1000] completed. Average Loss: 0.2733\n",
            "Epoch [74/1000] completed. Average Loss: 0.2718\n",
            "Epoch [75/1000] completed. Average Loss: 0.2721\n",
            "Epoch [76/1000] completed. Average Loss: 0.2715\n",
            "Epoch [77/1000] completed. Average Loss: 0.2715\n",
            "Epoch [78/1000] completed. Average Loss: 0.2718\n",
            "Epoch [79/1000] completed. Average Loss: 0.2721\n",
            "Epoch [80/1000] completed. Average Loss: 0.2731\n",
            "Epoch [81/1000] completed. Average Loss: 0.2722\n",
            "Epoch [82/1000] completed. Average Loss: 0.2726\n",
            "Epoch [83/1000] completed. Average Loss: 0.2713\n",
            "Epoch [84/1000] completed. Average Loss: 0.2723\n",
            "Epoch [85/1000] completed. Average Loss: 0.2736\n",
            "Epoch [86/1000] completed. Average Loss: 0.2722\n",
            "Epoch [87/1000] completed. Average Loss: 0.2718\n",
            "Epoch [88/1000] completed. Average Loss: 0.2720\n",
            "Epoch [89/1000] completed. Average Loss: 0.2722\n",
            "Epoch [90/1000] completed. Average Loss: 0.2729\n",
            "Epoch [91/1000] completed. Average Loss: 0.2727\n",
            "Epoch [92/1000] completed. Average Loss: 0.2726\n",
            "Epoch [93/1000] completed. Average Loss: 0.2734\n",
            "Epoch [94/1000] completed. Average Loss: 0.2705\n",
            "Epoch [95/1000] completed. Average Loss: 0.2730\n",
            "Epoch [96/1000] completed. Average Loss: 0.2730\n",
            "Epoch [97/1000] completed. Average Loss: 0.2730\n",
            "Epoch [98/1000] completed. Average Loss: 0.2720\n",
            "Epoch [99/1000] completed. Average Loss: 0.2727\n",
            "Epoch [100/1000] completed. Average Loss: 0.2746\n",
            "Checkpoint saved for epoch 100\n",
            "Epoch [101/1000] completed. Average Loss: 0.2737\n",
            "Epoch [102/1000] completed. Average Loss: 0.2728\n",
            "Epoch [103/1000] completed. Average Loss: 0.2730\n",
            "Epoch [104/1000] completed. Average Loss: 0.2745\n",
            "Epoch [105/1000] completed. Average Loss: 0.2745\n",
            "Epoch [106/1000] completed. Average Loss: 0.2736\n",
            "Epoch [107/1000] completed. Average Loss: 0.2741\n",
            "Epoch [108/1000] completed. Average Loss: 0.2733\n",
            "Epoch [109/1000] completed. Average Loss: 0.2729\n",
            "Epoch [110/1000] completed. Average Loss: 0.2731\n",
            "Epoch [111/1000] completed. Average Loss: 0.2746\n",
            "Epoch [112/1000] completed. Average Loss: 0.2747\n",
            "Epoch [113/1000] completed. Average Loss: 0.2763\n",
            "Epoch [114/1000] completed. Average Loss: 0.2735\n",
            "Epoch [115/1000] completed. Average Loss: 0.2737\n",
            "Epoch [116/1000] completed. Average Loss: 0.2734\n",
            "Epoch [117/1000] completed. Average Loss: 0.2754\n",
            "Epoch [118/1000] completed. Average Loss: 0.2758\n",
            "Epoch [119/1000] completed. Average Loss: 0.2758\n",
            "Epoch [120/1000] completed. Average Loss: 0.2754\n",
            "Epoch [121/1000] completed. Average Loss: 0.2755\n",
            "Epoch [122/1000] completed. Average Loss: 0.2755\n",
            "Epoch [123/1000] completed. Average Loss: 0.2750\n",
            "Epoch [124/1000] completed. Average Loss: 0.2753\n",
            "Epoch [125/1000] completed. Average Loss: 0.2756\n",
            "Epoch [126/1000] completed. Average Loss: 0.2755\n",
            "Epoch [127/1000] completed. Average Loss: 0.2766\n",
            "Epoch [128/1000] completed. Average Loss: 0.2757\n",
            "Epoch [129/1000] completed. Average Loss: 0.2760\n",
            "Epoch [130/1000] completed. Average Loss: 0.2765\n",
            "Epoch [131/1000] completed. Average Loss: 0.2773\n",
            "Epoch [132/1000] completed. Average Loss: 0.2754\n",
            "Epoch [133/1000] completed. Average Loss: 0.2765\n",
            "Epoch [134/1000] completed. Average Loss: 0.2766\n",
            "Epoch [135/1000] completed. Average Loss: 0.2757\n",
            "Epoch [136/1000] completed. Average Loss: 0.2761\n",
            "Epoch [137/1000] completed. Average Loss: 0.2750\n",
            "Epoch [138/1000] completed. Average Loss: 0.2764\n",
            "Epoch [139/1000] completed. Average Loss: 0.2785\n",
            "Epoch [140/1000] completed. Average Loss: 0.2765\n",
            "Epoch [141/1000] completed. Average Loss: 0.2776\n",
            "Epoch [142/1000] completed. Average Loss: 0.2768\n",
            "Epoch [143/1000] completed. Average Loss: 0.2790\n",
            "Epoch [144/1000] completed. Average Loss: 0.2778\n",
            "Epoch [145/1000] completed. Average Loss: 0.2758\n",
            "Epoch [146/1000] completed. Average Loss: 0.2777\n",
            "Epoch [147/1000] completed. Average Loss: 0.2778\n",
            "Epoch [148/1000] completed. Average Loss: 0.2777\n",
            "Epoch [149/1000] completed. Average Loss: 0.2784\n",
            "Epoch [150/1000] completed. Average Loss: 0.2787\n",
            "Checkpoint saved for epoch 150\n",
            "Epoch [151/1000] completed. Average Loss: 0.2788\n",
            "Epoch [152/1000] completed. Average Loss: 0.2788\n",
            "Epoch [153/1000] completed. Average Loss: 0.2779\n",
            "Epoch [154/1000] completed. Average Loss: 0.2790\n",
            "Epoch [155/1000] completed. Average Loss: 0.2783\n",
            "Epoch [156/1000] completed. Average Loss: 0.2792\n",
            "Epoch [157/1000] completed. Average Loss: 0.2789\n",
            "Epoch [158/1000] completed. Average Loss: 0.2800\n",
            "Epoch [159/1000] completed. Average Loss: 0.2776\n",
            "Epoch [160/1000] completed. Average Loss: 0.2803\n",
            "Epoch [161/1000] completed. Average Loss: 0.2792\n",
            "Epoch [162/1000] completed. Average Loss: 0.2795\n",
            "Epoch [163/1000] completed. Average Loss: 0.2808\n",
            "Epoch [164/1000] completed. Average Loss: 0.2814\n",
            "Epoch [165/1000] completed. Average Loss: 0.2810\n",
            "Epoch [166/1000] completed. Average Loss: 0.2806\n",
            "Epoch [167/1000] completed. Average Loss: 0.2805\n",
            "Epoch [168/1000] completed. Average Loss: 0.2809\n",
            "Epoch [169/1000] completed. Average Loss: 0.2812\n",
            "Epoch [170/1000] completed. Average Loss: 0.2795\n",
            "Epoch [171/1000] completed. Average Loss: 0.2800\n",
            "Epoch [172/1000] completed. Average Loss: 0.2817\n",
            "Epoch [173/1000] completed. Average Loss: 0.2809\n",
            "Epoch [174/1000] completed. Average Loss: 0.2806\n",
            "Epoch [175/1000] completed. Average Loss: 0.2802\n",
            "Epoch [176/1000] completed. Average Loss: 0.2818\n",
            "Epoch [177/1000] completed. Average Loss: 0.2810\n",
            "Epoch [178/1000] completed. Average Loss: 0.2813\n",
            "Epoch [179/1000] completed. Average Loss: 0.2832\n",
            "Epoch [180/1000] completed. Average Loss: 0.2814\n",
            "Epoch [181/1000] completed. Average Loss: 0.2816\n",
            "Epoch [182/1000] completed. Average Loss: 0.2818\n",
            "Epoch [183/1000] completed. Average Loss: 0.2825\n",
            "Epoch [184/1000] completed. Average Loss: 0.2812\n",
            "Epoch [185/1000] completed. Average Loss: 0.2836\n",
            "Epoch [186/1000] completed. Average Loss: 0.2838\n",
            "Epoch [187/1000] completed. Average Loss: 0.2818\n",
            "Epoch [188/1000] completed. Average Loss: 0.2816\n",
            "Epoch [189/1000] completed. Average Loss: 0.2842\n",
            "Epoch [190/1000] completed. Average Loss: 0.2843\n",
            "Epoch [191/1000] completed. Average Loss: 0.2834\n",
            "Epoch [192/1000] completed. Average Loss: 0.2835\n",
            "Epoch [193/1000] completed. Average Loss: 0.2832\n",
            "Epoch [194/1000] completed. Average Loss: 0.2844\n",
            "Epoch [196/1000] completed. Average Loss: 0.2831\n",
            "Epoch [197/1000] completed. Average Loss: 0.2851\n",
            "Epoch [198/1000] completed. Average Loss: 0.2840\n",
            "Epoch [199/1000] completed. Average Loss: 0.2851\n",
            "Epoch [200/1000] completed. Average Loss: 0.2861\n",
            "Checkpoint saved for epoch 200\n",
            "Epoch [201/1000] completed. Average Loss: 0.2854\n",
            "Epoch [202/1000] completed. Average Loss: 0.2850\n",
            "Epoch [203/1000] completed. Average Loss: 0.2845\n",
            "Epoch [204/1000] completed. Average Loss: 0.2845\n",
            "Epoch [205/1000] completed. Average Loss: 0.2857\n",
            "Epoch [206/1000] completed. Average Loss: 0.2853\n",
            "Epoch [207/1000] completed. Average Loss: 0.2859\n",
            "Epoch [208/1000] completed. Average Loss: 0.2858\n",
            "Epoch [209/1000] completed. Average Loss: 0.2863\n",
            "Epoch [210/1000] completed. Average Loss: 0.2864\n",
            "Epoch [211/1000] completed. Average Loss: 0.2880\n",
            "Epoch [212/1000] completed. Average Loss: 0.2868\n",
            "Epoch [213/1000] completed. Average Loss: 0.2874\n",
            "Epoch [214/1000] completed. Average Loss: 0.2867\n",
            "Epoch [215/1000] completed. Average Loss: 0.2877\n",
            "Epoch [216/1000] completed. Average Loss: 0.2867\n",
            "Epoch [217/1000] completed. Average Loss: 0.2864\n",
            "Epoch [218/1000] completed. Average Loss: 0.2881\n",
            "Epoch [219/1000] completed. Average Loss: 0.2859\n",
            "Epoch [220/1000] completed. Average Loss: 0.2879\n",
            "Epoch [221/1000] completed. Average Loss: 0.2869\n",
            "Epoch [222/1000] completed. Average Loss: 0.2862\n",
            "Epoch [223/1000] completed. Average Loss: 0.2883\n",
            "Epoch [224/1000] completed. Average Loss: 0.2875\n",
            "Epoch [225/1000] completed. Average Loss: 0.2888\n",
            "Epoch [226/1000] completed. Average Loss: 0.2883\n",
            "Epoch [227/1000] completed. Average Loss: 0.2896\n",
            "Epoch [228/1000] completed. Average Loss: 0.2895\n",
            "Epoch [229/1000] completed. Average Loss: 0.2884\n",
            "Epoch [230/1000] completed. Average Loss: 0.2902\n",
            "Epoch [231/1000] completed. Average Loss: 0.2889\n",
            "Epoch [232/1000] completed. Average Loss: 0.2893\n",
            "Epoch [233/1000] completed. Average Loss: 0.2895\n",
            "Epoch [234/1000] completed. Average Loss: 0.2887\n",
            "Epoch [235/1000] completed. Average Loss: 0.2894\n",
            "Epoch [236/1000] completed. Average Loss: 0.2894\n",
            "Epoch [237/1000] completed. Average Loss: 0.2885\n",
            "Epoch [238/1000] completed. Average Loss: 0.2907\n",
            "Epoch [239/1000] completed. Average Loss: 0.2915\n",
            "Epoch [240/1000] completed. Average Loss: 0.2891\n",
            "Epoch [241/1000] completed. Average Loss: 0.2899\n",
            "Epoch [242/1000] completed. Average Loss: 0.2905\n",
            "Epoch [243/1000] completed. Average Loss: 0.2910\n",
            "Epoch [244/1000] completed. Average Loss: 0.2920\n",
            "Epoch [245/1000] completed. Average Loss: 0.2920\n",
            "Epoch [246/1000] completed. Average Loss: 0.2920\n",
            "Epoch [247/1000] completed. Average Loss: 0.2922\n",
            "Epoch [248/1000] completed. Average Loss: 0.2918\n",
            "Epoch [249/1000] completed. Average Loss: 0.2911\n",
            "Epoch [250/1000] completed. Average Loss: 0.2916\n",
            "Checkpoint saved for epoch 250\n",
            "Epoch [251/1000] completed. Average Loss: 0.2926\n",
            "Epoch [252/1000] completed. Average Loss: 0.2925\n",
            "Epoch [253/1000] completed. Average Loss: 0.2917\n",
            "Epoch [254/1000] completed. Average Loss: 0.2912\n",
            "Epoch [255/1000] completed. Average Loss: 0.2925\n",
            "Epoch [256/1000] completed. Average Loss: 0.2937\n",
            "Epoch [257/1000] completed. Average Loss: 0.2930\n",
            "Epoch [258/1000] completed. Average Loss: 0.2947\n",
            "Epoch [259/1000] completed. Average Loss: 0.2934\n",
            "Epoch [260/1000] completed. Average Loss: 0.2940\n",
            "Epoch [261/1000] completed. Average Loss: 0.2940\n",
            "Epoch [262/1000] completed. Average Loss: 0.2923\n",
            "Epoch [263/1000] completed. Average Loss: 0.2918\n",
            "Epoch [264/1000] completed. Average Loss: 0.2947\n",
            "Epoch [265/1000] completed. Average Loss: 0.2924\n",
            "Epoch [266/1000] completed. Average Loss: 0.2942\n",
            "Epoch [267/1000] completed. Average Loss: 0.2948\n",
            "Epoch [268/1000] completed. Average Loss: 0.2932\n",
            "Epoch [269/1000] completed. Average Loss: 0.2954\n",
            "Epoch [270/1000] completed. Average Loss: 0.2957\n",
            "Epoch [271/1000] completed. Average Loss: 0.2950\n",
            "Epoch [272/1000] completed. Average Loss: 0.2945\n",
            "Epoch [273/1000] completed. Average Loss: 0.2965\n",
            "Epoch [274/1000] completed. Average Loss: 0.2959\n",
            "Epoch [275/1000] completed. Average Loss: 0.2957\n",
            "Epoch [276/1000] completed. Average Loss: 0.2946\n",
            "Epoch [277/1000] completed. Average Loss: 0.2956\n",
            "Epoch [278/1000] completed. Average Loss: 0.2968\n",
            "Epoch [279/1000] completed. Average Loss: 0.2957\n",
            "Epoch [280/1000] completed. Average Loss: 0.2958\n",
            "Epoch [281/1000] completed. Average Loss: 0.2959\n",
            "Epoch [282/1000] completed. Average Loss: 0.2968\n",
            "Epoch [283/1000] completed. Average Loss: 0.2966\n",
            "Epoch [284/1000] completed. Average Loss: 0.2975\n",
            "Epoch [285/1000] completed. Average Loss: 0.2960\n",
            "Epoch [286/1000] completed. Average Loss: 0.2977\n",
            "Epoch [287/1000] completed. Average Loss: 0.2979\n",
            "Epoch [288/1000] completed. Average Loss: 0.2968\n",
            "Epoch [289/1000] completed. Average Loss: 0.2978\n",
            "Epoch [290/1000] completed. Average Loss: 0.2970\n",
            "Epoch [291/1000] completed. Average Loss: 0.2992\n",
            "Epoch [292/1000] completed. Average Loss: 0.2982\n",
            "Epoch [293/1000] completed. Average Loss: 0.2986\n",
            "Epoch [294/1000] completed. Average Loss: 0.2987\n",
            "Epoch [295/1000] completed. Average Loss: 0.2997\n",
            "Epoch [296/1000] completed. Average Loss: 0.2995\n",
            "Epoch [297/1000] completed. Average Loss: 0.3007\n",
            "Epoch [298/1000] completed. Average Loss: 0.2996\n",
            "Epoch [299/1000] completed. Average Loss: 0.3005\n",
            "Epoch [300/1000] completed. Average Loss: 0.2993\n",
            "Checkpoint saved for epoch 300\n",
            "Epoch [301/1000] completed. Average Loss: 0.3005\n",
            "Epoch [302/1000] completed. Average Loss: 0.3009\n",
            "Epoch [303/1000] completed. Average Loss: 0.3014\n",
            "Epoch [304/1000] completed. Average Loss: 0.3009\n",
            "Epoch [305/1000] completed. Average Loss: 0.3010\n",
            "Epoch [306/1000] completed. Average Loss: 0.3016\n",
            "Epoch [307/1000] completed. Average Loss: 0.3008\n",
            "Epoch [308/1000] completed. Average Loss: 0.3005\n",
            "Epoch [309/1000] completed. Average Loss: 0.3006\n",
            "Epoch [310/1000] completed. Average Loss: 0.3007\n",
            "Epoch [311/1000] completed. Average Loss: 0.3023\n",
            "Epoch [312/1000] completed. Average Loss: 0.3006\n",
            "Epoch [313/1000] completed. Average Loss: 0.3010\n",
            "Epoch [314/1000] completed. Average Loss: 0.3029\n",
            "Epoch [315/1000] completed. Average Loss: 0.3024\n",
            "Epoch [316/1000] completed. Average Loss: 0.3011\n",
            "Epoch [317/1000] completed. Average Loss: 0.3039\n",
            "Epoch [318/1000] completed. Average Loss: 0.3016\n",
            "Epoch [319/1000] completed. Average Loss: 0.3015\n",
            "Epoch [320/1000] completed. Average Loss: 0.3045\n",
            "Epoch [321/1000] completed. Average Loss: 0.3039\n",
            "Epoch [322/1000] completed. Average Loss: 0.3029\n",
            "Epoch [323/1000] completed. Average Loss: 0.3052\n",
            "Epoch [324/1000] completed. Average Loss: 0.3030\n",
            "Epoch [325/1000] completed. Average Loss: 0.3051\n",
            "Epoch [326/1000] completed. Average Loss: 0.3041\n",
            "Epoch [327/1000] completed. Average Loss: 0.3025\n",
            "Epoch [328/1000] completed. Average Loss: 0.3044\n",
            "Epoch [329/1000] completed. Average Loss: 0.3064\n",
            "Epoch [330/1000] completed. Average Loss: 0.3072\n",
            "Epoch [331/1000] completed. Average Loss: 0.3036\n",
            "Epoch [332/1000] completed. Average Loss: 0.3041\n",
            "Epoch [333/1000] completed. Average Loss: 0.3052\n",
            "Epoch [334/1000] completed. Average Loss: 0.3056\n",
            "Epoch [335/1000] completed. Average Loss: 0.3060\n",
            "Epoch [336/1000] completed. Average Loss: 0.3075\n",
            "Epoch [337/1000] completed. Average Loss: 0.3058\n",
            "Epoch [338/1000] completed. Average Loss: 0.3063\n",
            "Epoch [339/1000] completed. Average Loss: 0.3057\n",
            "Epoch [340/1000] completed. Average Loss: 0.3057\n",
            "Epoch [341/1000] completed. Average Loss: 0.3072\n",
            "Epoch [342/1000] completed. Average Loss: 0.3049\n",
            "Epoch [343/1000] completed. Average Loss: 0.3065\n",
            "Epoch [344/1000] completed. Average Loss: 0.3056\n",
            "Epoch [345/1000] completed. Average Loss: 0.3074\n",
            "Epoch [346/1000] completed. Average Loss: 0.3071\n",
            "Epoch [347/1000] completed. Average Loss: 0.3065\n",
            "Epoch [348/1000] completed. Average Loss: 0.3064\n",
            "Epoch [349/1000] completed. Average Loss: 0.3093\n",
            "Epoch [350/1000] completed. Average Loss: 0.3068\n",
            "Checkpoint saved for epoch 350\n",
            "Epoch [351/1000] completed. Average Loss: 0.3083\n",
            "Epoch [352/1000] completed. Average Loss: 0.3068\n",
            "Epoch [353/1000] completed. Average Loss: 0.3076\n",
            "Epoch [354/1000] completed. Average Loss: 0.3084\n",
            "Epoch [355/1000] completed. Average Loss: 0.3076\n",
            "Epoch [356/1000] completed. Average Loss: 0.3088\n",
            "Epoch [357/1000] completed. Average Loss: 0.3081\n",
            "Epoch [358/1000] completed. Average Loss: 0.3082\n",
            "Epoch [359/1000] completed. Average Loss: 0.3094\n",
            "Epoch [360/1000] completed. Average Loss: 0.3113\n",
            "Epoch [361/1000] completed. Average Loss: 0.3110\n",
            "Epoch [362/1000] completed. Average Loss: 0.3106\n",
            "Epoch [363/1000] completed. Average Loss: 0.3122\n",
            "Epoch [364/1000] completed. Average Loss: 0.3100\n",
            "Epoch [365/1000] completed. Average Loss: 0.3123\n",
            "Epoch [366/1000] completed. Average Loss: 0.3113\n",
            "Epoch [367/1000] completed. Average Loss: 0.3118\n",
            "Epoch [368/1000] completed. Average Loss: 0.3111\n",
            "Epoch [369/1000] completed. Average Loss: 0.3129\n",
            "Epoch [370/1000] completed. Average Loss: 0.3116\n",
            "Epoch [371/1000] completed. Average Loss: 0.3109\n",
            "Epoch [372/1000] completed. Average Loss: 0.3130\n",
            "Epoch [373/1000] completed. Average Loss: 0.3138\n",
            "Epoch [374/1000] completed. Average Loss: 0.3112\n",
            "Epoch [375/1000] completed. Average Loss: 0.3137\n",
            "Epoch [376/1000] completed. Average Loss: 0.3140\n",
            "Epoch [377/1000] completed. Average Loss: 0.3115\n",
            "Epoch [378/1000] completed. Average Loss: 0.3123\n",
            "Epoch [379/1000] completed. Average Loss: 0.3122\n",
            "Epoch [380/1000] completed. Average Loss: 0.3128\n",
            "Epoch [381/1000] completed. Average Loss: 0.3149\n",
            "Epoch [382/1000] completed. Average Loss: 0.3159\n",
            "Epoch [383/1000] completed. Average Loss: 0.3156\n",
            "Epoch [384/1000] completed. Average Loss: 0.3166\n",
            "Epoch [385/1000] completed. Average Loss: 0.3157\n",
            "Epoch [386/1000] completed. Average Loss: 0.3140\n",
            "Epoch [387/1000] completed. Average Loss: 0.3130\n",
            "Epoch [388/1000] completed. Average Loss: 0.3149\n",
            "Epoch [389/1000] completed. Average Loss: 0.3137\n",
            "Epoch [390/1000] completed. Average Loss: 0.3180\n",
            "Epoch [391/1000] completed. Average Loss: 0.3156\n",
            "Epoch [392/1000] completed. Average Loss: 0.3186\n",
            "Epoch [393/1000] completed. Average Loss: 0.3177\n",
            "Epoch [394/1000] completed. Average Loss: 0.3158\n",
            "Epoch [395/1000] completed. Average Loss: 0.3176\n",
            "Epoch [396/1000] completed. Average Loss: 0.3168\n",
            "Epoch [397/1000] completed. Average Loss: 0.3161\n",
            "Epoch [398/1000] completed. Average Loss: 0.3173\n",
            "Epoch [399/1000] completed. Average Loss: 0.3174\n",
            "Epoch [400/1000] completed. Average Loss: 0.3173\n",
            "Checkpoint saved for epoch 400\n",
            "Epoch [401/1000] completed. Average Loss: 0.3172\n",
            "Epoch [402/1000] completed. Average Loss: 0.3173\n",
            "Epoch [403/1000] completed. Average Loss: 0.3196\n",
            "Epoch [404/1000] completed. Average Loss: 0.3186\n",
            "Epoch [405/1000] completed. Average Loss: 0.3171\n",
            "Epoch [406/1000] completed. Average Loss: 0.3178\n",
            "Epoch [407/1000] completed. Average Loss: 0.3181\n",
            "Epoch [408/1000] completed. Average Loss: 0.3193\n",
            "Epoch [409/1000] completed. Average Loss: 0.3179\n",
            "Epoch [410/1000] completed. Average Loss: 0.3212\n",
            "Epoch [411/1000] completed. Average Loss: 0.3195\n",
            "Epoch [412/1000] completed. Average Loss: 0.3186\n",
            "Epoch [413/1000] completed. Average Loss: 0.3201\n",
            "Epoch [414/1000] completed. Average Loss: 0.3199\n",
            "Epoch [415/1000] completed. Average Loss: 0.3202\n",
            "Epoch [416/1000] completed. Average Loss: 0.3225\n",
            "Epoch [417/1000] completed. Average Loss: 0.3217\n",
            "Epoch [418/1000] completed. Average Loss: 0.3195\n",
            "Epoch [419/1000] completed. Average Loss: 0.3236\n",
            "Epoch [420/1000] completed. Average Loss: 0.3218\n",
            "Epoch [421/1000] completed. Average Loss: 0.3202\n",
            "Epoch [422/1000] completed. Average Loss: 0.3234\n",
            "Epoch [423/1000] completed. Average Loss: 0.3196\n",
            "Epoch [424/1000] completed. Average Loss: 0.3246\n",
            "Epoch [425/1000] completed. Average Loss: 0.3226\n",
            "Epoch [426/1000] completed. Average Loss: 0.3215\n",
            "Epoch [427/1000] completed. Average Loss: 0.3220\n",
            "Epoch [428/1000] completed. Average Loss: 0.3223\n",
            "Epoch [429/1000] completed. Average Loss: 0.3218\n",
            "Epoch [430/1000] completed. Average Loss: 0.3217\n",
            "Epoch [431/1000] completed. Average Loss: 0.3242\n",
            "Epoch [432/1000] completed. Average Loss: 0.3258\n",
            "Epoch [433/1000] completed. Average Loss: 0.3215\n",
            "Epoch [434/1000] completed. Average Loss: 0.3223\n",
            "Epoch [435/1000] completed. Average Loss: 0.3243\n",
            "Epoch [436/1000] completed. Average Loss: 0.3224\n",
            "Epoch [437/1000] completed. Average Loss: 0.3272\n",
            "Epoch [438/1000] completed. Average Loss: 0.3249\n",
            "Epoch [439/1000] completed. Average Loss: 0.3233\n",
            "Epoch [440/1000] completed. Average Loss: 0.3269\n",
            "Epoch [441/1000] completed. Average Loss: 0.3280\n",
            "Epoch [442/1000] completed. Average Loss: 0.3234\n",
            "Epoch [443/1000] completed. Average Loss: 0.3236\n",
            "Epoch [444/1000] completed. Average Loss: 0.3249\n",
            "Epoch [445/1000] completed. Average Loss: 0.3269\n",
            "Epoch [446/1000] completed. Average Loss: 0.3261\n",
            "Epoch [447/1000] completed. Average Loss: 0.3271\n",
            "Epoch [448/1000] completed. Average Loss: 0.3284\n",
            "Epoch [449/1000] completed. Average Loss: 0.3286\n",
            "Epoch [450/1000] completed. Average Loss: 0.3247\n",
            "Checkpoint saved for epoch 450\n",
            "Epoch [451/1000] completed. Average Loss: 0.3250\n",
            "Epoch [452/1000] completed. Average Loss: 0.3285\n",
            "Epoch [453/1000] completed. Average Loss: 0.3262\n",
            "Epoch [454/1000] completed. Average Loss: 0.3264\n",
            "Epoch [455/1000] completed. Average Loss: 0.3302\n",
            "Epoch [456/1000] completed. Average Loss: 0.3311\n",
            "Epoch [457/1000] completed. Average Loss: 0.3281\n",
            "Epoch [458/1000] completed. Average Loss: 0.3264\n",
            "Epoch [459/1000] completed. Average Loss: 0.3293\n",
            "Epoch [460/1000] completed. Average Loss: 0.3301\n",
            "Epoch [461/1000] completed. Average Loss: 0.3309\n",
            "Epoch [462/1000] completed. Average Loss: 0.3275\n",
            "Epoch [463/1000] completed. Average Loss: 0.3300\n",
            "Epoch [464/1000] completed. Average Loss: 0.3285\n",
            "Epoch [465/1000] completed. Average Loss: 0.3315\n",
            "Epoch [466/1000] completed. Average Loss: 0.3293\n",
            "Epoch [467/1000] completed. Average Loss: 0.3311\n",
            "Epoch [468/1000] completed. Average Loss: 0.3292\n",
            "Epoch [469/1000] completed. Average Loss: 0.3334\n",
            "Epoch [470/1000] completed. Average Loss: 0.3295\n",
            "Epoch [471/1000] completed. Average Loss: 0.3318\n",
            "Epoch [472/1000] completed. Average Loss: 0.3332\n",
            "Epoch [473/1000] completed. Average Loss: 0.3313\n",
            "Epoch [474/1000] completed. Average Loss: 0.3310\n",
            "Epoch [475/1000] completed. Average Loss: 0.3329\n",
            "Epoch [476/1000] completed. Average Loss: 0.3320\n",
            "Epoch [477/1000] completed. Average Loss: 0.3335\n",
            "Epoch [478/1000] completed. Average Loss: 0.3334\n",
            "Epoch [479/1000] completed. Average Loss: 0.3354\n",
            "Epoch [480/1000] completed. Average Loss: 0.3341\n",
            "Epoch [481/1000] completed. Average Loss: 0.3310\n",
            "Epoch [482/1000] completed. Average Loss: 0.3312\n",
            "Epoch [483/1000] completed. Average Loss: 0.3340\n",
            "Epoch [484/1000] completed. Average Loss: 0.3323\n",
            "Epoch [485/1000] completed. Average Loss: 0.3353\n",
            "Epoch [486/1000] completed. Average Loss: 0.3338\n",
            "Epoch [487/1000] completed. Average Loss: 0.3325\n",
            "Epoch [488/1000] completed. Average Loss: 0.3353\n",
            "Epoch [489/1000] completed. Average Loss: 0.3373\n",
            "Epoch [490/1000] completed. Average Loss: 0.3377\n",
            "Epoch [491/1000] completed. Average Loss: 0.3351\n",
            "Epoch [492/1000] completed. Average Loss: 0.3390\n",
            "Epoch [493/1000] completed. Average Loss: 0.3338\n",
            "Epoch [494/1000] completed. Average Loss: 0.3366\n",
            "Epoch [495/1000] completed. Average Loss: 0.3387\n",
            "Epoch [496/1000] completed. Average Loss: 0.3370\n",
            "Epoch [497/1000] completed. Average Loss: 0.3361\n",
            "Epoch [498/1000] completed. Average Loss: 0.3385\n",
            "Epoch [499/1000] completed. Average Loss: 0.3379\n",
            "Epoch [500/1000] completed. Average Loss: 0.3407\n",
            "Checkpoint saved for epoch 500\n",
            "Epoch [501/1000] completed. Average Loss: 0.3382\n",
            "Epoch [502/1000] completed. Average Loss: 0.3386\n",
            "Epoch [503/1000] completed. Average Loss: 0.3400\n",
            "Epoch [504/1000] completed. Average Loss: 0.3372\n",
            "Epoch [505/1000] completed. Average Loss: 0.3395\n",
            "Epoch [506/1000] completed. Average Loss: 0.3367\n",
            "Epoch [507/1000] completed. Average Loss: 0.3387\n",
            "Epoch [508/1000] completed. Average Loss: 0.3365\n",
            "Epoch [509/1000] completed. Average Loss: 0.3372\n",
            "Epoch [510/1000] completed. Average Loss: 0.3412\n",
            "Epoch [511/1000] completed. Average Loss: 0.3372\n",
            "Epoch [512/1000] completed. Average Loss: 0.3377\n",
            "Epoch [513/1000] completed. Average Loss: 0.3416\n",
            "Epoch [514/1000] completed. Average Loss: 0.3398\n",
            "Epoch [515/1000] completed. Average Loss: 0.3386\n",
            "Epoch [516/1000] completed. Average Loss: 0.3389\n",
            "Epoch [517/1000] completed. Average Loss: 0.3388\n",
            "Epoch [518/1000] completed. Average Loss: 0.3399\n",
            "Epoch [519/1000] completed. Average Loss: 0.3394\n",
            "Epoch [520/1000] completed. Average Loss: 0.3404\n",
            "Epoch [521/1000] completed. Average Loss: 0.3429\n",
            "Epoch [522/1000] completed. Average Loss: 0.3397\n",
            "Epoch [523/1000] completed. Average Loss: 0.3407\n",
            "Epoch [524/1000] completed. Average Loss: 0.3407\n",
            "Epoch [525/1000] completed. Average Loss: 0.3435\n",
            "Epoch [526/1000] completed. Average Loss: 0.3441\n",
            "Epoch [527/1000] completed. Average Loss: 0.3414\n",
            "Epoch [528/1000] completed. Average Loss: 0.3422\n",
            "Epoch [529/1000] completed. Average Loss: 0.3408\n",
            "Epoch [530/1000] completed. Average Loss: 0.3480\n",
            "Epoch [531/1000] completed. Average Loss: 0.3475\n",
            "Epoch [532/1000] completed. Average Loss: 0.3460\n",
            "Epoch [533/1000] completed. Average Loss: 0.3418\n",
            "Epoch [534/1000] completed. Average Loss: 0.3470\n",
            "Epoch [535/1000] completed. Average Loss: 0.3436\n",
            "Epoch [536/1000] completed. Average Loss: 0.3444\n",
            "Epoch [537/1000] completed. Average Loss: 0.3492\n",
            "Epoch [538/1000] completed. Average Loss: 0.3469\n",
            "Epoch [539/1000] completed. Average Loss: 0.3454\n",
            "Epoch [540/1000] completed. Average Loss: 0.3436\n",
            "Epoch [541/1000] completed. Average Loss: 0.3454\n",
            "Epoch [542/1000] completed. Average Loss: 0.3485\n",
            "Epoch [543/1000] completed. Average Loss: 0.3486\n",
            "Epoch [544/1000] completed. Average Loss: 0.3441\n",
            "Epoch [545/1000] completed. Average Loss: 0.3509\n",
            "Epoch [546/1000] completed. Average Loss: 0.3496\n",
            "Epoch [547/1000] completed. Average Loss: 0.3446\n",
            "Epoch [548/1000] completed. Average Loss: 0.3509\n",
            "Epoch [549/1000] completed. Average Loss: 0.3486\n",
            "Epoch [550/1000] completed. Average Loss: 0.3481\n",
            "Checkpoint saved for epoch 550\n",
            "Epoch [551/1000] completed. Average Loss: 0.3520\n",
            "Epoch [552/1000] completed. Average Loss: 0.3476\n",
            "Epoch [553/1000] completed. Average Loss: 0.3476\n",
            "Epoch [554/1000] completed. Average Loss: 0.3463\n",
            "Epoch [555/1000] completed. Average Loss: 0.3478\n",
            "Epoch [556/1000] completed. Average Loss: 0.3476\n",
            "Epoch [557/1000] completed. Average Loss: 0.3511\n",
            "Epoch [558/1000] completed. Average Loss: 0.3484\n",
            "Epoch [559/1000] completed. Average Loss: 0.3557\n",
            "Epoch [560/1000] completed. Average Loss: 0.3481\n",
            "Epoch [561/1000] completed. Average Loss: 0.3515\n",
            "Epoch [562/1000] completed. Average Loss: 0.3524\n",
            "Epoch [563/1000] completed. Average Loss: 0.3486\n",
            "Epoch [564/1000] completed. Average Loss: 0.3496\n",
            "Epoch [565/1000] completed. Average Loss: 0.3483\n",
            "Epoch [566/1000] completed. Average Loss: 0.3504\n",
            "Epoch [567/1000] completed. Average Loss: 0.3560\n",
            "Epoch [568/1000] completed. Average Loss: 0.3495\n",
            "Epoch [569/1000] completed. Average Loss: 0.3553\n",
            "Epoch [570/1000] completed. Average Loss: 0.3569\n",
            "Epoch [571/1000] completed. Average Loss: 0.3546\n",
            "Epoch [572/1000] completed. Average Loss: 0.3508\n",
            "Epoch [573/1000] completed. Average Loss: 0.3536\n",
            "Epoch [574/1000] completed. Average Loss: 0.3552\n",
            "Epoch [575/1000] completed. Average Loss: 0.3577\n",
            "Epoch [576/1000] completed. Average Loss: 0.3581\n",
            "Epoch [577/1000] completed. Average Loss: 0.3574\n",
            "Epoch [578/1000] completed. Average Loss: 0.3535\n",
            "Epoch [579/1000] completed. Average Loss: 0.3559\n",
            "Epoch [580/1000] completed. Average Loss: 0.3522\n",
            "Epoch [581/1000] completed. Average Loss: 0.3560\n",
            "Epoch [582/1000] completed. Average Loss: 0.3527\n",
            "Epoch [583/1000] completed. Average Loss: 0.3573\n",
            "Epoch [584/1000] completed. Average Loss: 0.3575\n",
            "Epoch [585/1000] completed. Average Loss: 0.3552\n",
            "Epoch [586/1000] completed. Average Loss: 0.3599\n",
            "Epoch [587/1000] completed. Average Loss: 0.3532\n",
            "Epoch [588/1000] completed. Average Loss: 0.3626\n",
            "Epoch [589/1000] completed. Average Loss: 0.3538\n",
            "Epoch [590/1000] completed. Average Loss: 0.3590\n",
            "Epoch [591/1000] completed. Average Loss: 0.3617\n",
            "Epoch [592/1000] completed. Average Loss: 0.3560\n",
            "Epoch [593/1000] completed. Average Loss: 0.3560\n",
            "Epoch [594/1000] completed. Average Loss: 0.3563\n",
            "Epoch [595/1000] completed. Average Loss: 0.3582\n",
            "Epoch [596/1000] completed. Average Loss: 0.3654\n",
            "Epoch [597/1000] completed. Average Loss: 0.3645\n",
            "Epoch [598/1000] completed. Average Loss: 0.3566\n",
            "Epoch [599/1000] completed. Average Loss: 0.3582\n",
            "Epoch [600/1000] completed. Average Loss: 0.3595\n",
            "Checkpoint saved for epoch 600\n",
            "Epoch [601/1000] completed. Average Loss: 0.3603\n",
            "Epoch [602/1000] completed. Average Loss: 0.3630\n",
            "Epoch [603/1000] completed. Average Loss: 0.3590\n",
            "Epoch [604/1000] completed. Average Loss: 0.3584\n",
            "Epoch [605/1000] completed. Average Loss: 0.3665\n",
            "Epoch [606/1000] completed. Average Loss: 0.3627\n",
            "Epoch [607/1000] completed. Average Loss: 0.3663\n",
            "Epoch [608/1000] completed. Average Loss: 0.3633\n",
            "Epoch [609/1000] completed. Average Loss: 0.3655\n",
            "Epoch [610/1000] completed. Average Loss: 0.3592\n",
            "Epoch [611/1000] completed. Average Loss: 0.3621\n",
            "Epoch [612/1000] completed. Average Loss: 0.3615\n",
            "Epoch [613/1000] completed. Average Loss: 0.3642\n",
            "Epoch [614/1000] completed. Average Loss: 0.3631\n",
            "Epoch [615/1000] completed. Average Loss: 0.3674\n",
            "Epoch [616/1000] completed. Average Loss: 0.3635\n",
            "Epoch [617/1000] completed. Average Loss: 0.3670\n",
            "Epoch [618/1000] completed. Average Loss: 0.3674\n",
            "Epoch [619/1000] completed. Average Loss: 0.3611\n",
            "Epoch [620/1000] completed. Average Loss: 0.3652\n",
            "Epoch [621/1000] completed. Average Loss: 0.3612\n",
            "Epoch [622/1000] completed. Average Loss: 0.3713\n",
            "Epoch [623/1000] completed. Average Loss: 0.3684\n",
            "Epoch [624/1000] completed. Average Loss: 0.3629\n",
            "Epoch [625/1000] completed. Average Loss: 0.3654\n",
            "Epoch [626/1000] completed. Average Loss: 0.3697\n",
            "Epoch [627/1000] completed. Average Loss: 0.3692\n",
            "Epoch [628/1000] completed. Average Loss: 0.3647\n",
            "Epoch [629/1000] completed. Average Loss: 0.3674\n",
            "Epoch [630/1000] completed. Average Loss: 0.3739\n",
            "Epoch [631/1000] completed. Average Loss: 0.3680\n",
            "Epoch [632/1000] completed. Average Loss: 0.3700\n",
            "Epoch [633/1000] completed. Average Loss: 0.3665\n",
            "Epoch [634/1000] completed. Average Loss: 0.3756\n",
            "Epoch [635/1000] completed. Average Loss: 0.3702\n",
            "Epoch [636/1000] completed. Average Loss: 0.3696\n",
            "Epoch [637/1000] completed. Average Loss: 0.3682\n",
            "Epoch [638/1000] completed. Average Loss: 0.3710\n",
            "Epoch [639/1000] completed. Average Loss: 0.3685\n",
            "Epoch [640/1000] completed. Average Loss: 0.3751\n",
            "Epoch [641/1000] completed. Average Loss: 0.3673\n",
            "Epoch [642/1000] completed. Average Loss: 0.3691\n",
            "Epoch [643/1000] completed. Average Loss: 0.3730\n",
            "Epoch [644/1000] completed. Average Loss: 0.3738\n",
            "Epoch [645/1000] completed. Average Loss: 0.3730\n",
            "Epoch [646/1000] completed. Average Loss: 0.3722\n",
            "Epoch [647/1000] completed. Average Loss: 0.3719\n",
            "Epoch [648/1000] completed. Average Loss: 0.3749\n",
            "Epoch [649/1000] completed. Average Loss: 0.3795\n",
            "Epoch [650/1000] completed. Average Loss: 0.3806\n",
            "Checkpoint saved for epoch 650\n",
            "Epoch [651/1000] completed. Average Loss: 0.3747\n",
            "Epoch [652/1000] completed. Average Loss: 0.3783\n",
            "Epoch [653/1000] completed. Average Loss: 0.3718\n",
            "Epoch [654/1000] completed. Average Loss: 0.3735\n",
            "Epoch [655/1000] completed. Average Loss: 0.3729\n",
            "Epoch [656/1000] completed. Average Loss: 0.3759\n",
            "Epoch [657/1000] completed. Average Loss: 0.3755\n",
            "Epoch [658/1000] completed. Average Loss: 0.3816\n",
            "Epoch [659/1000] completed. Average Loss: 0.3794\n",
            "Epoch [660/1000] completed. Average Loss: 0.3781\n",
            "Epoch [661/1000] completed. Average Loss: 0.3801\n",
            "Epoch [662/1000] completed. Average Loss: 0.3834\n",
            "Epoch [663/1000] completed. Average Loss: 0.3822\n",
            "Epoch [664/1000] completed. Average Loss: 0.3811\n",
            "Epoch [665/1000] completed. Average Loss: 0.3797\n",
            "Epoch [666/1000] completed. Average Loss: 0.3796\n",
            "Epoch [667/1000] completed. Average Loss: 0.3752\n",
            "Epoch [668/1000] completed. Average Loss: 0.3769\n",
            "Epoch [669/1000] completed. Average Loss: 0.3745\n",
            "Epoch [670/1000] completed. Average Loss: 0.3777\n",
            "Epoch [671/1000] completed. Average Loss: 0.3849\n",
            "Epoch [672/1000] completed. Average Loss: 0.3747\n",
            "Epoch [673/1000] completed. Average Loss: 0.3754\n",
            "Epoch [674/1000] completed. Average Loss: 0.3771\n",
            "Epoch [675/1000] completed. Average Loss: 0.3877\n",
            "Epoch [676/1000] completed. Average Loss: 0.3780\n",
            "Epoch [677/1000] completed. Average Loss: 0.3826\n",
            "Epoch [678/1000] completed. Average Loss: 0.3784\n",
            "Epoch [679/1000] completed. Average Loss: 0.3865\n",
            "Epoch [680/1000] completed. Average Loss: 0.3855\n",
            "Epoch [681/1000] completed. Average Loss: 0.3785\n",
            "Epoch [682/1000] completed. Average Loss: 0.3836\n",
            "Epoch [683/1000] completed. Average Loss: 0.3881\n",
            "Epoch [684/1000] completed. Average Loss: 0.3818\n",
            "Epoch [685/1000] completed. Average Loss: 0.3821\n",
            "Epoch [686/1000] completed. Average Loss: 0.3844\n",
            "Epoch [687/1000] completed. Average Loss: 0.3860\n",
            "Epoch [688/1000] completed. Average Loss: 0.3883\n",
            "Epoch [689/1000] completed. Average Loss: 0.3809\n",
            "Epoch [690/1000] completed. Average Loss: 0.3902\n",
            "Epoch [691/1000] completed. Average Loss: 0.3930\n",
            "Epoch [692/1000] completed. Average Loss: 0.3882\n",
            "Epoch [693/1000] completed. Average Loss: 0.3871\n",
            "Epoch [694/1000] completed. Average Loss: 0.3840\n",
            "Epoch [695/1000] completed. Average Loss: 0.3835\n",
            "Epoch [696/1000] completed. Average Loss: 0.3953\n",
            "Epoch [697/1000] completed. Average Loss: 0.3855\n",
            "Epoch [698/1000] completed. Average Loss: 0.3911\n",
            "Epoch [699/1000] completed. Average Loss: 0.3957\n",
            "Epoch [700/1000] completed. Average Loss: 0.3925\n",
            "Checkpoint saved for epoch 700\n",
            "Epoch [701/1000] completed. Average Loss: 0.3920\n",
            "Epoch [702/1000] completed. Average Loss: 0.3971\n",
            "Epoch [703/1000] completed. Average Loss: 0.3896\n",
            "Epoch [704/1000] completed. Average Loss: 0.3907\n",
            "Epoch [705/1000] completed. Average Loss: 0.3921\n",
            "Epoch [706/1000] completed. Average Loss: 0.3932\n",
            "Epoch [707/1000] completed. Average Loss: 0.3957\n",
            "Epoch [708/1000] completed. Average Loss: 0.3953\n",
            "Epoch [709/1000] completed. Average Loss: 0.3963\n",
            "Epoch [710/1000] completed. Average Loss: 0.3916\n",
            "Epoch [711/1000] completed. Average Loss: 0.3896\n",
            "Epoch [712/1000] completed. Average Loss: 0.3924\n",
            "Epoch [713/1000] completed. Average Loss: 0.3887\n",
            "Epoch [714/1000] completed. Average Loss: 0.4001\n",
            "Epoch [715/1000] completed. Average Loss: 0.3984\n",
            "Epoch [716/1000] completed. Average Loss: 0.3951\n",
            "Epoch [717/1000] completed. Average Loss: 0.3967\n",
            "Epoch [718/1000] completed. Average Loss: 0.3894\n",
            "Epoch [719/1000] completed. Average Loss: 0.4019\n",
            "Epoch [720/1000] completed. Average Loss: 0.3965\n",
            "Epoch [721/1000] completed. Average Loss: 0.4013\n",
            "Epoch [722/1000] completed. Average Loss: 0.3928\n",
            "Epoch [723/1000] completed. Average Loss: 0.4024\n",
            "Epoch [724/1000] completed. Average Loss: 0.3982\n",
            "Epoch [725/1000] completed. Average Loss: 0.4034\n",
            "Epoch [726/1000] completed. Average Loss: 0.3998\n",
            "Epoch [727/1000] completed. Average Loss: 0.4008\n",
            "Epoch [728/1000] completed. Average Loss: 0.4028\n",
            "Epoch [729/1000] completed. Average Loss: 0.4028\n",
            "Epoch [730/1000] completed. Average Loss: 0.4004\n",
            "Epoch [731/1000] completed. Average Loss: 0.3950\n",
            "Epoch [732/1000] completed. Average Loss: 0.4028\n",
            "Epoch [733/1000] completed. Average Loss: 0.3969\n",
            "Epoch [734/1000] completed. Average Loss: 0.4023\n",
            "Epoch [735/1000] completed. Average Loss: 0.3946\n",
            "Epoch [736/1000] completed. Average Loss: 0.4032\n",
            "Epoch [737/1000] completed. Average Loss: 0.3984\n",
            "Epoch [738/1000] completed. Average Loss: 0.4014\n",
            "Epoch [739/1000] completed. Average Loss: 0.4045\n",
            "Epoch [740/1000] completed. Average Loss: 0.4052\n",
            "Epoch [741/1000] completed. Average Loss: 0.4097\n",
            "Epoch [742/1000] completed. Average Loss: 0.4056\n",
            "Epoch [743/1000] completed. Average Loss: 0.4003\n",
            "Epoch [744/1000] completed. Average Loss: 0.4109\n",
            "Epoch [745/1000] completed. Average Loss: 0.4110\n",
            "Epoch [746/1000] completed. Average Loss: 0.4099\n",
            "Epoch [747/1000] completed. Average Loss: 0.4108\n",
            "Epoch [748/1000] completed. Average Loss: 0.4031\n",
            "Epoch [749/1000] completed. Average Loss: 0.4155\n",
            "Epoch [750/1000] completed. Average Loss: 0.3987\n",
            "Checkpoint saved for epoch 750\n",
            "Epoch [751/1000] completed. Average Loss: 0.4024\n",
            "Epoch [752/1000] completed. Average Loss: 0.4023\n",
            "Epoch [753/1000] completed. Average Loss: 0.4136\n",
            "Epoch [754/1000] completed. Average Loss: 0.4170\n",
            "Epoch [755/1000] completed. Average Loss: 0.4163\n",
            "Epoch [756/1000] completed. Average Loss: 0.4056\n",
            "Epoch [757/1000] completed. Average Loss: 0.4038\n",
            "Epoch [758/1000] completed. Average Loss: 0.4145\n",
            "Epoch [759/1000] completed. Average Loss: 0.4048\n",
            "Epoch [760/1000] completed. Average Loss: 0.4164\n",
            "Epoch [761/1000] completed. Average Loss: 0.4093\n",
            "Epoch [762/1000] completed. Average Loss: 0.4097\n",
            "Epoch [763/1000] completed. Average Loss: 0.4101\n",
            "Epoch [764/1000] completed. Average Loss: 0.4029\n",
            "Epoch [765/1000] completed. Average Loss: 0.4055\n",
            "Epoch [766/1000] completed. Average Loss: 0.4186\n",
            "Epoch [767/1000] completed. Average Loss: 0.4134\n",
            "Epoch [768/1000] completed. Average Loss: 0.4051\n",
            "Epoch [769/1000] completed. Average Loss: 0.4164\n",
            "Epoch [770/1000] completed. Average Loss: 0.4131\n",
            "Epoch [771/1000] completed. Average Loss: 0.4064\n",
            "Epoch [772/1000] completed. Average Loss: 0.4078\n",
            "Epoch [773/1000] completed. Average Loss: 0.4165\n",
            "Epoch [774/1000] completed. Average Loss: 0.4171\n",
            "Epoch [775/1000] completed. Average Loss: 0.4131\n",
            "Epoch [776/1000] completed. Average Loss: 0.4152\n",
            "Epoch [777/1000] completed. Average Loss: 0.4270\n",
            "Epoch [778/1000] completed. Average Loss: 0.4109\n",
            "Epoch [779/1000] completed. Average Loss: 0.4121\n",
            "Epoch [780/1000] completed. Average Loss: 0.4152\n",
            "Epoch [781/1000] completed. Average Loss: 0.4097\n",
            "Epoch [782/1000] completed. Average Loss: 0.4123\n",
            "Epoch [783/1000] completed. Average Loss: 0.4164\n",
            "Epoch [784/1000] completed. Average Loss: 0.4146\n",
            "Epoch [785/1000] completed. Average Loss: 0.4111\n",
            "Epoch [786/1000] completed. Average Loss: 0.4195\n",
            "Epoch [787/1000] completed. Average Loss: 0.4214\n",
            "Epoch [788/1000] completed. Average Loss: 0.4160\n",
            "Epoch [789/1000] completed. Average Loss: 0.4229\n",
            "Epoch [790/1000] completed. Average Loss: 0.4145\n",
            "Epoch [791/1000] completed. Average Loss: 0.4329\n",
            "Epoch [792/1000] completed. Average Loss: 0.4287\n",
            "Epoch [793/1000] completed. Average Loss: 0.4237\n",
            "Epoch [794/1000] completed. Average Loss: 0.4323\n",
            "Epoch [795/1000] completed. Average Loss: 0.4277\n",
            "Epoch [796/1000] completed. Average Loss: 0.4270\n",
            "Epoch [797/1000] completed. Average Loss: 0.4327\n",
            "Epoch [798/1000] completed. Average Loss: 0.4259\n",
            "Epoch [799/1000] completed. Average Loss: 0.4152\n",
            "Epoch [800/1000] completed. Average Loss: 0.4343\n",
            "Checkpoint saved for epoch 800\n",
            "Epoch [801/1000] completed. Average Loss: 0.4266\n",
            "Epoch [802/1000] completed. Average Loss: 0.4220\n",
            "Epoch [803/1000] completed. Average Loss: 0.4192\n",
            "Epoch [804/1000] completed. Average Loss: 0.4379\n",
            "Epoch [805/1000] completed. Average Loss: 0.4255\n",
            "Epoch [806/1000] completed. Average Loss: 0.4281\n",
            "Epoch [807/1000] completed. Average Loss: 0.4307\n",
            "Epoch [808/1000] completed. Average Loss: 0.4226\n",
            "Epoch [809/1000] completed. Average Loss: 0.4230\n",
            "Epoch [810/1000] completed. Average Loss: 0.4407\n",
            "Epoch [811/1000] completed. Average Loss: 0.4293\n",
            "Epoch [812/1000] completed. Average Loss: 0.4246\n",
            "Epoch [813/1000] completed. Average Loss: 0.4225\n",
            "Epoch [814/1000] completed. Average Loss: 0.4411\n",
            "Epoch [815/1000] completed. Average Loss: 0.4224\n",
            "Epoch [816/1000] completed. Average Loss: 0.4373\n",
            "Epoch [817/1000] completed. Average Loss: 0.4363\n",
            "Epoch [818/1000] completed. Average Loss: 0.4284\n",
            "Epoch [819/1000] completed. Average Loss: 0.4314\n",
            "Epoch [820/1000] completed. Average Loss: 0.4257\n",
            "Epoch [821/1000] completed. Average Loss: 0.4368\n",
            "Epoch [822/1000] completed. Average Loss: 0.4386\n",
            "Epoch [823/1000] completed. Average Loss: 0.4438\n",
            "Epoch [824/1000] completed. Average Loss: 0.4431\n",
            "Epoch [825/1000] completed. Average Loss: 0.4316\n",
            "Epoch [826/1000] completed. Average Loss: 0.4389\n",
            "Epoch [827/1000] completed. Average Loss: 0.4347\n",
            "Epoch [828/1000] completed. Average Loss: 0.4279\n",
            "Epoch [829/1000] completed. Average Loss: 0.4371\n",
            "Epoch [830/1000] completed. Average Loss: 0.4349\n",
            "Epoch [831/1000] completed. Average Loss: 0.4404\n",
            "Epoch [832/1000] completed. Average Loss: 0.4513\n",
            "Epoch [833/1000] completed. Average Loss: 0.4351\n",
            "Epoch [834/1000] completed. Average Loss: 0.4414\n",
            "Epoch [835/1000] completed. Average Loss: 0.4340\n",
            "Epoch [836/1000] completed. Average Loss: 0.4360\n",
            "Epoch [837/1000] completed. Average Loss: 0.4377\n",
            "Epoch [838/1000] completed. Average Loss: 0.4548\n",
            "Epoch [839/1000] completed. Average Loss: 0.4447\n",
            "Epoch [840/1000] completed. Average Loss: 0.4341\n",
            "Epoch [841/1000] completed. Average Loss: 0.4562\n",
            "Epoch [842/1000] completed. Average Loss: 0.4570\n",
            "Epoch [843/1000] completed. Average Loss: 0.4363\n",
            "Epoch [844/1000] completed. Average Loss: 0.4351\n",
            "Epoch [845/1000] completed. Average Loss: 0.4370\n",
            "Epoch [846/1000] completed. Average Loss: 0.4390\n",
            "Epoch [847/1000] completed. Average Loss: 0.4581\n",
            "Epoch [848/1000] completed. Average Loss: 0.4500\n",
            "Epoch [849/1000] completed. Average Loss: 0.4580\n",
            "Epoch [850/1000] completed. Average Loss: 0.4399\n",
            "Checkpoint saved for epoch 850\n",
            "Epoch [851/1000] completed. Average Loss: 0.4471\n",
            "Epoch [852/1000] completed. Average Loss: 0.4520\n",
            "Epoch [853/1000] completed. Average Loss: 0.4388\n",
            "Epoch [854/1000] completed. Average Loss: 0.4452\n",
            "Epoch [855/1000] completed. Average Loss: 0.4577\n",
            "Epoch [856/1000] completed. Average Loss: 0.4645\n",
            "Epoch [857/1000] completed. Average Loss: 0.4505\n",
            "Epoch [858/1000] completed. Average Loss: 0.4540\n",
            "Epoch [859/1000] completed. Average Loss: 0.4618\n",
            "Epoch [860/1000] completed. Average Loss: 0.4596\n",
            "Epoch [861/1000] completed. Average Loss: 0.4675\n",
            "Epoch [862/1000] completed. Average Loss: 0.4632\n",
            "Epoch [863/1000] completed. Average Loss: 0.4492\n",
            "Epoch [864/1000] completed. Average Loss: 0.4493\n",
            "Epoch [865/1000] completed. Average Loss: 0.4433\n",
            "Epoch [866/1000] completed. Average Loss: 0.4535\n",
            "Epoch [867/1000] completed. Average Loss: 0.4674\n",
            "Epoch [868/1000] completed. Average Loss: 0.4722\n",
            "Epoch [869/1000] completed. Average Loss: 0.4467\n",
            "Epoch [870/1000] completed. Average Loss: 0.4471\n",
            "Epoch [871/1000] completed. Average Loss: 0.4621\n",
            "Epoch [872/1000] completed. Average Loss: 0.4650\n",
            "Epoch [873/1000] completed. Average Loss: 0.4606\n",
            "Epoch [874/1000] completed. Average Loss: 0.4661\n",
            "Epoch [875/1000] completed. Average Loss: 0.4685\n",
            "Epoch [876/1000] completed. Average Loss: 0.4611\n",
            "Epoch [877/1000] completed. Average Loss: 0.4536\n",
            "Epoch [878/1000] completed. Average Loss: 0.4626\n",
            "Epoch [879/1000] completed. Average Loss: 0.4749\n",
            "Epoch [880/1000] completed. Average Loss: 0.4508\n",
            "Epoch [881/1000] completed. Average Loss: 0.4514\n",
            "Epoch [882/1000] completed. Average Loss: 0.4574\n",
            "Epoch [883/1000] completed. Average Loss: 0.4589\n",
            "Epoch [884/1000] completed. Average Loss: 0.4549\n",
            "Epoch [885/1000] completed. Average Loss: 0.4620\n",
            "Epoch [886/1000] completed. Average Loss: 0.4620\n",
            "Epoch [887/1000] completed. Average Loss: 0.4726\n",
            "Epoch [888/1000] completed. Average Loss: 0.4675\n",
            "Epoch [889/1000] completed. Average Loss: 0.4610\n",
            "Epoch [890/1000] completed. Average Loss: 0.4619\n",
            "Epoch [891/1000] completed. Average Loss: 0.4556\n",
            "Epoch [892/1000] completed. Average Loss: 0.4804\n",
            "Epoch [893/1000] completed. Average Loss: 0.4769\n",
            "Epoch [894/1000] completed. Average Loss: 0.4841\n",
            "Epoch [895/1000] completed. Average Loss: 0.4702\n",
            "Epoch [896/1000] completed. Average Loss: 0.4580\n",
            "Epoch [897/1000] completed. Average Loss: 0.4840\n",
            "Epoch [898/1000] completed. Average Loss: 0.4831\n",
            "Epoch [899/1000] completed. Average Loss: 0.4588\n",
            "Epoch [900/1000] completed. Average Loss: 0.4707\n",
            "Checkpoint saved for epoch 900\n",
            "Epoch [901/1000] completed. Average Loss: 0.4795\n",
            "Epoch [902/1000] completed. Average Loss: 0.4653\n",
            "Epoch [903/1000] completed. Average Loss: 0.4906\n",
            "Epoch [904/1000] completed. Average Loss: 0.4956\n",
            "Epoch [905/1000] completed. Average Loss: 0.4690\n",
            "Epoch [906/1000] completed. Average Loss: 0.4813\n",
            "Epoch [907/1000] completed. Average Loss: 0.4802\n",
            "Epoch [908/1000] completed. Average Loss: 0.4810\n",
            "Epoch [909/1000] completed. Average Loss: 0.4886\n",
            "Epoch [910/1000] completed. Average Loss: 0.4782\n",
            "Epoch [911/1000] completed. Average Loss: 0.4687\n",
            "Epoch [912/1000] completed. Average Loss: 0.4998\n",
            "Epoch [913/1000] completed. Average Loss: 0.4811\n",
            "Epoch [914/1000] completed. Average Loss: 0.4778\n",
            "Epoch [915/1000] completed. Average Loss: 0.4778\n",
            "Epoch [916/1000] completed. Average Loss: 0.5015\n",
            "Epoch [917/1000] completed. Average Loss: 0.4731\n",
            "Epoch [918/1000] completed. Average Loss: 0.4789\n",
            "Epoch [919/1000] completed. Average Loss: 0.4925\n",
            "Epoch [920/1000] completed. Average Loss: 0.4809\n",
            "Epoch [921/1000] completed. Average Loss: 0.4783\n",
            "Epoch [922/1000] completed. Average Loss: 0.4782\n",
            "Epoch [923/1000] completed. Average Loss: 0.4810\n",
            "Epoch [924/1000] completed. Average Loss: 0.4901\n",
            "Epoch [925/1000] completed. Average Loss: 0.4942\n",
            "Epoch [926/1000] completed. Average Loss: 0.5079\n",
            "Epoch [927/1000] completed. Average Loss: 0.4733\n",
            "Epoch [928/1000] completed. Average Loss: 0.4783\n",
            "Epoch [929/1000] completed. Average Loss: 0.4926\n",
            "Epoch [930/1000] completed. Average Loss: 0.4974\n",
            "Epoch [931/1000] completed. Average Loss: 0.4756\n",
            "Epoch [932/1000] completed. Average Loss: 0.4803\n",
            "Epoch [933/1000] completed. Average Loss: 0.4882\n",
            "Epoch [934/1000] completed. Average Loss: 0.4809\n",
            "Epoch [935/1000] completed. Average Loss: 0.4907\n",
            "Epoch [936/1000] completed. Average Loss: 0.5087\n",
            "Epoch [937/1000] completed. Average Loss: 0.5029\n",
            "Epoch [938/1000] completed. Average Loss: 0.4868\n",
            "Epoch [939/1000] completed. Average Loss: 0.5104\n",
            "Epoch [940/1000] completed. Average Loss: 0.5030\n",
            "Epoch [941/1000] completed. Average Loss: 0.5154\n",
            "Epoch [942/1000] completed. Average Loss: 0.4932\n",
            "Epoch [943/1000] completed. Average Loss: 0.5244\n",
            "Epoch [944/1000] completed. Average Loss: 0.5179\n",
            "Epoch [945/1000] completed. Average Loss: 0.4940\n",
            "Epoch [946/1000] completed. Average Loss: 0.5245\n",
            "Epoch [947/1000] completed. Average Loss: 0.4914\n",
            "Epoch [948/1000] completed. Average Loss: 0.5241\n",
            "Epoch [949/1000] completed. Average Loss: 0.5272\n",
            "Epoch [950/1000] completed. Average Loss: 0.4922\n",
            "Checkpoint saved for epoch 950\n",
            "Epoch [951/1000] completed. Average Loss: 0.4926\n",
            "Epoch [952/1000] completed. Average Loss: 0.4994\n",
            "Epoch [953/1000] completed. Average Loss: 0.5034\n",
            "Epoch [954/1000] completed. Average Loss: 0.4988\n",
            "Epoch [955/1000] completed. Average Loss: 0.4991\n",
            "Epoch [956/1000] completed. Average Loss: 0.5331\n",
            "Epoch [957/1000] completed. Average Loss: 0.5185\n",
            "Epoch [958/1000] completed. Average Loss: 0.5012\n",
            "Epoch [959/1000] completed. Average Loss: 0.4992\n",
            "Epoch [960/1000] completed. Average Loss: 0.5235\n",
            "Epoch [961/1000] completed. Average Loss: 0.4956\n",
            "Epoch [962/1000] completed. Average Loss: 0.5065\n",
            "Epoch [963/1000] completed. Average Loss: 0.5030\n",
            "Epoch [964/1000] completed. Average Loss: 0.4962\n",
            "Epoch [965/1000] completed. Average Loss: 0.5057\n",
            "Epoch [966/1000] completed. Average Loss: 0.5066\n",
            "Epoch [967/1000] completed. Average Loss: 0.5335\n",
            "Epoch [968/1000] completed. Average Loss: 0.5031\n",
            "Epoch [969/1000] completed. Average Loss: 0.5150\n",
            "Epoch [970/1000] completed. Average Loss: 0.5339\n",
            "Epoch [971/1000] completed. Average Loss: 0.5347\n",
            "Epoch [972/1000] completed. Average Loss: 0.5240\n",
            "Epoch [973/1000] completed. Average Loss: 0.5269\n",
            "Epoch [974/1000] completed. Average Loss: 0.5112\n",
            "Epoch [975/1000] completed. Average Loss: 0.5327\n",
            "Epoch [976/1000] completed. Average Loss: 0.5266\n",
            "Epoch [977/1000] completed. Average Loss: 0.5340\n",
            "Epoch [978/1000] completed. Average Loss: 0.5250\n",
            "Epoch [979/1000] completed. Average Loss: 0.5143\n",
            "Epoch [980/1000] completed. Average Loss: 0.5285\n",
            "Epoch [981/1000] completed. Average Loss: 0.5204\n",
            "Epoch [982/1000] completed. Average Loss: 0.5250\n",
            "Epoch [983/1000] completed. Average Loss: 0.5335\n",
            "Epoch [984/1000] completed. Average Loss: 0.5270\n",
            "Epoch [985/1000] completed. Average Loss: 0.5330\n",
            "Epoch [986/1000] completed. Average Loss: 0.5122\n",
            "Epoch [987/1000] completed. Average Loss: 0.5274\n",
            "Epoch [988/1000] completed. Average Loss: 0.5329\n",
            "Epoch [989/1000] completed. Average Loss: 0.5131\n",
            "Epoch [990/1000] completed. Average Loss: 0.5337\n",
            "Epoch [991/1000] completed. Average Loss: 0.5216\n",
            "Epoch [992/1000] completed. Average Loss: 0.5320\n",
            "Epoch [993/1000] completed. Average Loss: 0.5240\n",
            "Epoch [994/1000] completed. Average Loss: 0.5335\n",
            "Epoch [995/1000] completed. Average Loss: 0.5325\n",
            "Epoch [996/1000] completed. Average Loss: 0.5326\n",
            "Epoch [997/1000] completed. Average Loss: 0.5327\n",
            "Epoch [998/1000] completed. Average Loss: 0.5326\n",
            "Epoch [999/1000] completed. Average Loss: 0.5328\n",
            "Epoch [1000/1000] completed. Average Loss: 0.5313\n",
            "Checkpoint saved for epoch 1000\n",
            "Final model saved as 'vision_transformer_final_balanced.pth'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>▃▁▂▂▁▂▂▃▂▁▂▂▃▃▂▃▂▃▄▄▄▃▄▄▅▅▅▅▅▄▅▆▅▆▆█▆▇██</td></tr><tr><td>epoch_loss</td><td>▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▅▅▅▅▅▅▅▅▅▆▆▇▇▇█</td></tr><tr><td>masked_patches</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.52589</td></tr><tr><td>epoch_loss</td><td>0.53131</td></tr><tr><td>masked_patches</td><td>921</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-shadow-56</strong> at: <a href='https://wandb.ai/oscars/vision-transformer/runs/nsyxjjyn' target=\"_blank\">https://wandb.ai/oscars/vision-transformer/runs/nsyxjjyn</a><br/> View project at: <a href='https://wandb.ai/oscars/vision-transformer' target=\"_blank\">https://wandb.ai/oscars/vision-transformer</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3021 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241206_135607-nsyxjjyn/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}