{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X52W1oNdDg8T",
        "outputId": "3472d39d-30a3-43f4-81b4-369f0ac062bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4jIK9pWDfU0"
      },
      "source": [
        "# Define the Vision Transfomer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "TJ-t9dPNDfU1",
        "outputId": "93cf0520-817d-4a48-a547-c26713ee2d7b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, feedforward_dim, num_layers, num_tokens, max_patches, dropout=0.0, hidden_dim=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(hidden_dim, num_heads, feedforward_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Load hidden_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Linear layer to project input embeddings\n",
        "        self.input_proj = nn.Linear(embed_dim, self.hidden_dim)\n",
        "\n",
        "        # Positional embeddings\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(max_patches, 1, self.hidden_dim))  # Shape: (seq_len, 1, hidden_dim)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(self.hidden_dim, num_tokens-1)\n",
        "\n",
        "    def forward(self, patches):\n",
        "\n",
        "        # Prepare input for transformer layers\n",
        "        x = patches.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Extract seq_len and batch_size\n",
        "        seq_len, batch_size, _ = x.size()\n",
        "\n",
        "        # Project input to hidden_dim\n",
        "        z = self.input_proj(x)  # Shape: (seq_len, batch_size, hidden_dim)\n",
        "\n",
        "        # Add positional embedding\n",
        "        pos_emb = self.positional_embedding[:seq_len, :, :].expand(-1, batch_size, -1)  # Shape: (seq_len, batch_size, hidden_dim)\n",
        "        z = z + pos_emb\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        for layer in self.encoder_layers:\n",
        "            z = layer(z)\n",
        "\n",
        "        # Output logits\n",
        "        z = z.permute(1, 0, 2)  # Back to (batch_size, seq_len, hidden_dim)\n",
        "        logits = self.fc_out(z)  # (batch_size, seq_len, num_tokens-1)\n",
        "        return logits\n",
        "\n",
        "    def get_probabilities(self, logits):\n",
        "        \"\"\"Compute probabilities using softmax.\"\"\"\n",
        "        return torch.softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, feedforward_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Ensure hidden_dim is divisible by num_heads\n",
        "        assert self.hidden_dim % self.num_heads == 0, \"Hidden dimension must be divisible by the number of heads.\"\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=self.hidden_dim, num_heads=num_heads, dropout=dropout)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, feedforward_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, self.hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(self.hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(self.hidden_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: Tensor of shape (seq_len, batch_size, hidden_dim)\n",
        "        Returns:\n",
        "            Tensor of shape (seq_len, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        seq_len, batch_size, hidden_dim = z.size()\n",
        "\n",
        "        # Apply LayerNorm\n",
        "        z_norm = self.norm1(z)\n",
        "\n",
        "        # Self-attention\n",
        "        attn_output, _ = self.attention(z_norm, z_norm, z_norm)  # Shape: (seq_len, batch_size, hidden_dim)\n",
        "\n",
        "        # Residual connection\n",
        "        z = z + self.dropout(attn_output)\n",
        "\n",
        "        # Feedforward layer\n",
        "        z_norm = self.norm2(z)\n",
        "        feedforward_output = self.feedforward(z_norm)\n",
        "\n",
        "        # Final residual connection\n",
        "        z = z + self.dropout(feedforward_output)\n",
        "\n",
        "        return z\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Toy Dataset: 100 Binary 4x4 Images\n",
        "images = torch.randint(0, 2, (100, 4, 4)).long()\n",
        "\n",
        "# Plot the first 10 images in a 5x2 subplot\n",
        "fig, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
        "fig.suptitle(\"10 First Realizations of Dataset\", fontsize=16)\n",
        "\n",
        "for idx, ax in enumerate(axs.flat):  # Flatten the 2D array of axes for easy iteration\n",
        "    ax.imshow(images[idx].numpy(), cmap=\"gray\")\n",
        "    ax.set_title(f\"Image {idx + 1}\")\n",
        "    ax.axis(\"off\")  # Turn off the axes for cleaner visualization\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit the title\n",
        "plt.show()\n",
        "\n",
        "import wandb\n",
        "# Parameters\n",
        "batch_size = 1\n",
        "embed_dim = 2\n",
        "hidden_dim = 3\n",
        "num_heads = 1\n",
        "feedforward_dim = hidden_dim*2  # (2-4)\n",
        "num_layers = 1\n",
        "num_tokens = 5  # 4 tokens + 1 mask token\n",
        "max_patches = 8\n",
        "dropout = 0.2\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 1\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model\n",
        "model = VisionTransformer(embed_dim, num_heads, feedforward_dim, num_layers, num_tokens, max_patches, dropout, hidden_dim).to(device)\n",
        "#model.load_state_dict(checkpoint)  # Load model weights\n",
        "# Optimizer and Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Dataloader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (Tensor): Tensor of shape (num_images, 64, 64) with binary values (0 or 1).\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        return torch.tensor(image, dtype=torch.float32)\n",
        "\n",
        "dataset = BinaryImageDataset(images)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def preprocess_image_with_indices(image):\n",
        "    \"\"\"\n",
        "    Splits a 4x4 binary image into flattened 2-cell patches.\n",
        "    Returns patch indices tensor (num_patches,).\n",
        "    \"\"\"\n",
        "    patches = image.unfold(1, 2, 2).reshape(-1, 2)  # (num_patches, 2)\n",
        "    patch_indices = (patches * torch.tensor([2, 1])).sum(dim=1)  # Binary to decimal\n",
        "\n",
        "    return patch_indices\n",
        "\n",
        "def indices_to_patches_batched(patch_indices):\n",
        "    \"\"\"\n",
        "    Converts batched patch indices back to 2-cell patches.\n",
        "    Includes the masked patch [0.5, 0.5] for index 4.\n",
        "\n",
        "    Args:\n",
        "        patch_indices (torch.Tensor): Tensor of shape (batch_size, num_patches).\n",
        "\n",
        "    Returns:\n",
        "        patches (torch.Tensor): Tensor of shape (batch_size, num_patches, 2).\n",
        "    \"\"\"\n",
        "    # Define the index-to-patch mapping\n",
        "    index_to_patch = {\n",
        "        0: torch.tensor([0.0, 0.0]),\n",
        "        1: torch.tensor([0.0, 1.0]),\n",
        "        2: torch.tensor([1.0, 0.0]),\n",
        "        3: torch.tensor([1.0, 1.0]),\n",
        "        4: torch.tensor([0.5, 0.5])  # Masked patch\n",
        "    }\n",
        "\n",
        "    # Map each index in the batch to its corresponding patch\n",
        "    patches = torch.stack([\n",
        "        torch.stack([index_to_patch[idx.item()] for idx in batch])\n",
        "        for batch in patch_indices\n",
        "    ])\n",
        "\n",
        "    return patches\n",
        "\n",
        "def create_valid_mask(masked_patches, mask_prob=0.5):\n",
        "    \"\"\"\n",
        "    Creates a mask with at least one `True` and one `False` per batch.\n",
        "\n",
        "    Args:\n",
        "        masked_patches (torch.Tensor): Tensor of shape (batch_size, num_patches).\n",
        "        mask_prob (float): Probability of masking each patch.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Binary mask of the same shape as `masked_patches`.\n",
        "    \"\"\"\n",
        "    # Initial random mask\n",
        "    mask = torch.rand(masked_patches.shape) < mask_prob  # (batch_size, num_patches)\n",
        "\n",
        "    # Ensure at least one `True` and one `False` per batch\n",
        "    for i in range(mask.shape[0]):  # Iterate over batches\n",
        "        if not mask[i].any():  # If all values are `False`\n",
        "            mask[i, torch.randint(0, mask.shape[1], (1,))] = True  # Set one random patch to `True`\n",
        "        if mask[i].all():  # If all values are `True`\n",
        "            mask[i, torch.randint(0, mask.shape[1], (1,))] = False  # Set one random patch to `False`\n",
        "\n",
        "    return mask\n",
        "\n",
        "def image_from_indices_batched(patch_indices, image_shape=(4, 4)):\n",
        "    \"\"\"\n",
        "    Reconstructs binary images from batched patch indices.\n",
        "\n",
        "    Args:\n",
        "        patch_indices (torch.Tensor): Tensor of shape (batch_size, num_patches).\n",
        "        image_shape (tuple): Shape of each reconstructed image (default is 4x4).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Reconstructed binary images of shape (batch_size, *image_shape).\n",
        "    \"\"\"\n",
        "    batch_size, num_patches = patch_indices.shape\n",
        "    num_rows, num_cols = image_shape\n",
        "    assert num_rows % 2 == 0 and num_cols % 2 == 0, \"Image dimensions must be divisible by 2.\"\n",
        "\n",
        "    # Number of patches in each dimension\n",
        "    patch_rows = num_rows // 2\n",
        "    patch_cols = num_cols // 2\n",
        "    assert num_patches == patch_rows * patch_cols, \"Number of patches does not match image size.\"\n",
        "\n",
        "    # Convert indices back to patches\n",
        "    patches = torch.stack([\n",
        "        ((patch_indices >> 1) & 1),  # Extract the first bit (most significant)\n",
        "        (patch_indices & 1)         # Extract the second bit (least significant)\n",
        "    ], dim=-1).to(torch.float32)  # Shape: (batch_size, num_patches, 2)\n",
        "\n",
        "    # Reshape patches into grid\n",
        "    patches = patches.view(batch_size, patch_rows, patch_cols, 2)  # Shape: (batch_size, patch_rows, patch_cols, 2)\n",
        "\n",
        "    # Create the full image by placing patches into rows\n",
        "    image = torch.zeros(batch_size, num_rows, num_cols, dtype=torch.float32)\n",
        "    for i in range(patch_rows):\n",
        "        for j in range(patch_cols):\n",
        "            image[:, i, j * 2:j * 2 + 2] = patches[:, i, j]\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "X2gi7ds3Gvww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_idx,images = next(enumerate(dataloader))\n",
        "# Preprocess images\n",
        "Y = torch.stack([preprocess_image_with_indices(img) for img in images]).long() #(batch_size,num_patches,)\n",
        "print(\"Y (Original Image):\", Y)\n",
        "print(\"Y shape:\",Y.size())\n",
        "masked_patches = Y.clone() #(batch_size,num_patches,)\n",
        "\n",
        "# Masking\n",
        "masky = torch.zeros(batch_size, max_patches)\n",
        "mask = create_valid_mask(masky,mask_prob=0.5) #(batch_size,num_patches,)\n",
        "print(\"mask:\",mask)\n",
        "print(\"mask shape:\",mask.size())\n",
        "masked_patches[mask] = num_tokens - 1\n",
        "print(\"masked_patches (Masked Image):\",masked_patches)\n",
        "print(\"masked_patches shape:\",masked_patches.size())\n",
        "X = indices_to_patches_batched(masked_patches) #(batch_size,num_patches,patch_size)\n",
        "print(\"X (Patches):\",X)\n",
        "print(\"X shape:\",X.size())\n",
        "X = X.to(device)\n",
        "Y = Y.to(device)\n",
        "\n",
        "\n",
        "# Define weighted CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Forward pass\n",
        "logits = model(X) #(batch_size,num_patches,num_tokens-1)\n",
        "probabilities = model.get_probabilities(logits) #(batch_size,num_patches,num_tokens-1)\n",
        "print(\"probabilities:\",probabilities)\n",
        "print(\"probabilities shape:\",probabilities.size())\n",
        "masked_probs = probabilities[mask] #(num_masked_patches,num_tokens-1)\n",
        "print(\"Masked probabilities:\", masked_probs)\n",
        "print(\"Masked probabilities shape:\",masked_probs.size())\n",
        "masked_Y = Y[mask] #(num_masked_patches,)\n",
        "print(\"Masked Y:\", masked_Y)\n",
        "print(\"Masked Y shape:\",masked_Y.size())\n",
        "loss = criterion(masked_probs, masked_Y)\n",
        "print(\"loss:\",loss)\n",
        "predicted_indices = torch.argmax(masked_probs, dim=-1)\n",
        "print(\"predicted_indices:\",predicted_indices)\n",
        "print(\"predicted_indices shape:\",predicted_indices.size())\n",
        "reconstructed = Y.clone()#(batch_size,num_patches,patch_size)\n",
        "reconstructed[mask] = predicted_indices\n",
        "print(\"reconstructed:\",reconstructed)\n",
        "print(\"reconstructed shape:\",reconstructed.size())\n",
        "# Reconstruct images\n",
        "reconstructed_image = image_from_indices_batched(reconstructed, image_shape=(4, 4))\n",
        "masked_image = image_from_indices_batched(masked_patches, image_shape=(4, 4))\n",
        "print(\"reconstructed_image:\", reconstructed_image)\n",
        "print(\"reconstructed_image shape:\", reconstructed_image.size())\n",
        "print(\"masked_image:\", masked_image)\n",
        "print(\"masked_image shape:\", masked_image.size())\n",
        "fig, axs = plt.subplots(3, figsize=(15, 5))\n",
        "axs[0].imshow(images[-1].squeeze(), cmap=\"gray\")\n",
        "axs[0].set_title(f\"Original Image\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].imshow(masked_image[-1].squeeze(), cmap=\"gray\")\n",
        "axs[1].set_title(f\"Masked Image\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[2].imshow(reconstructed_image[-1].squeeze(), cmap=\"gray\")\n",
        "axs[2].set_title(f\"Reconstructed Image\")\n",
        "axs[2].axis(\"off\")\n",
        "plt.show()\n",
        "#reconstructed_image = reconstruct_ima_from_patches(predicted_indices)\n",
        "\n",
        "#visualized_masked_patches = masked_patches.cpu()[0].clone()\n",
        "#visualized_masked_patches[visualized_masked_patches == num_tokens - 1] = -1\n",
        "#masked_image = reconstruct_image_from_patches(visualized_masked_patches)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eAp54EJAWcxb",
        "outputId": "6d17981f-7ff5-45a5-93fd-b03412aacda4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y (Original Image): tensor([[1, 2, 1, 3, 3, 3, 0, 3]])\n",
            "Y shape: torch.Size([1, 8])\n",
            "mask: tensor([[ True,  True, False,  True, False,  True, False,  True]])\n",
            "mask shape: torch.Size([1, 8])\n",
            "masked_patches (Masked Image): tensor([[4, 4, 1, 4, 3, 4, 0, 4]])\n",
            "masked_patches shape: torch.Size([1, 8])\n",
            "X (Patches): tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000],\n",
            "         [0.0000, 1.0000],\n",
            "         [0.5000, 0.5000],\n",
            "         [1.0000, 1.0000],\n",
            "         [0.5000, 0.5000],\n",
            "         [0.0000, 0.0000],\n",
            "         [0.5000, 0.5000]]])\n",
            "X shape: torch.Size([1, 8, 2])\n",
            "probabilities: tensor([[[0.1831, 0.6900, 0.0166, 0.1104],\n",
            "         [0.3578, 0.0579, 0.0854, 0.4989],\n",
            "         [0.1478, 0.2815, 0.3785, 0.1922],\n",
            "         [0.2596, 0.1772, 0.2306, 0.3326],\n",
            "         [0.0654, 0.0196, 0.6907, 0.2244],\n",
            "         [0.3567, 0.1084, 0.0756, 0.4593],\n",
            "         [0.2241, 0.0306, 0.2106, 0.5347],\n",
            "         [0.2805, 0.3343, 0.0971, 0.2880]]], grad_fn=<SoftmaxBackward0>)\n",
            "probabilities shape: torch.Size([1, 8, 4])\n",
            "Masked probabilities: tensor([[0.1831, 0.6900, 0.0166, 0.1104],\n",
            "        [0.3578, 0.0579, 0.0854, 0.4989],\n",
            "        [0.2596, 0.1772, 0.2306, 0.3326],\n",
            "        [0.3567, 0.1084, 0.0756, 0.4593],\n",
            "        [0.2805, 0.3343, 0.0971, 0.2880]], grad_fn=<IndexBackward0>)\n",
            "Masked probabilities shape: torch.Size([5, 4])\n",
            "Masked Y: tensor([1, 2, 3, 3, 3])\n",
            "Masked Y shape: torch.Size([5])\n",
            "loss: tensor(1.2798, grad_fn=<NllLossBackward0>)\n",
            "predicted_indices: tensor([1, 3, 3, 3, 1])\n",
            "predicted_indices shape: torch.Size([5])\n",
            "reconstructed: tensor([[1, 3, 1, 3, 3, 3, 0, 1]])\n",
            "reconstructed shape: torch.Size([1, 8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-6bf4a18c6f09>:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(image, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Number of patches does not match image size.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-ce1d6c26046f>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reconstructed shape:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Reconstruct images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mreconstructed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_from_indices_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mmasked_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_from_indices_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reconstructed_image:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-6bf4a18c6f09>\u001b[0m in \u001b[0;36mimage_from_indices_batched\u001b[0;34m(patch_indices, image_shape)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mpatch_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mpatch_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnum_patches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpatch_rows\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpatch_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Number of patches does not match image size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;31m# Convert indices back to patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Number of patches does not match image size."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"vision-transformer-toy-example\",\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"embed_dim\": embed_dim,\n",
        "        \"num_heads\": num_heads,\n",
        "        \"feedforward_dim\": feedforward_dim,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"num_tokens\": num_tokens,\n",
        "        \"max_patches\": max_patches,\n",
        "        \"dropout\": dropout,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"num_epochs\": num_epochs,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    current_mask_rate = 0.5\n",
        "\n",
        "    for batch_idx, images in enumerate(dataloader):\n",
        "        # Preprocess images\n",
        "        Y = torch.stack([preprocess_image_with_indices(img) for img in images]).long() #(batch_size,num_patches,)\n",
        "        X = Y.clone() #(batch_size,num_patches,)\n",
        "\n",
        "        # Masking\n",
        "        mask = torch.rand(X.shape) < 0.5 #(batch_size,num_patches,)\n",
        "        X[mask] = num_tokens - 1\n",
        "        X = indices_to_patches_batched(X) #(batch_size,num_patches,patch_size)\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "\n",
        "        # Define weighted CrossEntropyLoss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(X) #(batch_size,num_patches,num_tokens-1)\n",
        "        probabilities = model.get_probabilities(logits) #(batch_size,num_patches,num_tokens-1)\n",
        "\n",
        "        masked_probs = probabilities[mask] #(num_masked_patches,num_tokens-1)\n",
        "\n",
        "        masked_Y = Y[mask] #(num_masked_patches,)\n",
        "\n",
        "        loss = criterion(masked_probs, masked_Y)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Log batch metrics\n",
        "        wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Log epoch metrics\n",
        "        wandb.log({\"epoch_loss\": avg_loss})\n",
        "\n",
        "        # Add the 5 visualization steps here.....\n",
        "        with torch.no_grad():\n",
        "          predicted_indices = torch.argmax(logits, dim=-1).cpu()[0]\n",
        "          reconstructed_image = reconstruct_image_from_patches(predicted_indices)\n",
        "\n",
        "          visualized_masked_patches = masked_patches.cpu()[0].clone()\n",
        "          visualized_masked_patches[visualized_masked_patches == num_tokens - 1] = -1\n",
        "          masked_image = reconstruct_image_from_patches(visualized_masked_patches)\n",
        "\n",
        "          # Log visualizations to wandb\n",
        "          wandb.log({\n",
        "              \"Original Image\": wandb.Image(\n",
        "                  reconstruct_image_from_patches(patch_indices.cpu()[0])\n",
        "              ),\n",
        "              \"Masked Image\": wandb.Image(masked_image, caption=\"Masked Image\"),\n",
        "              \"Reconstructed Image\": wandb.Image(\n",
        "                  reconstructed_image, caption=\"Reconstructed Image\"\n",
        "              ),\n",
        "          })\n",
        "\n",
        "# Save the final model\n",
        "torch.save(model.state_dict(), \"vision_transformer_final_balanced.pth\")\n",
        "wandb.save(\"vision_transformer_final_balanced.pth\")\n",
        "print(\"Final model saved as 'vision_transformer_final_balanced.pth'.\")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Miu05semF-YK",
        "outputId": "29a3efd1-5ce9-4be7-c4b2-5212f5424fdf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241129_143233-glvhv78r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oscars/vision-transformer-toy-example/runs/glvhv78r' target=\"_blank\">soft-rain-3</a></strong> to <a href='https://wandb.ai/oscars/vision-transformer-toy-example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/oscars/vision-transformer-toy-example' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-toy-example</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/oscars/vision-transformer-toy-example/runs/glvhv78r' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-toy-example/runs/glvhv78r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-2fd9fe305da3>:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(image, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1] completed. Average Loss: 0.0143\n",
            "Epoch [1/1] completed. Average Loss: 0.0286\n",
            "Epoch [1/1] completed. Average Loss: 0.0430\n",
            "Epoch [1/1] completed. Average Loss: 0.0569\n",
            "Epoch [1/1] completed. Average Loss: 0.0708\n",
            "Epoch [1/1] completed. Average Loss: 0.0843\n",
            "Epoch [1/1] completed. Average Loss: 0.0982\n",
            "Epoch [1/1] completed. Average Loss: 0.1127\n",
            "Epoch [1/1] completed. Average Loss: 0.1266\n",
            "Epoch [1/1] completed. Average Loss: 0.1416\n",
            "Epoch [1/1] completed. Average Loss: 0.1561\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Epoch [1/1] completed. Average Loss: nan\n",
            "Final model saved as 'vision_transformer_final_balanced.pth'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td></td></tr><tr><td>epoch_loss</td><td>                                   </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>1.40133</td></tr><tr><td>epoch_loss</td><td>nan</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-rain-3</strong> at: <a href='https://wandb.ai/oscars/vision-transformer-toy-example/runs/glvhv78r' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-toy-example/runs/glvhv78r</a><br/> View project at: <a href='https://wandb.ai/oscars/vision-transformer-toy-example' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-toy-example</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241129_143233-glvhv78r/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fL5JfQ6DfU5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}