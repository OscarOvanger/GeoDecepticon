{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQwb7bUT6z3F",
        "outputId": "1f436b54-713b-4594-f45d-c71fc19c46ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m882.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OscarOvanger/GeoDecepticon.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZXtl1g-7FeE",
        "outputId": "77482164-c95c-49d8-e0ba-f5ed7a40496b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GeoDecepticon'...\n",
            "remote: Enumerating objects: 20025, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 20025 (delta 155), reused 131 (delta 129), pack-reused 19846 (from 3)\u001b[K\n",
            "Receiving objects: 100% (20025/20025), 107.61 MiB | 12.09 MiB/s, done.\n",
            "Resolving deltas: 100% (433/433), done.\n",
            "Updating files: 100% (79/79), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/GeoDecepticon')"
      ],
      "metadata": {
        "id": "u5fsIzjR7GZa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "from GeneralViT import VisionTransformer, create_model\n",
        "from tqdm import tqdm  # For progress bars"
      ],
      "metadata": {
        "id": "SZFtoPVzEsId"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and plot training data"
      ],
      "metadata": {
        "id": "LT8iK4NzEOg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "training_data = arr[:9000]\n",
        "test_data = arr[9000:]\n",
        "# We reshape it to 60x60 dataset\n",
        "training_data = training_data.reshape(-1, 64, 64)\n",
        "test_data = test_data.reshape(-1, 64, 64)\n",
        "training_data = training_data[:,:60,:60]\n",
        "test_data = test_data[:,:60,:60]\n",
        "print(training_data.shape)\n",
        "training_data = torch.tensor(training_data,dtype=torch.float32)\n",
        "test_data = torch.tensor(test_data,dtype=torch.float32)\n",
        "#plot the first data\n",
        "plt.imshow(training_data[0],cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ggDcm9hCEXRB",
        "outputId": "fa1e3673-85df-4872-f89a-a3a296300d00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9000, 60, 60)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIVpJREFUeJzt3X9sVFX+//HXVNoBhZkCakuXlsX4o6sGjFXKRM1moUqMMSr9gzUkS1yyRi1EqGbX/qFoskmJJrri4o+su/DPuqzdBA1u1CVVa3QLC1Ui/tgGDFlqYIZ1k86Uri2Enu8f+3U+jrSlM70z73tvn4/kJjAzvXPOuXf66pl5z7kR55wTAAAlVmbdAADA1EQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAExMK9aOt27dqqeeekrJZFKLFy/Wc889pyVLlpzz50ZGRnTs2DHNmjVLkUikWM0DABSJc04DAwOqqalRWdk48xxXBDt27HAVFRXuD3/4g/vss8/cL37xC1dZWelSqdQ5f7avr89JYmNjY2ML+NbX1zfu7/uIc94vRtrY2Kjrr79ev/3tbyX9b1ZTW1ur9evX65FHHhn3Z9PptCorK8e8bzTxeHxS7Q2qscYjX4WMX7Gfe7z9F/IzxW5TsVkeI0tevbYLOZ8sWR278cYinzZlMhnV1taqv79/3H16/hbcqVOn1NPTo7a2tuxtZWVlampqUnd391mPHx4e1vDwcPb/AwMDY+47Fot529iAsxyPYj93Ifv3Y5ssBa29xRS0sfBjewtp07k+RvG8COHrr7/WmTNnVFVVlXN7VVWVksnkWY9vb29XPB7PbrW1tV43CQDgQ+ZVcG1tbUqn09mtr6/PukkAgBLw/C24Cy+8UOedd55SqVTO7alUStXV1Wc9PhqNKhqNnnV7Op325TS0mMb6OG6saexYtxfhY70JP/dYxmqTl231Y5u8UkibrM6PQqpX8z0WYa+Q9epc9jvPZ0AVFRVqaGhQZ2dn9raRkRF1dnYqkUh4/XQAgIAqyveAWltbtWbNGl133XVasmSJfvOb32hwcFD33HNPMZ4OABBARQmgVatW6d///rcee+wxJZNJXXPNNXrrrbfOKkwAAExdRfke0GRkMhnF4/G8PgMKy/vBXr3fne8hLcX4WX7WMxafnfqeC8NnQF49x3j7D8PvD6+O6Xhjkc9zTPT3uHkVHABgairaWnClFJZKmaC1F8FU7JlR2GeW8A4zIACACQIIAGCCAAIAmCCAAAAmCCAAgAnfVsGNdg2JqVpdM1X7jckJ83njZd/CUEUb1HUQmQEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABO+LcP24pLcQVuCvRRLqlvxcgFMr8pmLS9pnq9C2jpVLutcLH68rIlXSnHJjIlgBgQAMEEAAQBMEEAAABMEEADABAEEADDh2yq4YrOqOCtFpVEpqnf8WIkW5iquMPfNWrGr17w8dpaVdsWoGmUGBAAwQQABAEwQQAAAEwQQAMAEAQQAMDFlq+C8EobqpFJc3ngsfqzqGUvQjnXQ2ouphxkQAMAEAQQAMEEAAQBMEEAAABMEEADABFVw3+NVVVYpKpCC1NZSPHeQrkhpyas1vbwc7zBX7PnxasBemsxzMwMCAJgggAAAJgggAIAJAggAYIIAAgCY8G0VXDwen/BjS7GWmeWVPsdS7LYWwo8VdV71z8srx1oKWyWV3/nxHPALZkAAABMEEADABAEEADBBAAEATBBAAAATvq2CKzarqpvxnjdIVxMdSymq/wCEAzMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGAi7wB6//33dfvtt6umpkaRSESvvfZazv3OOT322GOaN2+eZsyYoaamJh06dMir9o4qEomMuvmRc27MDcUx3pgX+ziMdW768ZzNd5wK2YrdJkt+bJPf5R1Ag4ODWrx4sbZu3Trq/U8++aS2bNmiF198UXv37tUFF1ygFStWaGhoaNKNBQCER8RNIqIjkYh27typO++8U9L//gKoqanRQw89pIcffliSlE6nVVVVpe3bt+unP/3pWfsYHh7W8PBw9v+ZTEa1tbWFNinHeF2z+mvTy7+IrP9izkeQ/hK0HNcgjVMh8h3bIH052/LY+fV3QTqdViwWG/N+Tz8DOnLkiJLJpJqamrK3xeNxNTY2qru7e9SfaW9vVzwez25ehQ8AwN88DaBkMilJqqqqyrm9qqoqe9/3tbW1KZ1OZ7e+vj4vmwQA8CnzteCi0aii0ah1MwAAJeZpAFVXV0uSUqmU5s2bl709lUrpmmuu8fKpJsSv74vmKyz9CIqp+jldGIT9M7Sx+PEy6xPh6VtwCxcuVHV1tTo7O7O3ZTIZ7d27V4lEwsunAgAEXN4zoJMnT+rw4cPZ/x85ckQHDhzQnDlzVFdXpw0bNujXv/61LrvsMi1cuFCPPvqoampqspVyAABIBQTQ/v379ZOf/CT7/9bWVknSmjVrtH37dv3yl7/U4OCg7r33XvX39+vGG2/UW2+9penTp3vXagBA4E3qe0DFkMlkFI/HrZtRNIUMt9/fx50In51mJVPs770EDeNRWta/O0r6PSAAACbKvAw7H0H6a2isvzys/yJBaeVbnVSK8yMMryM/9oFjlz9mQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOBqoKbqoK6ztN3BamaqRBBOhZ+5FW1oJevlXzPzVJcuygo36Oa6Pc5mQEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMBKoKzstKo2JXlnm56rVXlS9+rNQqRXVSKZSiAsorlud4sY+dl+Pqx9dLvvxefcoMCABgggACAJgggAAAJgggAIAJAggAYCJQVXBeKsVaUvnyS2WK3/m9sue7wlBJNZYgHYewCNvYMgMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACambBn2WMJcNht2lmXBnDfFEaSFdZE/ZkAAABMEEADABAEEADBBAAEATBBAAAATVMEViZeXmqbiB5b8uACmZZv8eDl1ry4JX+pxZQYEADBBAAEATBBAAAATBBAAwAQBBAAw4dsquHQ6rVgsVvLn9WOFC/yv2NVDnBv+EaRq1XzbVOo+MAMCAJgggAAAJgggAIAJAggAYIIAAgCY8G0VnJUgVbhgYkpx7IpdBWd5XoZh/Mbjl3XRJiOov5+YAQEATBBAAAATBBAAwAQBBAAwQQABAEzkFUDt7e26/vrrNWvWLF188cW688471dvbm/OYoaEhtbS0aO7cuZo5c6aam5uVSqU8bTTgN5FIJK/NK845s81LXo1TvsdhvOewOqZTSV4B1NXVpZaWFu3Zs0e7d+/W6dOndcstt2hwcDD7mI0bN2rXrl3q6OhQV1eXjh07ppUrV3recABAwLlJOHHihJPkurq6nHPO9ff3u/LyctfR0ZF9zBdffOEkue7u7gntM51OO0kunU5Ppmmek8TG5tkWBn4cJ+vjypa7nev3+KQ+A0qn05KkOXPmSJJ6enp0+vRpNTU1ZR9TX1+vuro6dXd3j7qP4eFhZTKZnA0AEH4FB9DIyIg2bNigG264QVdffbUkKZlMqqKiQpWVlTmPraqqUjKZHHU/7e3tisfj2a22trbQJgEAAqTgAGppadGnn36qHTt2TKoBbW1tSqfT2a2vr29S+wMABENBa8GtW7dOb7zxht5//33Nnz8/e3t1dbVOnTql/v7+nFlQKpVSdXX1qPuKRqOKRqOFNKOknEdVP35cV4sKntLz43ng1f697BvnZrjlNQNyzmndunXauXOn3nnnHS1cuDDn/oaGBpWXl6uzszN7W29vr44ePapEIuFNiwEAoZDXDKilpUWvvPKKXn/9dc2aNSv7uU48HteMGTMUj8e1du1atba2as6cOYrFYlq/fr0SiYSWLl1alA4AAALKixLHbdu2ZR/zzTffuAceeMDNnj3bnX/++e6uu+5yx48fn/Bz+LUM2ytjjaGXmx/bxFb6zYp1v9n8s53r93jk/58wvpHJZBSPx5VOpxWLxayb4zk/vvfP++zhZPXS5nzCt871e5y14AAAJgggAIAJLskNhFS+b4Xx1i1KjRkQAMAEAQQAMEEAAQBMEEAAABMEEADABFVwJVbIlwPzrTaiOgmF4LwpLcs1APxyrJkBAQBMEEAAABMEEADABAEEADBBAAEATPi2Ci4ej0/4sT67ooSksatMCmnrWD/jl0oWhJuXry/O2f9TyFj48XfdZDADAgCYIIAAACYIIACACQIIAGCCAAIAmPBtFVw+vKysKXaViWVbqUCaWsJWMYXwYQYEADBBAAEATBBAAAATBBAAwAQBBAAwEYoqOMu10kpR1ebVc7CmF0ohLOeGH69Y6uUak37ADAgAYIIAAgCYIIAAACYIIACACQIIAGAiFFVwXrKs4AlL9ZAX/FiBZClIVU6FtNWPY55vm7w8RvlWxOZbHeeXqywzAwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJkJRhu3HEk5MznjHtNglyX4sIw7LIpRheK16NeZBWyy5GJgBAQBMEEAAABMEEADABAEEADBBAAEATISiCg6wlm9llN+rkyYjzH2Twt+/UmIGBAAwQQABAEwQQAAAEwQQAMAEAQQAMJFXAL3wwgtatGiRYrGYYrGYEomE3nzzzez9Q0NDamlp0dy5czVz5kw1NzcrlUoV1LB0Oi3n3IS2qSoSiXiyofQmem6f6xz38rhyfqDU8gqg+fPna/Pmzerp6dH+/fu1bNky3XHHHfrss88kSRs3btSuXbvU0dGhrq4uHTt2TCtXrixKwwEAAecmafbs2e7ll192/f39rry83HV0dGTv++KLL5wk193dPeH9pdNpJ8ml0+kJ/4wktklshQhae4OuFONU7P2wTb3tXL/HC/4M6MyZM9qxY4cGBweVSCTU09Oj06dPq6mpKfuY+vp61dXVqbu7e8z9DA8PK5PJ5GwAgPDLO4AOHjyomTNnKhqN6r777tPOnTt15ZVXKplMqqKiQpWVlTmPr6qqUjKZHHN/7e3tisfj2a22tjbvTgAAgifvALriiit04MAB7d27V/fff7/WrFmjzz//vOAGtLW1KZ1OZ7e+vr6C9wUACI6814KrqKjQpZdeKklqaGjQvn379Oyzz2rVqlU6deqU+vv7c2ZBqVRK1dXVY+4vGo0qGo1O6LmpyCmOoI1rvu11IaiUHKsP441FvldR9WqcxttP0M41FNekvwc0MjKi4eFhNTQ0qLy8XJ2dndn7ent7dfToUSUSick+DQAgZPKaAbW1tenWW29VXV2dBgYG9Morr+i9997T22+/rXg8rrVr16q1tVVz5sxRLBbT+vXrlUgktHTp0mK1HwAQUHkF0IkTJ/Szn/1Mx48fVzwe16JFi/T222/r5ptvliQ988wzKisrU3Nzs4aHh7VixQo9//zzRWk4ACDYIs5nb5BnMhnF43Gl02nFYrGc+3j/GIXw2SnuqUJeE5bjwWt4ahnt9/h3sRYcAMBEoK6IWkglUJiV4i/ZYo9tIX3It7rLq/17KcyzsvHwGsZ3MQMCAJgggAAAJgggAIAJAggAYIIAAgCY8G0VXDwet25Cjnyrlrz8fsZY+/Kqcmi8vhW7aqmQirYwVJB5tZ6dl1WE+T63l6iOm5qYAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE74tw/Ybry5vXMgllL1SipLdfPmxpDosJcFBa+9oSvH1hzDz8vVVjLFlBgQAMEEAAQBMEEAAABMEEADABAEEADAR6iq48SpAir2YZin4sYIsX14u2lpsllWElou2+nHxUi+fO0iVc8UeW69ej5lMZkILSjMDAgCYIIAAACYIIACACQIIAGCCAAIAmAhUFVwYqr4K4cfKlzAopLIsX35cVy4MxzsMfShEsSsSS11FyAwIAGCCAAIAmCCAAAAmCCAAgAkCCABgIlBVcF7KtzqpFFdm9KpiqhSVL/nysmopSBVQxW6rH8eiFG0Kw5VSp2pV73cxAwIAmCCAAAAmCCAAgAkCCABgggACAJjwbRVcOp1WLBYr+fN6VZlSijWVgla9VmyW664FaZzCgPH2D66ICgAIHAIIAGCCAAIAmCCAAAAmCCAAgAnfVsHh3EpxRU+vlGLtLq8qo6iwQil4eZ758TU/EcyAAAAmCCAAgAkCCABgggACAJgggAAAJiYVQJs3b1YkEtGGDRuytw0NDamlpUVz587VzJkz1dzcrFQqNdl2AgBCpuAy7H379umll17SokWLcm7fuHGj/vrXv6qjo0PxeFzr1q3TypUr9eGHH+a1/4ksZPetoJYgfp9XpcpeLmrq1diWYqFVyqcnx3JxW6vzDLYKmgGdPHlSq1ev1u9+9zvNnj07e3s6ndbvf/97Pf3001q2bJkaGhq0bds2/f3vf9eePXs8azQAIPgKCqCWlhbddtttampqyrm9p6dHp0+fzrm9vr5edXV16u7uHnVfw8PDymQyORsAIPzyfgtux44d+uijj7Rv376z7ksmk6qoqFBlZWXO7VVVVUomk6Pur729XU888US+zQAABFxeM6C+vj49+OCD+uMf/6jp06d70oC2tjal0+ns1tfX58l+AQD+llcA9fT06MSJE7r22ms1bdo0TZs2TV1dXdqyZYumTZumqqoqnTp1Sv39/Tk/l0qlVF1dPeo+o9GoYrFYzgYACL+83oJbvny5Dh48mHPbPffco/r6ev3qV79SbW2tysvL1dnZqebmZklSb2+vjh49qkQiMenGhqXazSv5XoI6SONnWc3k5ThRlYUgGe98Lcbvj7wCaNasWbr66qtzbrvgggs0d+7c7O1r165Va2ur5syZo1gspvXr1yuRSGjp0qXetRoAEHieX47hmWeeUVlZmZqbmzU8PKwVK1bo+eef9/ppAAABF3E+e18mk8mM+SVUnzXVt/z4FlyQ3oqaqm/B8UXU4CrF+OXzHN/+Hk+n0+N+rs9acAAAEwQQAMCEby/JPdrUjUvYTozlWylh4MfzrJC3rvLth+Ux9eqtuan6NrNXz13qNRiZAQEATBBAAAATBBAAwAQBBAAwQQABAEz4tgounyuiFqIUV+gMM6+u3urV/gt5jjAIS5/9+PoKy9jmo5A+j3bsxltQ4LuYAQEATBBAAAATBBAAwAQBBAAwQQABAEz4tgrOb/x4iYMgKXbVnF/le96Epd9jKfbrJezjFzbMgAAAJgggAIAJAggAYIIAAgCYIIAAACaogpsky6tnTtXKPKtKqrBcfdRSsdcEZA3B0uOKqACAwCGAAAAmCCAAgAkCCABgggACAJgggAAAJnxbhp1OpxWLxXJu82O5pOUim2Eut7bsW5DGNexlx161dbxxCsPCsEHtAzMgAIAJAggAYIIAAgCYIIAAACYIIACACd9WwcXjcesmTEi+C1cGqcJqqrKsHPLy/PB7BVQphX0sgto/ZkAAABMEEADABAEEADBBAAEATBBAAAATvq2CCwqq2sInqOtqAUHDDAgAYIIAAgCYIIAAACYIIACACQIIAGCCKrhJynctOBRPkKrULK+ki+AK23nDDAgAYIIAAgCYIIAAACYIIACACQIIAGAirwB6/PHHFYlEcrb6+vrs/UNDQ2ppadHcuXM1c+ZMNTc3K5VKed5ohMf3z6fJbIBzbtQN/pT3DOiqq67S8ePHs9sHH3yQvW/jxo3atWuXOjo61NXVpWPHjmnlypWeNhgAEA55fw9o2rRpqq6uPuv2dDqt3//+93rllVe0bNkySdK2bdv0ox/9SHv27NHSpUsn31oAQGjkPQM6dOiQampqdMkll2j16tU6evSoJKmnp0enT59WU1NT9rH19fWqq6tTd3f3mPsbHh5WJpPJ2QAA4ZdXADU2Nmr79u1666239MILL+jIkSO66aabNDAwoGQyqYqKClVWVub8TFVVlZLJ5Jj7bG9vVzwez261tbUFdQQAECx5vQV36623Zv+9aNEiNTY2asGCBXr11Vc1Y8aMghrQ1tam1tbW7P8zmQwhBABTwKTWgqusrNTll1+uw4cP6+abb9apU6fU39+fMwtKpVKjfmb0rWg0qmg0OplmlASVNJNDlVouxiO4LK+Ym+9zePl7qxj9m9T3gE6ePKkvv/xS8+bNU0NDg8rLy9XZ2Zm9v7e3V0ePHlUikZh0QwEA4ZLXDOjhhx/W7bffrgULFujYsWPatGmTzjvvPN19992Kx+Nau3atWltbNWfOHMViMa1fv16JRIIKOADAWfIKoK+++kp33323/vOf/+iiiy7SjTfeqD179uiiiy6SJD3zzDMqKytTc3OzhoeHtWLFCj3//PNFaTgAINgizmcfbmQyGcXjcetmnMVnwxQ4fOaBUijF5zOWnwHly/ozoHQ6rVgsNub9rAUHADBBAAEATHBJ7gni0tuT48e3LcJ2eeNiCdI4+bEU2kuWv2/yee6JfpTCDAgAYIIAAgCYIIAAACYIIACACQIIAGBiylbBUb1WHEGqFPNjW/1YLZhvBWghry0/Hgs/Cls1LjMgAIAJAggAYIIAAgCYIIAAACYIIACAiSlbBRe2apJSy3f8wlDlVIrqrlKME+e4/02VY8QMCABgggACAJgggAAAJgggAIAJAggAYCLUVXBTpZLET8JQ7TaWQvrm1Tno5bjmu69SvI6CdNXVUghSle5kjgUzIACACQIIAGCCAAIAmCCAAAAmCCAAgAnfVsGl02nFYrEJPXasKozxqjOKXXXjx2oVL4V5zTdLrAWXi/MpV5Cq4yaCGRAAwAQBBAAwQQABAEwQQAAAEwQQAMCEb6vg8lFIRRbVNZNT7PErxdVHSyHfqiUvq5kKqQ7Nhx/XiBuLH88NL1lWx432HJlMRvF4/Jw/ywwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgIRRn2WMYrQSx2WaYfFw30sk1eLUYalnLrsQR1kcjvKkUfgnRM/ciPv1cmghkQAMAEAQQAMEEAAQBMEEAAABMEEADARKir4JDLj4tH+rH6qRQLglpWLQW1Ysrvpupl6lmMFAAQOAQQAMAEAQQAMEEAAQBM+K4I4dsPtDKZjHFLiiOs/QqTUhwjy/OAc7A4OKb/59v2nKvgJeJ8tljVV199pdraWutmAAAmqa+vT/Pnzx/zft8F0MjIiI4dO6ZZs2ZpYGBAtbW16uvrUywWs25ayWQyGfo9Rfo9FfssTc1+T6U+O+c0MDCgmpoalZWN/UmP796CKysryybmt/XzsVgs9AdsNPR76piKfZamZr+nSp/5HhAAwLcIIACACV8HUDQa1aZNmxSNRq2bUlL0e+r0eyr2WZqa/Z6KfT4X3xUhAACmBl/PgAAA4UUAAQBMEEAAABMEEADABAEEADDh6wDaunWrfvjDH2r69OlqbGzUP/7xD+smeer999/X7bffrpqaGkUiEb322ms59zvn9Nhjj2nevHmaMWOGmpqadOjQIZvGeqS9vV3XX3+9Zs2apYsvvlh33nmnent7cx4zNDSklpYWzZ07VzNnzlRzc7NSqZRRi73xwgsvaNGiRdlvwScSCb355pvZ+8PY5+/bvHmzIpGINmzYkL0tjP1+/PHHFYlEcrb6+vrs/WHsc6F8G0B//vOf1draqk2bNumjjz7S4sWLtWLFCp04ccK6aZ4ZHBzU4sWLtXXr1lHvf/LJJ7Vlyxa9+OKL2rt3ry644AKtWLFCQ0NDJW6pd7q6utTS0qI9e/Zo9+7dOn36tG655RYNDg5mH7Nx40bt2rVLHR0d6urq0rFjx7Ry5UrDVk/e/PnztXnzZvX09Gj//v1atmyZ7rjjDn322WeSwtnn79q3b59eeuklLVq0KOf2sPb7qquu0vHjx7PbBx98kL0vrH0uiPOpJUuWuJaWluz/z5w542pqalx7e7thq4pHktu5c2f2/yMjI666uto99dRT2dv6+/tdNBp1f/rTnwxaWBwnTpxwklxXV5dz7n99LC8vdx0dHdnHfPHFF06S6+7utmpmUcyePdu9/PLLoe/zwMCAu+yyy9zu3bvdj3/8Y/fggw8658J7rDdt2uQWL1486n1h7XOhfDkDOnXqlHp6etTU1JS9raysTE1NTeru7jZsWekcOXJEyWQyZwzi8bgaGxtDNQbpdFqSNGfOHElST0+PTp8+ndPv+vp61dXVhabfZ86c0Y4dOzQ4OKhEIhH6Pre0tOi2227L6Z8U7mN96NAh1dTU6JJLLtHq1at19OhRSeHucyF8txq2JH399dc6c+aMqqqqcm6vqqrSP//5T6NWlVYymZSkUcfg2/uCbmRkRBs2bNANN9ygq6++WtL/+l1RUaHKysqcx4ah3wcPHlQikdDQ0JBmzpypnTt36sorr9SBAwdC2+cdO3boo48+0r59+866L6zHurGxUdu3b9cVV1yh48eP64knntBNN92kTz/9NLR9LpQvAwhTQ0tLiz799NOc98fD7IorrtCBAweUTqf1l7/8RWvWrFFXV5d1s4qmr69PDz74oHbv3q3p06dbN6dkbr311uy/Fy1apMbGRi1YsECvvvqqZsyYYdgy//HlW3AXXnihzjvvvLMqQ1KplKqrq41aVVrf9jOsY7Bu3Tq98cYbevfdd3OumFhdXa1Tp06pv78/5/Fh6HdFRYUuvfRSNTQ0qL29XYsXL9azzz4b2j739PToxIkTuvbaazVt2jRNmzZNXV1d2rJli6ZNm6aqqqpQ9vv7Kisrdfnll+vw4cOhPdaF8mUAVVRUqKGhQZ2dndnbRkZG1NnZqUQiYdiy0lm4cKGqq6tzxiCTyWjv3r2BHgPnnNatW6edO3fqnXfe0cKFC3Pub2hoUHl5eU6/e3t7dfTo0UD3ezQjIyMaHh4ObZ+XL1+ugwcP6sCBA9ntuuuu0+rVq7P/DmO/v+/kyZP68ssvNW/evNAe64JZV0GMZceOHS4ajbrt27e7zz//3N17772usrLSJZNJ66Z5ZmBgwH388cfu448/dpLc008/7T7++GP3r3/9yznn3ObNm11lZaV7/fXX3SeffOLuuOMOt3DhQvfNN98Yt7xw999/v4vH4+69995zx48fz27//e9/s4+57777XF1dnXvnnXfc/v37XSKRcIlEwrDVk/fII4+4rq4ud+TIEffJJ5+4Rx55xEUiEfe3v/3NORfOPo/mu1VwzoWz3w899JB777333JEjR9yHH37ompqa3IUXXuhOnDjhnAtnnwvl2wByzrnnnnvO1dXVuYqKCrdkyRK3Z88e6yZ56t1333WSztrWrFnjnPtfKfajjz7qqqqqXDQadcuXL3e9vb22jZ6k0foryW3bti37mG+++cY98MADbvbs2e788893d911lzt+/Lhdoz3w85//3C1YsMBVVFS4iy66yC1fvjwbPs6Fs8+j+X4AhbHfq1atcvPmzXMVFRXuBz/4gVu1apU7fPhw9v4w9rlQXA8IAGDCl58BAQDCjwACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm/h/6PgvnEDbHewAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT o3-mini-high approach"
      ],
      "metadata": {
        "id": "Q4DxmMjLIz6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "########################################\n",
        "# Helper: Convert Patches to Integer Code\n",
        "########################################\n",
        "\n",
        "def patch_to_int(patches):\n",
        "    \"\"\"\n",
        "    Converts a tensor of patches (shape: [N, patch_dim]) containing binary values\n",
        "    to an integer code. Assumes values are near 0 or 1.\n",
        "    \"\"\"\n",
        "    patches_bin = patches.round().long()  # ensure binary\n",
        "    patch_dim = patches_bin.shape[1]\n",
        "    powers = (2 ** torch.arange(patch_dim - 1, -1, -1, device=patches.device)).unsqueeze(0)\n",
        "    codes = (patches_bin * powers).sum(dim=1)\n",
        "    return codes\n",
        "\n",
        "########################################\n",
        "# Helper: Image Sampling (Conditional)\n",
        "########################################\n",
        "\n",
        "def sample_image_conditional(model, patch_size, image_size, temperature=1.0, condition_indices=None, condition_values=None):\n",
        "    \"\"\"\n",
        "    Generate one image using the autoregressive model while enforcing conditions.\n",
        "    This function is largely unchanged from before.\n",
        "\n",
        "    Returns:\n",
        "      - generated: a tensor of shape (1, num_patches, patch_dim)\n",
        "      - log_likelihood: the sum of log-probabilities for the sampled masked tokens.\n",
        "    \"\"\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_dim = patch_size * patch_size\n",
        "\n",
        "    # Initialize with the mask token.\n",
        "    generated = model.mask_token.detach().clone().unsqueeze(0).repeat(num_patches, 1)\n",
        "    log_likelihood = 0.0\n",
        "\n",
        "    # Pre-compute patch-level conditions.\n",
        "    patch_conditions = {}\n",
        "    if condition_indices is not None and condition_values is not None:\n",
        "        for cond_idx, cond_val in zip(condition_indices, condition_values):\n",
        "            global_row = int(cond_idx) // image_size\n",
        "            global_col = int(cond_idx) % image_size\n",
        "            patch_row = global_row // patch_size\n",
        "            patch_col = global_col // patch_size\n",
        "            num_patches_per_row = image_size // patch_size\n",
        "            patch_index = patch_row * num_patches_per_row + patch_col\n",
        "            local_row = global_row % patch_size\n",
        "            local_col = global_col % patch_size\n",
        "            local_index = local_row * patch_size + local_col\n",
        "            if patch_index not in patch_conditions:\n",
        "                patch_conditions[patch_index] = {}\n",
        "            patch_conditions[patch_index][local_index] = float(cond_val)\n",
        "\n",
        "    generated = generated.unsqueeze(0)  # shape: (1, num_patches, patch_dim)\n",
        "    for i in range(num_patches):\n",
        "        cond = patch_conditions.get(i, None)\n",
        "        logits = model(generated)  # shape: (1, num_patches, vocab_size)\n",
        "        logits_i = logits[0, i] / temperature\n",
        "        if cond is not None:\n",
        "            candidate_mask = torch.ones(model.vocab_size, dtype=torch.bool, device=logits_i.device)\n",
        "            for local_idx, cond_val in cond.items():\n",
        "                candidate_mask = candidate_mask & (model.vocab[:, local_idx] == cond_val)\n",
        "            logits_i = logits_i.masked_fill(~candidate_mask, -1e9)\n",
        "        probs = torch.softmax(logits_i, dim=-1)\n",
        "        token = torch.multinomial(probs, num_samples=1)\n",
        "        log_prob = torch.log(probs[token] + 1e-10)\n",
        "        log_likelihood += log_prob.item()\n",
        "        patch = model.vocab[token]\n",
        "        generated[0, i] = patch\n",
        "    return generated, log_likelihood\n",
        "\n",
        "########################################\n",
        "# Vision Transformer and Transformer Encoder Layer\n",
        "########################################\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(hidden_dim, num_heads, ffn_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def build_vocabulary(self, training_data, patch_size, full_mask=True, one_mask=True):\n",
        "        self.patch_size = patch_size\n",
        "        patch_dim = patch_size * patch_size\n",
        "        H, W = training_data.shape[1], training_data.shape[2]\n",
        "        num_patches = (H // patch_size) * (W // patch_size)\n",
        "        patches_tensor = training_data.unfold(1, patch_size, patch_size)\\\n",
        "                                        .unfold(2, patch_size, patch_size)\n",
        "        patches_tensor = patches_tensor.contiguous().view(-1, patch_dim)\n",
        "        unique_set = set()\n",
        "        unique_list = []\n",
        "        for patch in tqdm(patches_tensor, desc=f\"Building Vocabulary for {patch_size}x{patch_size}\", total=patches_tensor.shape[0]):\n",
        "            patch_cpu = patch.cpu()\n",
        "            patch_tuple = tuple(patch_cpu.tolist())\n",
        "            if patch_tuple not in unique_set:\n",
        "                unique_set.add(patch_tuple)\n",
        "                unique_list.append(patch_cpu)\n",
        "        if full_mask:\n",
        "            unique_list.append(torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu'))\n",
        "        if one_mask:\n",
        "            for i in range(patch_dim):\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 0.0\n",
        "                unique_list.append(mask_patch)\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 1.0\n",
        "                unique_list.append(mask_patch)\n",
        "        self.vocab = torch.stack(unique_list)\n",
        "        self.vocab_size = self.vocab.size(0)\n",
        "        self.vocab_int = patch_to_int(self.vocab)\n",
        "        self.embedding_projection = nn.Linear(patch_dim, self.hidden_dim)\n",
        "        self.output_projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, self.hidden_dim))\n",
        "        self.mask_token = nn.Parameter(torch.full((patch_dim,), 0.5, dtype=torch.float))\n",
        "        return self.vocab\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # patches: (B, num_patches, patch_dim)\n",
        "        B, num_patches, _ = patches.shape\n",
        "        embeddings = self.embedding_projection(patches)\n",
        "        embeddings = embeddings + self.pos_embedding[:, :num_patches, :]\n",
        "        z = self.dropout_layer(embeddings)\n",
        "        for layer in self.transformer_layers:\n",
        "            z = layer(z)\n",
        "        logits = self.output_projection(z)  # (B, num_patches, vocab_size)\n",
        "        return logits\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(hidden_dim, ffn_dim)\n",
        "        self.linear2 = nn.Linear(ffn_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, z):\n",
        "        z_norm = self.norm1(z)\n",
        "        z_t = z_norm.transpose(0, 1)\n",
        "        attn_output, _ = self.self_attn(z_t, z_t, z_t)\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "        z = z + self.dropout1(attn_output)\n",
        "        z_norm = self.norm2(z)\n",
        "        ff_output = self.linear2(self.dropout2(self.activation(self.linear1(z_norm))))\n",
        "        z = z + self.dropout2(ff_output)\n",
        "        return z\n",
        "\n",
        "########################################\n",
        "# Dataset and Patch Extraction\n",
        "########################################\n",
        "\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images  # shape: (N, H, W)\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "    @staticmethod\n",
        "    def batch_to_patches(images, patch_size):\n",
        "        B, H, W = images.shape\n",
        "        patches = images.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "        patches = patches.contiguous().view(B, -1, patch_size * patch_size)\n",
        "        return patches\n",
        "    @staticmethod\n",
        "    def patches_to_image(patches, patch_size, image_size):\n",
        "        # patches: (num_patches, patch_dim) ordered in raster scan.\n",
        "        image = torch.zeros(image_size, image_size, device=patches.device)\n",
        "        idx = 0\n",
        "        for i in range(0, image_size, patch_size):\n",
        "            for j in range(0, image_size, patch_size):\n",
        "                image[i:i+patch_size, j:j+patch_size] = patches[idx].view(patch_size, patch_size)\n",
        "                idx += 1\n",
        "        return image\n",
        "\n",
        "########################################\n",
        "# Training Pipeline (Loss only on masked tokens)\n",
        "########################################\n",
        "\n",
        "def train_model_for_patch(patch_size, training_data, num_epochs=1000, batch_size=128):\n",
        "    # Increased model capacity.\n",
        "    num_heads = 8\n",
        "    num_layers = 8\n",
        "    ffn_dim = 1024\n",
        "    hidden_dim = 256\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    training_data = training_data.to(device)\n",
        "    image_size = training_data.shape[1]\n",
        "\n",
        "    wandb.init(project=\"vision-transformer-comparison\",\n",
        "               config={\n",
        "                   \"num_heads\": num_heads,\n",
        "                   \"num_layers\": num_layers,\n",
        "                   \"ffn_dim\": ffn_dim,\n",
        "                   \"hidden_dim\": hidden_dim,\n",
        "                   \"patch_size\": patch_size,\n",
        "                   \"num_epochs\": num_epochs,\n",
        "                   \"batch_size\": batch_size\n",
        "               },\n",
        "               name=f\"patch_{patch_size}x{patch_size}\",\n",
        "               reinit=True)\n",
        "\n",
        "    model = VisionTransformer(num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.1)\n",
        "    model.build_vocabulary(training_data, patch_size)\n",
        "    model = model.to(device)\n",
        "    model.vocab = model.vocab.to(device)\n",
        "    model.vocab_int = patch_to_int(model.vocab).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = BinaryImageDataset(training_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        # Gradual mask schedule.\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        min_mask_rate = 1.0 / num_patches\n",
        "        base_mask_rate = min_mask_rate + (1.0 - min_mask_rate) * (epoch / num_epochs)\n",
        "        mask_rate = np.clip(base_mask_rate + np.random.uniform(-0.1, 0.1), min_mask_rate, 1.0)\n",
        "        partial_mask_rate = 0.3\n",
        "\n",
        "        epoch_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (patch {patch_size}x{patch_size})\", leave=False)\n",
        "        for batch in epoch_bar:\n",
        "            batch = batch.to(device)\n",
        "            B = batch.shape[0]\n",
        "            patches = BinaryImageDataset.batch_to_patches(batch, patch_size)  # (B, num_patches, patch_dim)\n",
        "            patch_dim = patches.shape[2]\n",
        "\n",
        "            # Create mask for each patch (only masked tokens contribute to loss).\n",
        "            mask = torch.rand(B, num_patches, device=device) < mask_rate\n",
        "            mask[:, 0] = True\n",
        "\n",
        "            partial = (torch.rand(B, num_patches, device=device) < partial_mask_rate) & mask\n",
        "            full = mask & (~partial)\n",
        "\n",
        "            masked_patches = patches.clone()\n",
        "            if full.any():\n",
        "                masked_patches[full] = model.mask_token\n",
        "            if partial.any():\n",
        "                partial_idx = torch.nonzero(partial)\n",
        "                num_partial = partial_idx.shape[0]\n",
        "                new_patches = torch.full((num_partial, patch_dim), 0.5, device=device)\n",
        "                rand_positions = torch.randint(0, patch_dim, (num_partial,), device=device)\n",
        "                orig_vals = patches[partial_idx[:, 0], partial_idx[:, 1], :].gather(1, rand_positions.unsqueeze(1)).squeeze(1)\n",
        "                new_patches[torch.arange(num_partial), rand_positions] = orig_vals\n",
        "                masked_patches[partial_idx[:, 0], partial_idx[:, 1]] = new_patches\n",
        "\n",
        "            # Forward pass.\n",
        "            logits = model(masked_patches)  # (B, num_patches, vocab_size)\n",
        "\n",
        "            # Compute targets over all patches.\n",
        "            all_patches = patches.view(B * num_patches, patch_dim)\n",
        "            codes = patch_to_int(all_patches)\n",
        "            vocab_codes = model.vocab_int\n",
        "            mask_eq = codes.unsqueeze(1) == vocab_codes.unsqueeze(0)\n",
        "            target_indices = mask_eq.float().argmax(dim=1)\n",
        "            targets_all = target_indices.view(B, num_patches)\n",
        "\n",
        "            # Now compute loss only over masked tokens.\n",
        "            if mask.sum() > 0:\n",
        "                loss = criterion(logits[mask], targets_all[mask])\n",
        "            else:\n",
        "                loss = torch.tensor(0.0, device=device)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            epoch_bar.set_postfix(loss=loss.item(), mask_rate=mask_rate)\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": avg_loss, \"mask_rate\": mask_rate})\n",
        "        print(f\"[Patch {patch_size}x{patch_size}] Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Mask Rate: {mask_rate:.2f}\")\n",
        "\n",
        "        # --- Visualization on First Batch ---\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            batch0 = next(iter(dataloader)).to(device)\n",
        "            patches0 = BinaryImageDataset.batch_to_patches(batch0, patch_size)\n",
        "            B0, num_patches, patch_dim = patches0.shape\n",
        "            # Create visualization mask (same as training mask for this batch).\n",
        "            vis_mask = (torch.rand(1, num_patches, device=device) < mask_rate).squeeze(0)\n",
        "            vis_mask[0] = True  # ensure at least one mask.\n",
        "            vis_partial = (torch.rand(1, num_patches, device=device) < partial_mask_rate).squeeze(0) & vis_mask\n",
        "            vis_full = vis_mask & (~vis_partial)\n",
        "            masked_vis = patches0[0].clone()\n",
        "            if vis_full.any():\n",
        "                masked_vis[vis_full] = model.mask_token\n",
        "            if vis_partial.any():\n",
        "                partial_idx_vis = torch.nonzero(vis_partial)\n",
        "                num_partial_vis = partial_idx_vis.shape[0]\n",
        "                new_patches_vis = torch.full((num_partial_vis, patch_dim), 0.5, device=device)\n",
        "                rand_positions_vis = torch.randint(0, patch_dim, (num_partial_vis,), device=device)\n",
        "                orig_vals_vis = patches0[0, partial_idx_vis[:, 0], :].gather(1, rand_positions_vis.unsqueeze(1)).squeeze(1)\n",
        "                new_patches_vis[torch.arange(num_partial_vis), rand_positions_vis] = orig_vals_vis\n",
        "                masked_vis[partial_idx_vis[:, 0]] = new_patches_vis\n",
        "            # For visualization, reconstruct only the masked patches.\n",
        "            logits_vis = model(masked_vis.unsqueeze(0))\n",
        "            predicted_tokens = torch.argmax(logits_vis, dim=-1).squeeze(0)\n",
        "            recon_vis = patches0[0].clone()  # start with original\n",
        "            # Replace only masked positions with model's predictions.\n",
        "            recon_vis[vis_mask] = model.vocab[predicted_tokens[vis_mask]]\n",
        "            masked_img = BinaryImageDataset.patches_to_image(masked_vis, patch_size, image_size)\n",
        "            recon_img = BinaryImageDataset.patches_to_image(recon_vis, patch_size, image_size)\n",
        "            original_img = BinaryImageDataset.patches_to_image(patches0[0], patch_size, image_size)\n",
        "            wandb.log({\n",
        "                \"original\": wandb.Image(original_img.cpu()),\n",
        "                \"masked\": wandb.Image(masked_img.cpu()),\n",
        "                \"reconstructed\": wandb.Image(recon_img.cpu())\n",
        "            })\n",
        "\n",
        "        # --- Every 100 Epochs: Conditional Sampling ---\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            condition_indices = np.array([876,3825,2122,2892,1556,2683,3667,1767,483,2351,2000,3312,\n",
        "                                          2953,289,2373,2720,872,2713,1206,1341,3541,2226,3423,1904,\n",
        "                                          2882,2540,1497,2524,264,1441])\n",
        "            condition_values = np.array([0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,1,0,1,1,0,1])\n",
        "            gen, ll = sample_image_conditional(model, patch_size, image_size, temperature=1.0,\n",
        "                                               condition_indices=condition_indices,\n",
        "                                               condition_values=condition_values)\n",
        "            gen_img = BinaryImageDataset.patches_to_image(gen[0], patch_size, image_size)\n",
        "            wandb.log({\"conditional_sample\": wandb.Image(gen_img.cpu(), caption=f\"LL: {ll:.2f}\")})\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            torch.save(model.state_dict(), f\"vision_transformer_patch{patch_size}x{patch_size}_epoch{epoch+1}.pth\")\n",
        "    wandb.finish()\n",
        "    return model\n",
        "\n",
        "########################################\n",
        "# Main: Train the Model\n",
        "########################################\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "    training_data = arr[:9000]\n",
        "    training_data = training_data.reshape(-1, 64, 64)  # 60x60 images; adjust if needed.\n",
        "    print(\"Training data shape:\", training_data.shape)\n",
        "    training_data = torch.tensor(training_data, dtype=torch.float32)\n",
        "    patch_size = 4\n",
        "    model = train_model_for_patch(patch_size, training_data)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Nz9OqyiXI5ln",
        "outputId": "46584de2-4724-4524-a897-25b8c5cdb35e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\\n    training_data = arr[:9000]\\n    training_data = training_data.reshape(-1, 64, 64)  # 60x60 images; adjust if needed.\\n    print(\"Training data shape:\", training_data.shape)\\n    training_data = torch.tensor(training_data, dtype=torch.float32)\\n    patch_size = 4\\n    model = train_model_for_patch(patch_size, training_data)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')"
      ],
      "metadata": {
        "id": "ikvct5VLoZNC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# =============================================================================\n",
        "# Assumptions:\n",
        "# - VisionTransformer, TransformerEncoderLayer, patch_to_int,\n",
        "#   sample_image_conditional, and BinaryImageDataset with patches_to_image()\n",
        "#   are defined exactly as in your training code.\n",
        "# - The vocabulary (vocab) was saved during training to \"vocab.pt\"\n",
        "# =============================================================================\n",
        "\n",
        "# Parameters.\n",
        "patch_size = 4\n",
        "image_size = 64  # note: training images are 64x64 now (adjust if needed)\n",
        "\n",
        "# Provided condition indices and values (global pixel indices over a flattened 64x64 image)\n",
        "condition_indices = np.array([876,3825,2122,2892,1556,2683,3667,1767,483,2351,\n",
        "                                2000,3312,2953,289,2373,2720,872,2713,1206,1341,\n",
        "                                3541,2226,3423,1904,2882,2540,1497,2524,264,1441])\n",
        "condition_values = np.array([0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,\n",
        "                               1,1,1,0,1,0,1,1,0,1])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Build the model architecture with the same hyperparameters as training.\n",
        "num_heads = 8\n",
        "num_layers = 8\n",
        "ffn_dim = 1024\n",
        "hidden_dim = 256\n",
        "dropout = 0.1\n",
        "\n",
        "model = VisionTransformer(num_heads, num_layers, ffn_dim, hidden_dim, dropout)\n",
        "\n",
        "arr = np.load(\"GeoDecepticon/Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "training_data = arr[:9000]\n",
        "training_data = training_data.reshape(-1, 64, 64)  # 60x60 images; adjust if needed.\n",
        "print(\"Training data shape:\", training_data.shape)\n",
        "training_data = torch.tensor(training_data, dtype=torch.float32)\n",
        "# Load the saved vocabulary so that the output projection has the correct dimensions.\n",
        "vocab = model.build_vocabulary(training_data,patch_size)\n",
        "model.vocab = vocab.to(device)\n",
        "model.vocab_int = patch_to_int(model.vocab).to(device)\n",
        "# Rebuild the output projection to match the vocab size.\n",
        "model.output_projection = nn.Linear(model.hidden_dim, model.vocab.size(0)).to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "# List of checkpoint epochs from which to generate samples.\n",
        "checkpoint_epochs = list(range(300, 801, 100))\n",
        "checkpoint_dir = \"\"  # Adjust if needed\n",
        "\n",
        "# Create directory to save sample files.\n",
        "save_dir = \"./conditional_samples/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for epoch in checkpoint_epochs:\n",
        "    ckpt_path = os.path.join(checkpoint_dir, f\"vision_transformer_patch{patch_size}x{patch_size}_epoch{epoch} (1).pth\")\n",
        "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
        "    state_dict = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "\n",
        "    sample_imgs = []  # to store generated images (as tensors of shape (image_size, image_size))\n",
        "    ll_values = []    # to store log likelihood values for each sample\n",
        "\n",
        "    # Generate 1000 conditional samples.\n",
        "    for i in range(1000):\n",
        "        # sample_image_conditional is assumed to generate one image (in patch form)\n",
        "        generated, ll = sample_image_conditional(model, patch_size, image_size,\n",
        "                                                   temperature=1.0,\n",
        "                                                   condition_indices=condition_indices,\n",
        "                                                   condition_values=condition_values)\n",
        "        # Convert patches (generated[0]) back to an image.\n",
        "        gen_img = BinaryImageDataset.patches_to_image(generated[0], patch_size, image_size)\n",
        "        sample_imgs.append(gen_img.cpu())  # store on CPU\n",
        "        ll_values.append(ll)\n",
        "\n",
        "    # Save the 1000 samples and their log-likelihoods in a compressed npz file.\n",
        "    np.savez(os.path.join(save_dir, f\"samples_epoch{epoch}.npz\"),\n",
        "             images=torch.stack(sample_imgs).numpy(),\n",
        "             log_likelihoods=np.array(ll_values))\n",
        "    print(f\"Saved 1000 samples for epoch {epoch}.\")\n",
        "\n",
        "    # Plot the first 5 samples.\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    fig.suptitle(f\"Epoch {epoch} Conditional Samples\\nLog-likelihoods: \" +\n",
        "                 \", \".join(f\"{ll_values[i]:.2f}\" for i in range(5)), fontsize=14)\n",
        "    # For each plotted sample, overlay scatter points showing the generated value at condition indices.\n",
        "    # Convert condition indices to (x, y) coordinates.\n",
        "    x_coords = condition_indices % image_size\n",
        "    y_coords = condition_indices // image_size\n",
        "    for ax, img in zip(axes.flat, sample_imgs[:5]):\n",
        "        img_np = img.numpy()\n",
        "        ax.imshow(img_np, cmap='gray', vmin=0, vmax=1)\n",
        "        # Extract generated pixel values at condition locations.\n",
        "        gen_cond_vals = img_np[y_coords, x_coords]\n",
        "        # Red dot for pixel value 1, blue for 0.\n",
        "        red_mask = gen_cond_vals == 1\n",
        "        blue_mask = gen_cond_vals == 0\n",
        "        ax.scatter(x_coords[red_mask], y_coords[red_mask], color='red', s=20, marker='o')\n",
        "        ax.scatter(x_coords[blue_mask], y_coords[blue_mask], color='blue', s=20, marker='o')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "ZbGFCVl0iSy9",
        "outputId": "ddc95b5c-73af-4f5d-9fa4-f77ea2c4e297"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (9000, 64, 64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Vocabulary for 4x4: 100%|██████████| 2304000/2304000 [00:14<00:00, 159569.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint: vision_transformer_patch4x4_epoch300 (1).pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-61962f782dc3>:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vision_transformer_patch4x4_epoch300 (1).pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-61962f782dc3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"vision_transformer_patch{patch_size}x{patch_size}_epoch{epoch} (1).pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading checkpoint: {ckpt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vision_transformer_patch4x4_epoch300 (1).pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Claude 3.7 Sonnet Thinking approach"
      ],
      "metadata": {
        "id": "LQPqfTU9JaCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "\n",
        "########################################\n",
        "# Helper: Convert Patches to Integer Code\n",
        "########################################\n",
        "\n",
        "def patch_to_int(patches):\n",
        "    \"\"\"\n",
        "    Converts a tensor of patches (shape: [N, patch_dim]) containing binary values\n",
        "    to an integer code. Assumes values are near 0 or 1.\n",
        "    \"\"\"\n",
        "    patches_bin = patches.round().long()  # ensure binary\n",
        "    patch_dim = patches_bin.shape[1]\n",
        "    powers = (2 ** torch.arange(patch_dim - 1, -1, -1, device=patches.device)).unsqueeze(0)\n",
        "    codes = (patches_bin * powers).sum(dim=1)\n",
        "    return codes\n",
        "\n",
        "########################################\n",
        "# Helper: Image sampling with improvements\n",
        "########################################\n",
        "\n",
        "def sample_image_conditional(model, patch_size, image_size, temperature=1.0,\n",
        "                            condition_indices=None, condition_values=None,\n",
        "                            top_k=40, adaptive_temp=True):\n",
        "    \"\"\"\n",
        "    Generate one image using the autoregressive model while enforcing conditions.\n",
        "\n",
        "    Args:\n",
        "        model: The trained VisionTransformer model\n",
        "        patch_size: Size of each patch (e.g., 3 for 3x3 patches)\n",
        "        image_size: Size of the image (e.g., 60 for 60x60 images)\n",
        "        temperature: Base temperature for sampling (higher = more diverse)\n",
        "        condition_indices: numpy array of global pixel indices (from a flattened image)\n",
        "        condition_values: numpy array of binary values corresponding to these indices\n",
        "        top_k: If > 0, use top-k filtering to prevent low-probability tokens\n",
        "        adaptive_temp: If True, decrease temperature as generation progresses\n",
        "\n",
        "    Returns: (generated, log_likelihood)\n",
        "      - generated: a tensor of shape (1, num_patches, patch_dim)\n",
        "      - log_likelihood: the sum of log-probabilities for each patch token\n",
        "    \"\"\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_dim = patch_size * patch_size\n",
        "\n",
        "    # Initialize generated patches with the model's mask token\n",
        "    generated = model.mask_token.detach().clone().unsqueeze(0).repeat(num_patches, 1)  # (num_patches, patch_dim)\n",
        "    log_likelihood = 0.0\n",
        "\n",
        "    # Pre-compute patch-level conditions\n",
        "    patch_conditions = {}\n",
        "    if condition_indices is not None and condition_values is not None:\n",
        "        for cond_idx, cond_val in zip(condition_indices, condition_values):\n",
        "            # Convert global pixel index to (row, col)\n",
        "            global_row = int(cond_idx) // image_size\n",
        "            global_col = int(cond_idx) % image_size\n",
        "            # Determine which patch this falls into\n",
        "            patch_row = global_row // patch_size\n",
        "            patch_col = global_col // patch_size\n",
        "            num_patches_per_row = image_size // patch_size\n",
        "            patch_index = patch_row * num_patches_per_row + patch_col\n",
        "            # Determine local index within the patch\n",
        "            local_row = global_row % patch_size\n",
        "            local_col = global_col % patch_size\n",
        "            local_index = local_row * patch_size + local_col\n",
        "            if patch_index not in patch_conditions:\n",
        "                patch_conditions[patch_index] = {}\n",
        "            patch_conditions[patch_index][local_index] = float(cond_val)\n",
        "\n",
        "    generated = generated.unsqueeze(0)  # shape: (1, num_patches, patch_dim)\n",
        "\n",
        "    # Track generation time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Sequentially generate patch tokens in raster-scan order\n",
        "    for i in range(num_patches):\n",
        "        # If conditions exist for this patch, get them\n",
        "        cond = patch_conditions.get(i, None)\n",
        "\n",
        "        # Forward pass: compute logits for the entire sequence\n",
        "        with torch.no_grad(), autocast():\n",
        "            logits = model(generated)  # shape: (1, num_patches, vocab_size)\n",
        "            logits_i = logits[0, i]  # (vocab_size,)\n",
        "\n",
        "            # Apply adaptive temperature scaling if enabled\n",
        "            if adaptive_temp:\n",
        "                # Start high, gradually reduce (diversity → quality)\n",
        "                current_temp = temperature * (1.0 - 0.3 * (i / num_patches))\n",
        "            else:\n",
        "                current_temp = temperature\n",
        "\n",
        "            logits_i = logits_i / current_temp\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0 and top_k < model.vocab_size:\n",
        "                values, _ = torch.topk(logits_i, top_k)\n",
        "                min_value = values[-1]\n",
        "                logits_i = torch.where(logits_i < min_value,\n",
        "                                      torch.full_like(logits_i, -float('inf')),\n",
        "                                      logits_i)\n",
        "\n",
        "        if cond is not None:\n",
        "            # For each conditioned local pixel, restrict the distribution\n",
        "            candidate_mask = torch.ones(model.vocab_size, dtype=torch.bool, device=logits_i.device)\n",
        "            for local_idx, cond_val in cond.items():\n",
        "                # For each token, check if its value at local_idx matches the condition\n",
        "                candidate_mask = candidate_mask & (model.vocab[:, local_idx] == cond_val)\n",
        "            logits_i = logits_i.masked_fill(~candidate_mask, -1e9)\n",
        "\n",
        "        probs = torch.softmax(logits_i, dim=-1)\n",
        "        token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Accumulate log probability\n",
        "        log_prob = torch.log(probs[token] + 1e-10)  # add epsilon for stability\n",
        "        log_likelihood += log_prob.item()\n",
        "\n",
        "        # Lookup the corresponding patch token from the vocabulary\n",
        "        patch = model.vocab[token]\n",
        "        generated[0, i] = patch\n",
        "\n",
        "    generation_time = time.time() - start_time\n",
        "    print(f\"Generation completed in {generation_time:.2f}s for {num_patches} patches\")\n",
        "\n",
        "    return generated, log_likelihood\n",
        "\n",
        "def generate_diverse_samples(model, patch_size, image_size, condition_indices=None,\n",
        "                            condition_values=None, num_samples=3, base_temp=1.0):\n",
        "    \"\"\"\n",
        "    Generate multiple diverse samples with the same conditions but different temperatures\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    log_probs = []\n",
        "\n",
        "    # Generate with increasing temperatures for more diversity\n",
        "    temps = [base_temp * (0.8 + 0.2 * i) for i in range(num_samples)]\n",
        "\n",
        "    for i, temp in enumerate(temps):\n",
        "        print(f\"Generating sample {i+1}/{num_samples} with temperature {temp:.2f}\")\n",
        "        gen, ll = sample_image_conditional(\n",
        "            model, patch_size, image_size,\n",
        "            temperature=temp,\n",
        "            condition_indices=condition_indices,\n",
        "            condition_values=condition_values,\n",
        "            top_k=40,  # Use top-k filtering\n",
        "            adaptive_temp=True  # Use adaptive temperature\n",
        "        )\n",
        "        samples.append(gen)\n",
        "        log_probs.append(ll)\n",
        "\n",
        "    return samples, log_probs\n",
        "\n",
        "########################################\n",
        "# Improved Vision Transformer Components\n",
        "########################################\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Stochastic Depth: randomly drops entire layers during training\n",
        "    for regularization, similar to dropout but on a layer level.\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=0.0):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.drop_prob == 0:\n",
        "            return x\n",
        "\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, device=x.device)\n",
        "        random_tensor = random_tensor.floor() / keep_prob\n",
        "        return x * random_tensor\n",
        "\n",
        "class ImprovedTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ffn_dim, dropout=0.1, drop_path=0.0):\n",
        "        super().__init__()\n",
        "        # Pre-norm architecture\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Expanded feed-forward network (wider for better capacity)\n",
        "        expansion_factor = 4  # Standard is 4x in modern transformers\n",
        "        self.linear1 = nn.Linear(hidden_dim, ffn_dim * expansion_factor)\n",
        "        self.linear2 = nn.Linear(ffn_dim * expansion_factor, hidden_dim)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        # Stochastic depth for regularization\n",
        "        self.drop_path = StochasticDepth(drop_path)\n",
        "\n",
        "    def forward(self, z, attn_mask=None):\n",
        "        # Pre-norm architecture (more stable training)\n",
        "        z_norm = self.norm1(z)\n",
        "\n",
        "        # Self-attention with proper masking handling\n",
        "        if attn_mask is not None:\n",
        "            # Convert boolean mask to float attention mask\n",
        "            if attn_mask.dtype == torch.bool:\n",
        "                attn_mask = attn_mask.float().masked_fill(\n",
        "                    ~attn_mask, float(\"-inf\")).masked_fill(attn_mask, float(0.0))\n",
        "\n",
        "            attn_output, _ = self.self_attn(z_norm, z_norm, z_norm, attn_mask=attn_mask)\n",
        "        else:\n",
        "            attn_output, _ = self.self_attn(z_norm, z_norm, z_norm)\n",
        "\n",
        "        # Apply residual connection with potential stochastic depth\n",
        "        z = z + self.drop_path(self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward block also with pre-norm\n",
        "        z_norm = self.norm2(z)\n",
        "        ff_output = self.linear2(self.dropout2(self.activation(self.linear1(z_norm))))\n",
        "\n",
        "        # Second residual connection\n",
        "        z = z + self.drop_path(self.dropout2(ff_output))\n",
        "        return z\n",
        "\n",
        "class ImprovedVisionTransformer(nn.Module):\n",
        "    \"\"\"Enhanced Vision Transformer with improved architecture and training stability\"\"\"\n",
        "    def __init__(self, num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Stochastic depth with increasing probability by depth\n",
        "        dpr = [x.item() for x in torch.linspace(0, 0.2, num_layers)]\n",
        "\n",
        "        # Improved transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            ImprovedTransformerEncoderLayer(\n",
        "                hidden_dim, num_heads, ffn_dim, dropout, drop_path=dpr[i]\n",
        "            ) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "        # Final layer norm (following modern architectures)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def build_vocabulary(self, training_data, patch_size, full_mask=True, one_mask=True):\n",
        "        self.patch_size = patch_size\n",
        "        patch_dim = patch_size * patch_size\n",
        "\n",
        "        # Assuming training_data shape is (N, H, W)\n",
        "        H, W = training_data.shape[1], training_data.shape[2]\n",
        "        num_patches = (H // patch_size) * (W // patch_size)\n",
        "\n",
        "        # Extract non-overlapping patches using unfold (vectorized)\n",
        "        patches_tensor = training_data.unfold(1, patch_size, patch_size)\\\n",
        "                                      .unfold(2, patch_size, patch_size)\n",
        "        patches_tensor = patches_tensor.contiguous().view(-1, patch_dim)\n",
        "\n",
        "        # Build vocabulary with a progress bar\n",
        "        unique_set = set()\n",
        "        unique_list = []\n",
        "\n",
        "        for patch in tqdm(patches_tensor, desc=f\"Building Vocabulary for {patch_size}x{patch_size}\",\n",
        "                      total=patches_tensor.shape[0]):\n",
        "            patch_cpu = patch.cpu()  # force on CPU for uniqueness checking\n",
        "            patch_tuple = tuple(patch_cpu.tolist())\n",
        "            if patch_tuple not in unique_set:\n",
        "                unique_set.add(patch_tuple)\n",
        "                unique_list.append(patch_cpu)\n",
        "\n",
        "        print(f\"Found {len(unique_list)} unique patch patterns\")\n",
        "\n",
        "        # Add full mask token (a patch filled with 0.5)\n",
        "        if full_mask:\n",
        "            unique_list.append(torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu'))\n",
        "\n",
        "        # Add one-mask tokens for each patch dimension\n",
        "        if one_mask:\n",
        "            for i in range(patch_dim):\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 0.0\n",
        "                unique_list.append(mask_patch)\n",
        "\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 1.0\n",
        "                unique_list.append(mask_patch)\n",
        "\n",
        "        self.vocab = torch.stack(unique_list)  # shape: (vocab_size, patch_dim)\n",
        "        self.vocab_size = self.vocab.size(0)\n",
        "\n",
        "        # Precompute integer encoding for vocabulary tokens (for faster matching)\n",
        "        self.vocab_int = patch_to_int(self.vocab)\n",
        "\n",
        "        # Embedding projection: from patch_dim to hidden_dim\n",
        "        self.embedding_projection = nn.Linear(patch_dim, self.hidden_dim)\n",
        "\n",
        "        # Output projection: from hidden_dim to vocab size (to yield logits)\n",
        "        self.output_projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "\n",
        "        # Learned positional encoding for the number of patches (with proper initialization)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, self.hidden_dim))\n",
        "\n",
        "        # Learnable mask token\n",
        "        self.mask_token = nn.Parameter(torch.full((patch_dim,), 0.5, dtype=torch.float))\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "        return self.vocab\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights properly for transformer with improved stability\n",
        "        \"\"\"\n",
        "        # Initialize embeddings\n",
        "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "        # Initialize projection layers\n",
        "        if hasattr(self, 'embedding_projection'):\n",
        "            nn.init.trunc_normal_(self.embedding_projection.weight, std=0.02)\n",
        "            if self.embedding_projection.bias is not None:\n",
        "                nn.init.zeros_(self.embedding_projection.bias)\n",
        "\n",
        "        if hasattr(self, 'output_projection'):\n",
        "            nn.init.trunc_normal_(self.output_projection.weight, std=0.02)\n",
        "            if self.output_projection.bias is not None:\n",
        "                nn.init.zeros_(self.output_projection.bias)\n",
        "\n",
        "        # Initialize mask token\n",
        "        if hasattr(self, 'mask_token'):\n",
        "            nn.init.normal_(self.mask_token, mean=0.5, std=0.01)\n",
        "\n",
        "    def forward(self, patches, attn_mask=None):\n",
        "        # patches shape: (batch_size, num_patches, patch_dim)\n",
        "        batch_size, num_patches, _ = patches.shape\n",
        "\n",
        "        # Convert patches to vector embeddings\n",
        "        embeddings = self.embedding_projection(patches)\n",
        "\n",
        "        # Add positional encoding\n",
        "        embeddings = embeddings + self.pos_embedding[:, :num_patches, :]\n",
        "\n",
        "        # Apply dropout\n",
        "        z = self.dropout_layer(embeddings)\n",
        "\n",
        "        # Pass through transformer encoder layers\n",
        "        for layer in self.transformer_layers:\n",
        "            z = layer(z, attn_mask=attn_mask)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        z = self.norm(z)\n",
        "\n",
        "        # Project to vocabulary space\n",
        "        logits = self.output_projection(z)  # shape: (batch_size, num_patches, vocab_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "########################################\n",
        "# Dataset and Patch Extraction\n",
        "########################################\n",
        "\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images  # shape: (N, H, W)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_to_patches(images, patch_size):\n",
        "        \"\"\"Vectorized patch extraction for a batch\"\"\"\n",
        "        B, H, W = images.shape\n",
        "        patches = images.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "        patches = patches.contiguous().view(B, -1, patch_size * patch_size)\n",
        "        return patches\n",
        "\n",
        "    @staticmethod\n",
        "    def patches_to_image(patches, patch_size, image_size):\n",
        "        \"\"\"Convert patches to image (assumes patch order is in raster scan)\"\"\"\n",
        "        image = torch.zeros(image_size, image_size, device=patches.device)\n",
        "        idx = 0\n",
        "        for i in range(0, image_size, patch_size):\n",
        "            for j in range(0, image_size, patch_size):\n",
        "                image[i:i+patch_size, j:j+patch_size] = patches[idx].view(patch_size, patch_size)\n",
        "                idx += 1\n",
        "        return image\n",
        "\n",
        "########################################\n",
        "# Vectorized Target Computation\n",
        "########################################\n",
        "\n",
        "def compute_targets_vectorized(patches, vocab, vocab_int=None):\n",
        "    \"\"\"\n",
        "    Efficiently compute closest vocabulary token targets for all patches\n",
        "\n",
        "    Args:\n",
        "        patches: Tensor of shape [batch, num_patches, patch_dim] or [num_patches, patch_dim]\n",
        "        vocab: Vocabulary tensor of shape [vocab_size, patch_dim]\n",
        "        vocab_int: Optional precomputed integer codes for vocab (for binary data)\n",
        "\n",
        "    Returns:\n",
        "        targets: Long tensor of target indices with shape matching patches batch dims + []\n",
        "    \"\"\"\n",
        "    # Handle both single image and batch cases\n",
        "    orig_shape = patches.shape[:-1]  # Original batch dimensions\n",
        "\n",
        "    # Reshape to 2D: [num_total_patches, patch_dim]\n",
        "    if patches.dim() > 2:\n",
        "        patches_flat = patches.reshape(-1, patches.shape[-1])\n",
        "    else:\n",
        "        patches_flat = patches\n",
        "\n",
        "    # For binary data, use integer encoding for exact matching (much faster)\n",
        "    if vocab_int is not None:\n",
        "        # Convert patches to integer codes\n",
        "        patch_codes = patch_to_int(patches_flat)\n",
        "\n",
        "        # Initialize targets tensor\n",
        "        targets_flat = torch.zeros(patches_flat.shape[0], dtype=torch.long, device=patches.device)\n",
        "\n",
        "        # Match each patch to closest vocab token by matching integer codes\n",
        "        # This is faster than computing distances for binary data\n",
        "        for i, code in enumerate(patch_codes):\n",
        "            # Find matching vocab entry\n",
        "            matches = (vocab_int == code)\n",
        "            if matches.any():\n",
        "                # If exact match found, use it\n",
        "                targets_flat[i] = matches.nonzero(as_tuple=True)[0][0]\n",
        "            else:\n",
        "                # If no exact match (shouldn't happen with binary data), fall back to L2\n",
        "                distances = torch.sum((vocab - patches_flat[i:i+1]) ** 2, dim=1)\n",
        "                targets_flat[i] = torch.argmin(distances)\n",
        "    else:\n",
        "        # For non-binary data or if vocab_int not provided, use L2 distance\n",
        "        # Compute all pairwise distances using efficient cdist\n",
        "        distances = torch.cdist(patches_flat, vocab)\n",
        "        targets_flat = torch.argmin(distances, dim=1)\n",
        "\n",
        "    # Reshape back to original batch dimensions\n",
        "    targets = targets_flat.reshape(orig_shape)\n",
        "\n",
        "    return targets\n",
        "\n",
        "########################################\n",
        "# Training Pipeline with All Improvements\n",
        "########################################\n",
        "\n",
        "def train_improved_model(patch_size, training_data, num_epochs=1000, batch_size=64,\n",
        "                         eval_every=20, save_every=100):\n",
        "    \"\"\"\n",
        "    Training pipeline with all the suggested improvements\n",
        "    \"\"\"\n",
        "    # Enhanced hyperparameters\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    ffn_dim = 512\n",
        "    hidden_dim = 192  # Increased from 128\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Keep training data on CPU until needed\n",
        "    image_size = training_data.shape[1]  # assume square images\n",
        "\n",
        "    # Initialize wandb run with detailed config\n",
        "    wandb.init(\n",
        "        project=\"improved-vision-transformer\",\n",
        "        config={\n",
        "            \"architecture\": \"ImprovedVisionTransformer\",\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"ffn_dim\": ffn_dim,\n",
        "            \"hidden_dim\": hidden_dim,\n",
        "            \"patch_size\": patch_size,\n",
        "            \"num_epochs\": num_epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"weight_decay\": 0.01,\n",
        "            \"learning_rate\": 0.001,\n",
        "            \"scheduler\": \"CosineAnnealing\",\n",
        "            \"mixed_precision\": True,\n",
        "            \"stochastic_depth\": True,\n",
        "        },\n",
        "        name=f\"improved_patch_{patch_size}x{patch_size}\"\n",
        "    )\n",
        "\n",
        "    # Build improved model\n",
        "    model = ImprovedVisionTransformer(num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.1)\n",
        "\n",
        "    # Build vocabulary (using CPU data)\n",
        "    print(\"Building vocabulary...\")\n",
        "    model.build_vocabulary(training_data, patch_size)\n",
        "\n",
        "    # Move model to device after building vocabulary\n",
        "    model = model.to(device)\n",
        "    model.vocab = model.vocab.to(device)\n",
        "    model.vocab_int = model.vocab_int.to(device)\n",
        "\n",
        "    # Improved optimizer: AdamW with weight decay\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Dataset and optimized DataLoader\n",
        "    dataset = BinaryImageDataset(training_data)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=4,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    # Setup mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Training metrics tracking\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Global step for wandb logging\n",
        "    global_step = 0\n",
        "\n",
        "    # Track training time\n",
        "    total_training_time = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Progressive masking curriculum instead of random\n",
        "        # Start with low masking rate, gradually increase\n",
        "        mask_rate = min(0.85, 0.15 + (0.7 * epoch / (num_epochs * 0.8)))\n",
        "        partial_mask_rate = 0.3  # 30% of masked patches will be partial\n",
        "\n",
        "        # Create progress bar\n",
        "        epoch_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (patch {patch_size}x{patch_size})\")\n",
        "\n",
        "        # Pre-compute mask tokens for efficiency\n",
        "        full_mask_token = model.mask_token.clone().detach()\n",
        "\n",
        "        batch_times = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(epoch_bar):\n",
        "            batch_start = time.time()\n",
        "\n",
        "            # Move batch to GPU\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            B = batch.shape[0]\n",
        "\n",
        "            # Convert to patches (single operation)\n",
        "            patches = BinaryImageDataset.batch_to_patches(batch, patch_size)\n",
        "            num_patches = patches.shape[1]\n",
        "            patch_dim = patches.shape[2]\n",
        "\n",
        "            # Create mask: for each patch in each image, mask with probability = mask_rate\n",
        "            mask = torch.rand(B, num_patches, device=device) < mask_rate\n",
        "\n",
        "            # Ensure at least one masked patch per sample\n",
        "            for i in range(B):\n",
        "                if not torch.any(mask[i]):\n",
        "                    random_idx = torch.randint(0, num_patches, (1,), device=device)\n",
        "                    mask[i, random_idx] = True\n",
        "\n",
        "            # Decide which masked patches are partial vs. full (vectorized)\n",
        "            partial = (torch.rand(B, num_patches, device=device) < partial_mask_rate) & mask\n",
        "            full = mask & (~partial)\n",
        "\n",
        "            # Create masked patches efficiently\n",
        "            masked_patches = patches.clone()\n",
        "\n",
        "            # Apply full masks (vectorized)\n",
        "            if full.any():\n",
        "                # Extract indices where full masking applies\n",
        "                full_indices = torch.nonzero(full, as_tuple=True)\n",
        "                masked_patches[full_indices] = full_mask_token\n",
        "\n",
        "            # Apply partial masks\n",
        "            if partial.any():\n",
        "                # Get indices where partial masking applies\n",
        "                partial_idx = torch.nonzero(partial, as_tuple=True)\n",
        "                num_partial = len(partial_idx[0])\n",
        "\n",
        "                # Create new patches with a single observed value\n",
        "                new_patches = torch.full((num_partial, patch_dim), 0.5, device=device)\n",
        "                rand_positions = torch.randint(0, patch_dim, (num_partial,), device=device)\n",
        "\n",
        "                # Extract original values at random positions\n",
        "                orig_vals = patches[partial_idx[0], partial_idx[1], :].gather(\n",
        "                    1, rand_positions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                # Update new patches with observed values\n",
        "                new_patches[torch.arange(num_partial, device=device), rand_positions] = orig_vals\n",
        "\n",
        "                # Update masked patches\n",
        "                masked_patches[partial_idx] = new_patches\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with autocast():\n",
        "                # Get model predictions\n",
        "                logits = model(masked_patches)  # shape: (B, num_patches, vocab_size)\n",
        "\n",
        "                # Compute targets efficiently\n",
        "                targets = compute_targets_vectorized(patches, model.vocab, model.vocab_int)\n",
        "\n",
        "                # Compute loss over all patches (or only masked ones)\n",
        "                # Option 1: Loss only on masked patches\n",
        "                masked_logits = logits.reshape(-1, model.vocab_size)[mask.reshape(-1)]\n",
        "                masked_targets = targets.reshape(-1)[mask.reshape(-1)]\n",
        "                loss = criterion(masked_logits, masked_targets)\n",
        "\n",
        "                # Option 2: Full loss with masked patches weighted more\n",
        "                # full_loss = criterion(logits.reshape(-1, model.vocab_size), targets.reshape(-1))\n",
        "                # masked_loss = criterion(masked_logits, masked_targets)\n",
        "                # loss = 0.3 * full_loss + 0.7 * masked_loss\n",
        "\n",
        "            # Backward and optimize with mixed precision\n",
        "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Optional: gradient clipping\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Track metrics\n",
        "            total_loss += loss.item()\n",
        "            batch_time = time.time() - batch_start\n",
        "            batch_times.append(batch_time)\n",
        "\n",
        "            # Update progress bar\n",
        "            avg_batch_time = sum(batch_times[-10:]) / min(len(batch_times), 10)\n",
        "            epoch_bar.set_postfix(\n",
        "                loss=f\"{loss.item():.4f}\",\n",
        "                mask_rate=f\"{mask_rate:.2f}\",\n",
        "                batch_time=f\"{avg_batch_time:.3f}s\"\n",
        "            )\n",
        "\n",
        "            # Update global step\n",
        "            global_step += 1\n",
        "\n",
        "            # Visualize first batch of each epoch\n",
        "            if batch_idx == 0:\n",
        "                visualize_step = epoch * 10000  # Separate step space for visualization\n",
        "\n",
        "                # Generate visualization\n",
        "                with torch.no_grad(), autocast():\n",
        "                    model.eval()\n",
        "\n",
        "                    # Keep track of original, masked and reconstructed images\n",
        "                    orig_img = batch[0].cpu()\n",
        "                    masked_img = BinaryImageDataset.patches_to_image(\n",
        "                        masked_patches[0], patch_size, image_size).cpu()\n",
        "\n",
        "                    # Generate reconstructed image\n",
        "                    recon_patches = masked_patches[0:1].clone()\n",
        "\n",
        "                    # For masked patches, predict the most likely token\n",
        "                    for p in range(num_patches):\n",
        "                        if mask[0, p]:\n",
        "                            # Forward pass for just this batch\n",
        "                            patch_logits = model(recon_patches)\n",
        "                            # Get the most likely token\n",
        "                            patch_idx = torch.argmax(patch_logits[0, p])\n",
        "                            # Replace with predicted token\n",
        "                            recon_patches[0, p] = model.vocab[patch_idx]\n",
        "\n",
        "                    # Convert to image\n",
        "                    recon_img = BinaryImageDataset.patches_to_image(\n",
        "                        recon_patches[0], patch_size, image_size).cpu()\n",
        "\n",
        "                    # Log to wandb\n",
        "                    wandb.log({\n",
        "                        \"visualization/original\": wandb.Image(orig_img),\n",
        "                        \"visualization/masked\": wandb.Image(masked_img),\n",
        "                        \"visualization/reconstructed\": wandb.Image(recon_img)\n",
        "                    }, step=visualize_step)\n",
        "\n",
        "                    # Return to training mode\n",
        "                    model.train()\n",
        "\n",
        "        # End of epoch processing\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        total_training_time += epoch_time\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log metrics\n",
        "        wandb.log({\n",
        "            \"training/epoch\": epoch,\n",
        "            \"training/loss\": avg_loss,\n",
        "            \"training/mask_rate\": mask_rate,\n",
        "            \"training/learning_rate\": scheduler.get_last_lr()[0],\n",
        "            \"training/epoch_time\": epoch_time,\n",
        "            \"training/total_training_time\": total_training_time\n",
        "        }, step=global_step)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, \"\n",
        "              f\"LR: {scheduler.get_last_lr()[0]:.6f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': best_loss,\n",
        "            }, f\"improved_vit_patch{patch_size}x{patch_size}_best.pth\")\n",
        "\n",
        "            wandb.run.summary[\"best_loss\"] = best_loss\n",
        "            wandb.run.summary[\"best_epoch\"] = epoch\n",
        "\n",
        "        # Periodic evaluation\n",
        "        if (epoch + 1) % eval_every == 0 or epoch == num_epochs - 1:\n",
        "            model.eval()\n",
        "            # Basic evaluation on a batch from the training set\n",
        "            with torch.no_grad(), autocast():\n",
        "                eval_batch = next(iter(dataloader)).to(device)\n",
        "                eval_patches = BinaryImageDataset.batch_to_patches(eval_batch, patch_size)\n",
        "\n",
        "                # Mask 50% of patches\n",
        "                eval_mask = torch.rand(eval_batch.size(0), num_patches, device=device) < 0.5\n",
        "                eval_masked = eval_patches.clone()\n",
        "                eval_masked[eval_mask] = model.mask_token\n",
        "\n",
        "                # Forward pass\n",
        "                eval_logits = model(eval_masked)\n",
        "\n",
        "                # Compute targets\n",
        "                eval_targets = compute_targets_vectorized(eval_patches, model.vocab, model.vocab_int)\n",
        "\n",
        "                # Compute accuracy (on masked patches only)\n",
        "                masked_preds = torch.argmax(eval_logits[eval_mask], dim=1)\n",
        "                masked_targets = eval_targets[eval_mask]\n",
        "                accuracy = (masked_preds == masked_targets).float().mean().item()\n",
        "\n",
        "                # Log eval metrics\n",
        "                wandb.log({\n",
        "                    \"eval/accuracy\": accuracy,\n",
        "                }, step=global_step)\n",
        "\n",
        "                print(f\"Eval accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (epoch + 1) % save_every == 0 or epoch == num_epochs - 1:\n",
        "            checkpoint_path = f\"improved_vit_patch{patch_size}x{patch_size}_epoch{epoch+1}.pth\"\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "                'best_loss': best_loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "            # Generate conditional samples\n",
        "            generate_conditional_samples(model, patch_size, image_size, device, global_step)\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Best loss: {best_loss:.4f}\")\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), f\"improved_vit_patch{patch_size}x{patch_size}_final.pth\")\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_conditional_samples(model, patch_size, image_size, device, step):\n",
        "    \"\"\"Generate and log conditional samples with the model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Provided condition indices and values (global indices for a 60x60 image)\n",
        "    condition_indices = np.array([876,2825,2122,2892,1556,2683,2667,1767,483,2351,2000,3312,2953,289,\n",
        "                                  2373,2720,872,2713,1206,1341,3541,2226,3423,1904,2882,2540,1497,2524,264,1441])\n",
        "    condition_values = np.array([0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,1,0,1,1,0,1])\n",
        "\n",
        "    # Create condition visualization\n",
        "    cond_img = torch.zeros(image_size, image_size, device=device)\n",
        "    for idx, val in zip(condition_indices, condition_values):\n",
        "        row = idx // image_size\n",
        "        col = idx % image_size\n",
        "        cond_img[row, col] = val\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate diverse samples\n",
        "        samples, log_likes = generate_diverse_samples(\n",
        "            model, patch_size, image_size,\n",
        "            condition_indices=condition_indices,\n",
        "            condition_values=condition_values,\n",
        "            num_samples=3\n",
        "        )\n",
        "\n",
        "        # Create figure with condition points and samples\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "        # Plot condition points\n",
        "        axes[0].imshow(cond_img.cpu(), cmap='gray')\n",
        "        axes[0].set_title(\"Condition Points\")\n",
        "\n",
        "        # Plot samples with different temperatures\n",
        "        for i, (sample, ll) in enumerate(zip(samples, log_likes)):\n",
        "            # Convert sample patches to image\n",
        "            sample_img = BinaryImageDataset.patches_to_image(\n",
        "                sample[0], patch_size, image_size)\n",
        "\n",
        "            # Display\n",
        "            axes[i+1].imshow(sample_img.cpu(), cmap='gray')\n",
        "            axes[i+1].set_title(f\"Sample {i+1}, LL: {ll:.2f}\")\n",
        "\n",
        "        # Save figure to wandb\n",
        "        wandb.log({\n",
        "            \"conditional_generation/samples\": wandb.Image(fig)\n",
        "        }, step=step)\n",
        "        plt.close(fig)\n",
        "\n",
        "########################################\n",
        "# Main Entry Point\n",
        "########################################\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess training data\n",
        "    print(\"Loading training data...\")\n",
        "    arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "    training_data = arr[:9000]\n",
        "    # Reshape to 60x60 images\n",
        "    training_data = training_data.reshape(-1, 64, 64)[:, :60, :60]\n",
        "    print(f\"Training data shape: {training_data.shape}\")\n",
        "\n",
        "    # Convert to PyTorch tensor (keep on CPU initially)\n",
        "    training_data = torch.tensor(training_data, dtype=torch.float32)\n",
        "\n",
        "    # Train the model with optimized pipeline\n",
        "    patch_size = 3  # 3x3 patches offer good balance for 60x60 binary images\n",
        "    model = train_improved_model(\n",
        "        patch_size=patch_size,\n",
        "        training_data=training_data,\n",
        "        num_epochs=1000,\n",
        "        batch_size=64,\n",
        "        eval_every=20,\n",
        "        save_every=100\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "iNP0nyCiJd8O",
        "outputId": "1b899e89-56e6-4418-8459-4b2e4888d491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Training data shape: (9000, 60, 60)\n",
            "Using device: cuda\n",
            "Building vocabulary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Vocabulary for 3x3: 100%|██████████| 3600000/3600000 [00:14<00:00, 253909.77it/s]\n",
            "<ipython-input-14-44497637417b>:530: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 512 unique patch patterns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/1000 (patch 3x3):   0%|          | 0/141 [00:00<?, ?it/s]<ipython-input-14-44497637417b>:615: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/1000 (patch 3x3):   0%|          | 0/141 [00:03<?, ?it/s, batch_time=3.203s, loss=6.3101, mask_rate=0.15]<ipython-input-14-44497637417b>:665: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n",
            "Epoch 1/1000 (patch 3x3):   1%|          | 1/141 [00:03<08:37,  3.70s/it, batch_time=3.203s, loss=6.3101, mask_rate=0.15]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 248. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
            "Epoch 1/1000 (patch 3x3):   6%|▋         | 9/141 [00:27<06:48,  3.10s/it, batch_time=2.851s, loss=4.1769, mask_rate=0.15]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-44497637417b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-44497637417b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;31m# Train the model with optimized pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m  \u001b[0;31m# 3x3 patches offer good balance for 60x60 binary images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m     model = train_improved_model(\n\u001b[0m\u001b[1;32m    862\u001b[0m         \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-44497637417b>\u001b[0m in \u001b[0;36mtrain_improved_model\u001b[0;34m(patch_size, training_data, num_epochs, batch_size, eval_every, save_every)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0;31m# Compute targets efficiently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_targets_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0;31m# Compute loss over all patches (or only masked ones)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-44497637417b>\u001b[0m in \u001b[0;36mcompute_targets_vectorized\u001b[0;34m(patches, vocab, vocab_int)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;31m# Find matching vocab entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvocab_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0;31m# If exact match found, use it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mtargets_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "\n",
        "########################################\n",
        "# Vision Transformer and Vocabulary\n",
        "########################################\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(hidden_dim, num_heads, ffn_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def build_vocabulary(self, training_data, patch_size, full_mask=True, one_mask=True):\n",
        "        self.patch_size = patch_size\n",
        "        patch_dim = patch_size * patch_size\n",
        "        # Assuming training_data shape is (N, H, W)\n",
        "        H, W = training_data.shape[1], training_data.shape[2]\n",
        "        num_patches = (H // patch_size) * (W // patch_size)\n",
        "\n",
        "        # Extract non-overlapping patches using unfold.\n",
        "        patches_tensor = training_data.unfold(1, patch_size, patch_size)\\\n",
        "                                        .unfold(2, patch_size, patch_size)\n",
        "        patches_tensor = patches_tensor.contiguous().view(-1, patch_dim)\n",
        "\n",
        "        # Build vocabulary with a progress bar.\n",
        "        unique_set = set()\n",
        "        unique_list = []\n",
        "        for patch in tqdm(patches_tensor, desc=f\"Building Vocabulary for {patch_size}x{patch_size}\", total=patches_tensor.shape[0]):\n",
        "            patch_cpu = patch.cpu()  # force on CPU for uniqueness checking\n",
        "            patch_tuple = tuple(patch_cpu.tolist())\n",
        "            if patch_tuple not in unique_set:\n",
        "                unique_set.add(patch_tuple)\n",
        "                unique_list.append(patch_cpu)\n",
        "        # Add full mask token (a patch filled with 0.5)\n",
        "        if full_mask:\n",
        "            unique_list.append(torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu'))\n",
        "        # Add one-mask tokens for each patch dimension.\n",
        "        if one_mask:\n",
        "            for i in range(patch_dim):\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 0.0\n",
        "                unique_list.append(mask_patch)\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 1.0\n",
        "                unique_list.append(mask_patch)\n",
        "\n",
        "        self.vocab = torch.stack(unique_list)\n",
        "        self.vocab_size = self.vocab.size(0)\n",
        "        # Set up the embedding projection (from patch_dim to hidden_dim)\n",
        "        self.embedding_projection = nn.Linear(patch_dim, self.hidden_dim)\n",
        "        # Output projection: from hidden_dim to vocab size (to yield logits)\n",
        "        self.output_projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        # Learned positional encoding for the number of patches.\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, self.hidden_dim))\n",
        "        return self.vocab\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # patches shape: (batch_size, num_patches, patch_dim)\n",
        "        batch_size, num_patches, _ = patches.shape\n",
        "        # Step 7: convert patches to vector embeddings\n",
        "        embeddings = self.embedding_projection(patches)\n",
        "        # Step 8: add positional encoding\n",
        "        embeddings = embeddings + self.pos_embedding[:, :num_patches, :]\n",
        "        z = self.dropout_layer(embeddings)\n",
        "        # Step 9: pass through transformer encoder layers\n",
        "        for layer in self.transformer_layers:\n",
        "            z = layer(z)\n",
        "        # Step 10: project context vectors to (batch, num_patches, vocab_size)\n",
        "        logits = self.output_projection(z)\n",
        "        return logits\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(hidden_dim, ffn_dim)\n",
        "        self.linear2 = nn.Linear(ffn_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Self-attention block with residual connection.\n",
        "        z_norm = self.norm1(z)\n",
        "        z_t = z_norm.transpose(0, 1)\n",
        "        attn_output, _ = self.self_attn(z_t, z_t, z_t)\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "        z = z + self.dropout1(attn_output)\n",
        "        # Feed-forward block with residual connection.\n",
        "        z_norm = self.norm2(z)\n",
        "        ff_output = self.linear2(self.dropout2(self.activation(self.linear1(z_norm))))\n",
        "        z = z + self.dropout2(ff_output)\n",
        "        return z\n",
        "\n",
        "########################################\n",
        "# Dataset and Patch Extraction\n",
        "########################################\n",
        "\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images  # images shape: (N, H, W)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_to_patches(images, patch_size):\n",
        "        # Vectorized patch extraction for a batch (images shape: (B, H, W))\n",
        "        B, H, W = images.shape\n",
        "        patches = images.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "        patches = patches.contiguous().view(B, -1, patch_size * patch_size)\n",
        "        return patches\n",
        "\n",
        "    @staticmethod\n",
        "    def patches_to_image(patches, patch_size, image_size):\n",
        "        # Reconstruct an image from a list of patches.\n",
        "        image = torch.zeros(image_size, image_size, device=patches.device)\n",
        "        idx = 0\n",
        "        for i in range(0, image_size, patch_size):\n",
        "            for j in range(0, image_size, patch_size):\n",
        "                image[i:i+patch_size, j:j+patch_size] = patches[idx].view(patch_size, patch_size)\n",
        "                idx += 1\n",
        "        return image\n",
        "\n",
        "########################################\n",
        "# Training Function for a Given Patch Size\n",
        "########################################\n",
        "\n",
        "def train_model_for_patch(patch_size, training_data, num_epochs=1000, batch_size=64):\n",
        "    # Common hyperparameters.\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    ffn_dim = 512\n",
        "    hidden_dim = 128\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    training_data = training_data.to(device)\n",
        "    image_size = training_data.shape[1]  # assume square images\n",
        "\n",
        "    # Initialize a unique wandb run so that runs can be compared side by side.\n",
        "    wandb.init(project=\"vision-transformer-comparison\",\n",
        "               config={\n",
        "                   \"num_heads\": num_heads,\n",
        "                   \"num_layers\": num_layers,\n",
        "                   \"ffn_dim\": ffn_dim,\n",
        "                   \"hidden_dim\": hidden_dim,\n",
        "                   \"patch_size\": patch_size,\n",
        "                   \"num_epochs\": num_epochs,\n",
        "                   \"batch_size\": batch_size\n",
        "               },\n",
        "               name=f\"patch_{patch_size}x{patch_size}\",\n",
        "               reinit=True)\n",
        "\n",
        "    # Create the model and build its vocabulary.\n",
        "    model = VisionTransformer(num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.0)\n",
        "    model.build_vocabulary(training_data, patch_size)\n",
        "    model = model.to(device)\n",
        "    model.vocab = model.vocab.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = BinaryImageDataset(training_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Step 4: Increase mask rate linearly from a very small value (ensuring at least one mask) to 1.\n",
        "        mask_rate = (epoch + 1) / num_epochs  # fraction of patches to mask\n",
        "        partial_mask_rate = 0.3  # 30% of masked patches will be partial\n",
        "\n",
        "        epoch_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (patch {patch_size}x{patch_size})\", leave=False)\n",
        "        for batch in epoch_bar:\n",
        "            batch = batch.to(device)\n",
        "            B = batch.shape[0]\n",
        "            # Step 5: Convert the batch to patches.\n",
        "            patches = BinaryImageDataset.batch_to_patches(batch, patch_size)\n",
        "            num_patches = patches.shape[1]\n",
        "            patch_dim = patches.shape[2]\n",
        "\n",
        "            # Step 6: Determine which patches to mask.\n",
        "            mask = torch.rand(B, num_patches, device=device) < mask_rate\n",
        "            mask[:, 0] = True  # Ensure at least one masked patch per sample\n",
        "\n",
        "            # Decide which masks are partial (30%) and which are full.\n",
        "            partial = (torch.rand(B, num_patches, device=device) < partial_mask_rate) & mask\n",
        "            full = mask & (~partial)\n",
        "\n",
        "            masked_patches = patches.clone()\n",
        "            # For full masks, use the full mask token (last token in vocab).\n",
        "            if full.any():\n",
        "                masked_patches[full] = model.vocab[-1]\n",
        "            # For partial masks, fill with 0.5 and restore one randomly chosen element.\n",
        "            if partial.any():\n",
        "                partial_idx = torch.nonzero(partial)\n",
        "                num_partial = partial_idx.shape[0]\n",
        "                new_patches = torch.full((num_partial, patch_dim), 0.5, device=device)\n",
        "                rand_positions = torch.randint(0, patch_dim, (num_partial,), device=device)\n",
        "                orig_vals = patches[partial_idx[:, 0], partial_idx[:, 1], :].gather(1, rand_positions.unsqueeze(1)).squeeze(1)\n",
        "                new_patches[torch.arange(num_partial), rand_positions] = orig_vals\n",
        "                masked_patches[partial_idx[:, 0], partial_idx[:, 1]] = new_patches\n",
        "\n",
        "            # Steps 7–10: Forward pass\n",
        "            logits = model(masked_patches)  # logits shape: (B, num_patches, vocab_size)\n",
        "\n",
        "            # Step 11–12: Compute the target token for each patch and select only masked patches.\n",
        "            all_patches = patches.view(B * num_patches, patch_dim)\n",
        "            vocab = model.vocab  # (vocab_size, patch_dim)\n",
        "            dists = torch.cdist(all_patches, vocab, p=2) ** 2  # squared Euclidean distances\n",
        "            targets_all = torch.argmin(dists, dim=1).view(B, num_patches)\n",
        "            masked_targets = targets_all[mask]\n",
        "\n",
        "            loss = criterion(logits[mask], masked_targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            epoch_bar.set_postfix(loss=loss.item(), mask_rate=mask_rate)\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": avg_loss, \"mask_rate\": mask_rate})\n",
        "        print(f\"[Patch {patch_size}x{patch_size}] Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Mask Rate: {mask_rate:.2f}\")\n",
        "\n",
        "        # Save a checkpoint every 100 epochs.\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            torch.save(model.state_dict(), f\"vision_transformer_patch{patch_size}x{patch_size}_epoch{epoch+1}.pth\")\n",
        "\n",
        "    wandb.finish()\n",
        "    return model\n",
        "\n",
        "########################################\n",
        "# Main: Train All Models\n",
        "########################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load training data (assumed shape: (9000, 60, 60) after reshaping)\n",
        "    arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "    training_data = arr[:9000]\n",
        "    # Reshape to 60x60 images (adjust if needed)\n",
        "    training_data = training_data.reshape(-1, 64, 64)[:, :60, :60]\n",
        "    print(\"Training data shape:\", training_data.shape)\n",
        "    training_data = torch.tensor(training_data, dtype=torch.float32)\n",
        "\n",
        "    # Train models for patch sizes 2x2, 3x3, and 4x4 using the same parameters.\n",
        "    patch_sizes = [2, 3, 4]\n",
        "    models = {}\n",
        "    for ps in patch_sizes:\n",
        "        print(f\"\\nTraining model for patch size {ps}x{ps}\")\n",
        "        models[ps] = train_model_for_patch(ps, training_data)\n",
        "\n",
        "    # With wandb, the runs will appear side by side in your project dashboard."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "AyUEdGeIH8e3",
        "outputId": "04b69dc3-d577-4ee9-dd8d-936d487533d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (9000, 60, 60)\n",
            "\n",
            "Training model for patch size 2x2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-746d0840133f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTraining model for patch size {ps}x{ps}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_for_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# With wandb, the runs will appear side by side in your project dashboard.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-746d0840133f>\u001b[0m in \u001b[0;36mtrain_model_for_patch\u001b[0;34m(patch_size, training_data, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# Initialize a unique wandb run so that runs can be compared side by side.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     wandb.init(project=\"vision-transformer-comparison\",\n\u001b[0m\u001b[1;32m    157\u001b[0m                config={\n\u001b[1;32m    158\u001b[0m                    \u001b[0;34m\"num_heads\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1466\u001b[0m             \u001b[0minit_telemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_side_derived_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, settings, config)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"finishing previous run: {wandb.run.id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wandb.init() called while a run is active\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2104\u001b[0m                 ),\n\u001b[1;32m   2105\u001b[0m             )\n\u001b[0;32m-> 2106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m     def _finish(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atexit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0;31m# Run hooks that should happen after the last messages to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2607\u001b[0m         ) as progress_printer:\n\u001b[1;32m   2608\u001b[0m             \u001b[0;31m# Wait for the run to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2609\u001b[0;31m             wait_with_progress(\n\u001b[0m\u001b[1;32m   2610\u001b[0m                 \u001b[0mexit_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2611\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36mwait_with_progress\u001b[0;34m(handle, timeout, progress_after, display_progress)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpassing\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mwait_all_with_progress\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m     return wait_all_with_progress(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36mwait_all_with_progress\u001b[0;34m(handle_list, timeout, progress_after, display_progress)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wait_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_after\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36m_wait_handles\u001b[0;34m(handle_list, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mremaining_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/mailbox_handle.py\u001b[0m in \u001b[0;36mwait_or\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/response_handle.py\u001b[0m in \u001b[0;36mwait_or\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timeout must be finite or None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             raise TimeoutError(\n\u001b[1;32m     90\u001b[0m                 \u001b[0;34mf\"Timed out waiting for response on {self._address}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "\n",
        "########################################\n",
        "# Helper: Convert Patches to Integer Code\n",
        "########################################\n",
        "\n",
        "def patch_to_int(patches):\n",
        "    \"\"\"\n",
        "    Converts a tensor of patches (shape: [N, patch_dim]) containing binary values\n",
        "    to an integer code. Assumes values are near 0 or 1.\n",
        "    \"\"\"\n",
        "    patches_bin = patches.round().long()  # ensure binary\n",
        "    patch_dim = patches_bin.shape[1]\n",
        "    powers = (2 ** torch.arange(patch_dim - 1, -1, -1, device=patches.device)).unsqueeze(0)\n",
        "    codes = (patches_bin * powers).sum(dim=1)\n",
        "    return codes\n",
        "\n",
        "########################################\n",
        "# Vision Transformer and Vocabulary\n",
        "########################################\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(hidden_dim, num_heads, ffn_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def build_vocabulary(self, training_data, patch_size, full_mask=True, one_mask=True):\n",
        "        self.patch_size = patch_size\n",
        "        patch_dim = patch_size * patch_size\n",
        "        # Assuming training_data shape is (N, H, W)\n",
        "        H, W = training_data.shape[1], training_data.shape[2]\n",
        "        num_patches = (H // patch_size) * (W // patch_size)\n",
        "\n",
        "        # Extract non-overlapping patches using unfold.\n",
        "        patches_tensor = training_data.unfold(1, patch_size, patch_size)\\\n",
        "                                        .unfold(2, patch_size, patch_size)\n",
        "        patches_tensor = patches_tensor.contiguous().view(-1, patch_dim)\n",
        "\n",
        "        # Build vocabulary with a progress bar.\n",
        "        unique_set = set()\n",
        "        unique_list = []\n",
        "        for patch in tqdm(patches_tensor, desc=f\"Building Vocabulary for {patch_size}x{patch_size}\", total=patches_tensor.shape[0]):\n",
        "            patch_cpu = patch.cpu()  # force on CPU for uniqueness checking\n",
        "            patch_tuple = tuple(patch_cpu.tolist())\n",
        "            if patch_tuple not in unique_set:\n",
        "                unique_set.add(patch_tuple)\n",
        "                unique_list.append(patch_cpu)\n",
        "        # Add full mask token (a patch filled with 0.5) to the vocabulary if desired.\n",
        "        if full_mask:\n",
        "            unique_list.append(torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu'))\n",
        "        # Add one-mask tokens for each patch dimension.\n",
        "        if one_mask:\n",
        "            for i in range(patch_dim):\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 0.0\n",
        "                unique_list.append(mask_patch)\n",
        "                mask_patch = torch.full((patch_dim,), 0.5, dtype=torch.float, device='cpu')\n",
        "                mask_patch[i] = 1.0\n",
        "                unique_list.append(mask_patch)\n",
        "\n",
        "        self.vocab = torch.stack(unique_list)\n",
        "        self.vocab_size = self.vocab.size(0)\n",
        "        # Precompute integer encoding for vocabulary tokens.\n",
        "        self.vocab_int = patch_to_int(self.vocab)\n",
        "        # Set up the embedding projection (from patch_dim to hidden_dim)\n",
        "        self.embedding_projection = nn.Linear(patch_dim, self.hidden_dim)\n",
        "        # Output projection: from hidden_dim to vocab size (to yield logits)\n",
        "        self.output_projection = nn.Linear(self.hidden_dim, self.vocab_size)\n",
        "        # Learned positional encoding for the number of patches.\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, self.hidden_dim))\n",
        "        # --- Learnable Mask Token ---\n",
        "        self.mask_token = nn.Parameter(torch.full((patch_dim,), 0.5, dtype=torch.float))\n",
        "        return self.vocab\n",
        "\n",
        "    def forward(self, patches):\n",
        "        # patches shape: (batch_size, num_patches, patch_dim)\n",
        "        batch_size, num_patches, _ = patches.shape\n",
        "        # Convert patches to vector embeddings.\n",
        "        embeddings = self.embedding_projection(patches)\n",
        "        # Add positional encoding.\n",
        "        embeddings = embeddings + self.pos_embedding[:, :num_patches, :]\n",
        "        z = self.dropout_layer(embeddings)\n",
        "        # Pass through transformer encoder layers.\n",
        "        for layer in self.transformer_layers:\n",
        "            z = layer(z)\n",
        "        # Project context vectors to (batch, num_patches, vocab_size)\n",
        "        logits = self.output_projection(z)\n",
        "        return logits\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(hidden_dim, ffn_dim)\n",
        "        self.linear2 = nn.Linear(ffn_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Self-attention block with residual connection.\n",
        "        z_norm = self.norm1(z)\n",
        "        z_t = z_norm.transpose(0, 1)\n",
        "        attn_output, _ = self.self_attn(z_t, z_t, z_t)\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "        z = z + self.dropout1(attn_output)\n",
        "        # Feed-forward block with residual connection.\n",
        "        z_norm = self.norm2(z)\n",
        "        ff_output = self.linear2(self.dropout2(self.activation(self.linear1(z_norm))))\n",
        "        z = z + self.dropout2(ff_output)\n",
        "        return z\n",
        "\n",
        "########################################\n",
        "# Dataset and Patch Extraction\n",
        "########################################\n",
        "\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images  # images shape: (N, H, W)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_to_patches(images, patch_size):\n",
        "        # Vectorized patch extraction for a batch (images shape: (B, H, W))\n",
        "        B, H, W = images.shape\n",
        "        patches = images.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "        patches = patches.contiguous().view(B, -1, patch_size * patch_size)\n",
        "        return patches\n",
        "\n",
        "    @staticmethod\n",
        "    def patches_to_image(patches, patch_size, image_size):\n",
        "        # Reconstruct an image from a list of patches.\n",
        "        image = torch.zeros(image_size, image_size, device=patches.device)\n",
        "        idx = 0\n",
        "        for i in range(0, image_size, patch_size):\n",
        "            for j in range(0, image_size, patch_size):\n",
        "                image[i:i+patch_size, j:j+patch_size] = patches[idx].view(patch_size, patch_size)\n",
        "                idx += 1\n",
        "        return image\n",
        "\n",
        "########################################\n",
        "# Training Function for a Given Patch Size\n",
        "########################################\n",
        "\n",
        "def train_model_for_patch(patch_size, training_data, num_epochs=1000, batch_size=64):\n",
        "    # Common hyperparameters.\n",
        "    num_heads = 8\n",
        "    num_layers = 6\n",
        "    ffn_dim = 512\n",
        "    hidden_dim = 256\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    training_data = training_data.to(device)\n",
        "    image_size = training_data.shape[1]  # assume square images\n",
        "\n",
        "    # Initialize a unique wandb run for side-by-side comparison.\n",
        "    wandb.init(project=\"vision-transformer-comparison\",\n",
        "               config={\n",
        "                   \"num_heads\": num_heads,\n",
        "                   \"num_layers\": num_layers,\n",
        "                   \"ffn_dim\": ffn_dim,\n",
        "                   \"hidden_dim\": hidden_dim,\n",
        "                   \"patch_size\": patch_size,\n",
        "                   \"num_epochs\": num_epochs,\n",
        "                   \"batch_size\": batch_size\n",
        "               },\n",
        "               name=f\"patch_{patch_size}x{patch_size}\",\n",
        "               reinit=True)\n",
        "\n",
        "    # Create the model and build its vocabulary.\n",
        "    model = VisionTransformer(num_heads, num_layers, ffn_dim, hidden_dim, dropout=0.0)\n",
        "    model.build_vocabulary(training_data, patch_size)\n",
        "    model = model.to(device)\n",
        "    model.vocab = model.vocab.to(device)\n",
        "    model.vocab_int = patch_to_int(model.vocab).to(device)  # Precompute vocab codes\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = BinaryImageDataset(training_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # --- Random Mask Schedule ---\n",
        "        # For each epoch, sample a random mask rate between 10% and 100%.\n",
        "        mask_rate = np.random.uniform(0.1, 1.0)\n",
        "        partial_mask_rate = 0.3  # 30% of masked patches will be partial\n",
        "\n",
        "        epoch_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (patch {patch_size}x{patch_size})\", leave=False)\n",
        "        for batch in epoch_bar:\n",
        "            batch = batch.to(device)\n",
        "            B = batch.shape[0]\n",
        "            # Convert the batch to patches.\n",
        "            patches = BinaryImageDataset.batch_to_patches(batch, patch_size)\n",
        "            num_patches = patches.shape[1]\n",
        "            patch_dim = patches.shape[2]\n",
        "\n",
        "            # Determine which patches to mask using the random mask rate.\n",
        "            mask = torch.rand(B, num_patches, device=device) < mask_rate\n",
        "            mask[:, 0] = True  # Ensure at least one masked patch per sample\n",
        "\n",
        "            # Decide which masked patches are partial vs. full.\n",
        "            partial = (torch.rand(B, num_patches, device=device) < partial_mask_rate) & mask\n",
        "            full = mask & (~partial)\n",
        "\n",
        "            masked_patches = patches.clone()\n",
        "            # For full masks, use the learnable mask token.\n",
        "            if full.any():\n",
        "                masked_patches[full] = model.mask_token\n",
        "            # For partial masks, fill with 0.5 and restore one randomly chosen element.\n",
        "            if partial.any():\n",
        "                partial_idx = torch.nonzero(partial)\n",
        "                num_partial = partial_idx.shape[0]\n",
        "                new_patches = torch.full((num_partial, patch_dim), 0.5, device=device)\n",
        "                rand_positions = torch.randint(0, patch_dim, (num_partial,), device=device)\n",
        "                orig_vals = patches[partial_idx[:, 0], partial_idx[:, 1], :].gather(1, rand_positions.unsqueeze(1)).squeeze(1)\n",
        "                new_patches[torch.arange(num_partial), rand_positions] = orig_vals\n",
        "                masked_patches[partial_idx[:, 0], partial_idx[:, 1]] = new_patches\n",
        "\n",
        "            # Forward pass: get logits (shape: [B, num_patches, vocab_size]).\n",
        "            logits = model(masked_patches)\n",
        "\n",
        "            # --- Full Target Computation ---\n",
        "            # Compute integer codes for every original patch.\n",
        "            all_patches = patches.view(B * num_patches, patch_dim)\n",
        "            codes = patch_to_int(all_patches)  # shape: (B*num_patches,)\n",
        "            vocab_codes = model.vocab_int  # shape: (vocab_size,)\n",
        "            mask_eq = codes.unsqueeze(1) == vocab_codes.unsqueeze(0)\n",
        "            target_indices = mask_eq.float().argmax(dim=1)  # shape: (B*num_patches,)\n",
        "            targets_all = target_indices.view(B, num_patches)\n",
        "            # Compute loss over all patches.\n",
        "            loss = criterion(logits.view(-1, model.vocab_size), targets_all.view(-1))\n",
        "            # ---------------------------------\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            epoch_bar.set_postfix(loss=loss.item(), mask_rate=mask_rate)\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": avg_loss, \"mask_rate\": mask_rate})\n",
        "        print(f\"[Patch {patch_size}x{patch_size}] Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Mask Rate: {mask_rate:.2f}\")\n",
        "\n",
        "        # Save a checkpoint every 100 epochs.\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            torch.save(model.state_dict(), f\"vision_transformer_patch{patch_size}x{patch_size}_epoch{epoch+1}.pth\")\n",
        "\n",
        "    wandb.finish()\n",
        "    return model\n",
        "\n",
        "########################################\n",
        "# Main: Train All Models\n",
        "########################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load training data (assumed shape: (9000, 60, 60) after reshaping)\n",
        "    arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "    training_data = arr[:9000]\n",
        "    # Reshape to 60x60 images (adjust if needed)\n",
        "    training_data = training_data.reshape(-1, 64, 64)[:, :60, :60]\n",
        "    print(\"Training data shape:\", training_data.shape)\n",
        "    training_data = torch.tensor(training_data, dtype=torch.float32)\n",
        "\n",
        "    # Train models for patch sizes 2x2, 3x3, and 4x4 using the same parameters.\n",
        "    patch_sizes = [3, 4]\n",
        "    models = {}\n",
        "    for ps in patch_sizes:\n",
        "        print(f\"\\nTraining model for patch size {ps}x{ps}\")\n",
        "        models[ps] = train_model_for_patch(ps, training_data)\n",
        "\n",
        "    # With wandb, the runs will appear side by side in your project dashboard."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Iv26Qz4JkVKE",
        "outputId": "53e9f561-6553-4cda-8db2-b7ab04831309"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (9000, 60, 60)\n",
            "\n",
            "Training model for patch size 3x3\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">patch_2x2</strong> at: <a href='https://wandb.ai/oscars/vision-transformer-comparison/runs/v23q0q3k' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-comparison/runs/v23q0q3k</a><br> View project at: <a href='https://wandb.ai/oscars/vision-transformer-comparison' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250311_113033-v23q0q3k/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/GeoDecepticon/wandb/run-20250311_113143-i369ci2k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oscars/vision-transformer-comparison/runs/i369ci2k' target=\"_blank\">patch_3x3</a></strong> to <a href='https://wandb.ai/oscars/vision-transformer-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oscars/vision-transformer-comparison' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-comparison</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oscars/vision-transformer-comparison/runs/i369ci2k' target=\"_blank\">https://wandb.ai/oscars/vision-transformer-comparison/runs/i369ci2k</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building Vocabulary for 3x3: 100%|██████████| 3600000/3600000 [01:22<00:00, 43526.94it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 1/1000, Loss: 3.5374, Mask Rate: 0.79\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 2/1000, Loss: 2.7708, Mask Rate: 0.80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 3/1000, Loss: 1.9159, Mask Rate: 0.57\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 4/1000, Loss: 1.6463, Mask Rate: 0.49\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 5/1000, Loss: 0.9271, Mask Rate: 0.28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 6/1000, Loss: 2.7681, Mask Rate: 0.83\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 7/1000, Loss: 1.4006, Mask Rate: 0.42\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 8/1000, Loss: 3.0961, Mask Rate: 0.93\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 9/1000, Loss: 1.7360, Mask Rate: 0.52\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 10/1000, Loss: 2.1311, Mask Rate: 0.64\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 11/1000, Loss: 1.2334, Mask Rate: 0.37\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 12/1000, Loss: 2.3734, Mask Rate: 0.72\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 13/1000, Loss: 1.2521, Mask Rate: 0.38\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 14/1000, Loss: 2.1787, Mask Rate: 0.66\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 15/1000, Loss: 2.2240, Mask Rate: 0.67\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 16/1000, Loss: 1.4163, Mask Rate: 0.43\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 17/1000, Loss: 1.1280, Mask Rate: 0.35\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 18/1000, Loss: 0.7193, Mask Rate: 0.23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 19/1000, Loss: 9.1959, Mask Rate: 0.65\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 20/1000, Loss: 3.4708, Mask Rate: 0.86\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 21/1000, Loss: 3.4526, Mask Rate: 0.93\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 22/1000, Loss: 3.1252, Mask Rate: 0.87\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 23/1000, Loss: 2.1927, Mask Rate: 0.62\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 24/1000, Loss: 2.4351, Mask Rate: 0.72\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 25/1000, Loss: 2.2617, Mask Rate: 0.67\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 26/1000, Loss: 3.4519, Mask Rate: 0.99\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 27/1000, Loss: 1.3564, Mask Rate: 0.39\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 28/1000, Loss: 3.2152, Mask Rate: 0.95\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 29/1000, Loss: 2.8952, Mask Rate: 0.88\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 30/1000, Loss: 2.4469, Mask Rate: 0.75\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 31/1000, Loss: 0.7765, Mask Rate: 0.24\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 32/1000, Loss: 0.4281, Mask Rate: 0.14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 33/1000, Loss: 2.5819, Mask Rate: 0.81\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 34/1000, Loss: 2.9465, Mask Rate: 0.92\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 35/1000, Loss: 2.1543, Mask Rate: 0.69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 36/1000, Loss: 2.5414, Mask Rate: 0.81\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 37/1000, Loss: 1.1537, Mask Rate: 0.38\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 38/1000, Loss: 2.0382, Mask Rate: 0.66\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 39/1000, Loss: 1.9347, Mask Rate: 0.63\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 40/1000, Loss: 2.2219, Mask Rate: 0.72\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 41/1000, Loss: 1.3868, Mask Rate: 0.46\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 42/1000, Loss: 3.0961, Mask Rate: 0.96\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 43/1000, Loss: 2.6292, Mask Rate: 0.84\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 44/1000, Loss: 0.7906, Mask Rate: 0.27\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 45/1000, Loss: 2.0880, Mask Rate: 0.68\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 46/1000, Loss: 1.8018, Mask Rate: 0.60\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 47/1000, Loss: 2.8726, Mask Rate: 0.91\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 48/1000, Loss: 1.6598, Mask Rate: 0.56\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 49/1000, Loss: 2.0836, Mask Rate: 0.69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 50/1000, Loss: 1.4938, Mask Rate: 0.52\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 51/1000, Loss: 1.0191, Mask Rate: 0.39\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 52/1000, Loss: 1.9715, Mask Rate: 0.69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 53/1000, Loss: 1.1141, Mask Rate: 0.43\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 54/1000, Loss: 1.4749, Mask Rate: 0.55\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 55/1000, Loss: 19.1214, Mask Rate: 0.98\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 56/1000, Loss: 2.7329, Mask Rate: 0.50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 57/1000, Loss: 0.5388, Mask Rate: 0.13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 58/1000, Loss: 0.3897, Mask Rate: 0.11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 59/1000, Loss: 2.6179, Mask Rate: 0.66\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 60/1000, Loss: 3.1949, Mask Rate: 0.92\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 61/1000, Loss: 2.0741, Mask Rate: 0.64\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 62/1000, Loss: 0.5015, Mask Rate: 0.16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 63/1000, Loss: 1.5623, Mask Rate: 0.50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 64/1000, Loss: 1.3359, Mask Rate: 0.43\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 65/1000, Loss: 1.3598, Mask Rate: 0.45\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 66/1000, Loss: 2.9182, Mask Rate: 0.89\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 67/1000, Loss: 1.1799, Mask Rate: 0.41\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 68/1000, Loss: 3.0386, Mask Rate: 0.93\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 69/1000, Loss: 1.5361, Mask Rate: 0.53\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 70/1000, Loss: 2.3014, Mask Rate: 0.76\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 71/1000, Loss: 0.8552, Mask Rate: 0.32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 72/1000, Loss: 1.0759, Mask Rate: 0.40\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 73/1000, Loss: 1.3598, Mask Rate: 0.49\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 74/1000, Loss: 1.0351, Mask Rate: 0.39\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 75/1000, Loss: 1.2643, Mask Rate: 0.46\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 76/1000, Loss: 5.3187, Mask Rate: 0.92\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 77/1000, Loss: 2.8289, Mask Rate: 0.88\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 78/1000, Loss: 1.7526, Mask Rate: 0.60\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 79/1000, Loss: 0.7599, Mask Rate: 0.29\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 80/1000, Loss: 0.8225, Mask Rate: 0.31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 81/1000, Loss: 0.7773, Mask Rate: 0.30\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 82/1000, Loss: 1.2876, Mask Rate: 0.47\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 83/1000, Loss: 1.8811, Mask Rate: 0.66\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 84/1000, Loss: 1.8636, Mask Rate: 0.66\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 85/1000, Loss: 1.3846, Mask Rate: 0.51\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 86/1000, Loss: 0.5714, Mask Rate: 0.23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 87/1000, Loss: 0.9096, Mask Rate: 0.35\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 88/1000, Loss: 2.4373, Mask Rate: 0.81\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 89/1000, Loss: 3.1512, Mask Rate: 0.98\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 90/1000, Loss: 1.0870, Mask Rate: 0.26\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 91/1000, Loss: 0.6837, Mask Rate: 0.26\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 92/1000, Loss: 0.3182, Mask Rate: 0.12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 93/1000, Loss: 2.3078, Mask Rate: 0.70\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 94/1000, Loss: 0.6114, Mask Rate: 0.23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 95/1000, Loss: 2.9187, Mask Rate: 0.91\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 96/1000, Loss: 0.5290, Mask Rate: 0.20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 97/1000, Loss: 1.3419, Mask Rate: 0.49\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 98/1000, Loss: 1.9882, Mask Rate: 0.69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 99/1000, Loss: 2.5867, Mask Rate: 0.85\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 100/1000, Loss: 1.5245, Mask Rate: 0.56\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 101/1000, Loss: 2.1008, Mask Rate: 0.73\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 102/1000, Loss: 2.1437, Mask Rate: 0.74\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 103/1000, Loss: 2.1077, Mask Rate: 0.73\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 104/1000, Loss: 3.1560, Mask Rate: 0.99\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 105/1000, Loss: 1.3789, Mask Rate: 0.51\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 106/1000, Loss: 1.5217, Mask Rate: 0.57\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 107/1000, Loss: 0.3249, Mask Rate: 0.14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 108/1000, Loss: 2.7840, Mask Rate: 0.91\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 109/1000, Loss: 2.0678, Mask Rate: 0.73\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 110/1000, Loss: 0.7620, Mask Rate: 0.31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 111/1000, Loss: 1.5455, Mask Rate: 0.58\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 112/1000, Loss: 2.5514, Mask Rate: 0.86\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 113/1000, Loss: 1.9188, Mask Rate: 0.69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 114/1000, Loss: 0.9878, Mask Rate: 0.40\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 115/1000, Loss: 1.1281, Mask Rate: 0.45\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 116/1000, Loss: 1.1121, Mask Rate: 0.45\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 117/1000, Loss: 1.2507, Mask Rate: 0.50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 118/1000, Loss: 3.0837, Mask Rate: 0.94\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 119/1000, Loss: 1.4954, Mask Rate: 0.57\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 120/1000, Loss: 0.7798, Mask Rate: 0.34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 121/1000, Loss: 1.6005, Mask Rate: 0.61\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 122/1000, Loss: 0.3584, Mask Rate: 0.16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 123/1000, Loss: 1.0599, Mask Rate: 0.44\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 124/1000, Loss: 2.7931, Mask Rate: 0.92\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 125/1000, Loss: 2.0048, Mask Rate: 0.73\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 126/1000, Loss: 2.8753, Mask Rate: 0.95\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 127/1000, Loss: 0.7527, Mask Rate: 0.32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 128/1000, Loss: 2.4130, Mask Rate: 0.84\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 129/1000, Loss: 0.2426, Mask Rate: 0.11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 130/1000, Loss: 1.8651, Mask Rate: 0.70\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 131/1000, Loss: 1.7567, Mask Rate: 0.67\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 132/1000, Loss: 1.5871, Mask Rate: 0.62\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 133/1000, Loss: 0.7114, Mask Rate: 0.32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 134/1000, Loss: 1.3316, Mask Rate: 0.54\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 135/1000, Loss: 2.2993, Mask Rate: 0.81\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 136/1000, Loss: 1.0693, Mask Rate: 0.45\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 137/1000, Loss: 0.7569, Mask Rate: 0.34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 138/1000, Loss: 0.8140, Mask Rate: 0.36\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 139/1000, Loss: 1.3577, Mask Rate: 0.55\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 140/1000, Loss: 0.3797, Mask Rate: 0.18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 141/1000, Loss: 3.5118, Mask Rate: 0.97\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 142/1000, Loss: 1.9978, Mask Rate: 0.72\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 143/1000, Loss: 1.3056, Mask Rate: 0.53\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 144/1000, Loss: 0.5641, Mask Rate: 0.26\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 145/1000, Loss: 0.4901, Mask Rate: 0.23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 146/1000, Loss: 0.4558, Mask Rate: 0.21\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 147/1000, Loss: 1.5024, Mask Rate: 0.60\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 148/1000, Loss: 0.2339, Mask Rate: 0.11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 149/1000, Loss: 2.6372, Mask Rate: 0.89\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 150/1000, Loss: 2.3146, Mask Rate: 0.82\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 151/1000, Loss: 0.3431, Mask Rate: 0.16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 152/1000, Loss: 2.2526, Mask Rate: 0.81\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 153/1000, Loss: 2.5350, Mask Rate: 0.88\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 154/1000, Loss: 1.7987, Mask Rate: 0.69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 155/1000, Loss: 1.6688, Mask Rate: 0.65\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 156/1000, Loss: 0.9093, Mask Rate: 0.40\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 157/1000, Loss: 2.2979, Mask Rate: 0.82\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 158/1000, Loss: 1.0785, Mask Rate: 0.46\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 159/1000, Loss: 3.0507, Mask Rate: 0.99\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 160/1000, Loss: 0.7291, Mask Rate: 0.33\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Patch 3x3] Epoch 161/1000, Loss: 0.2650, Mask Rate: 0.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 162/1000, Loss: 0.2217, Mask Rate: 0.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 163/1000, Loss: 3.0701, Mask Rate: 0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 164/1000, Loss: 0.5962, Mask Rate: 0.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 165/1000, Loss: 0.4743, Mask Rate: 0.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 166/1000, Loss: 2.5799, Mask Rate: 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 167/1000, Loss: 2.0749, Mask Rate: 0.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 168/1000, Loss: 2.3568, Mask Rate: 0.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 169/1000, Loss: 2.7371, Mask Rate: 0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 170/1000, Loss: 0.7334, Mask Rate: 0.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 171/1000, Loss: 0.3203, Mask Rate: 0.16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 172/1000, Loss: 1.8457, Mask Rate: 0.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 173/1000, Loss: 2.1058, Mask Rate: 0.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 174/1000, Loss: 1.8625, Mask Rate: 0.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 175/1000, Loss: 1.3424, Mask Rate: 0.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 176/1000, Loss: 0.8104, Mask Rate: 0.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 177/1000, Loss: 0.6102, Mask Rate: 0.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 178/1000, Loss: 2.4839, Mask Rate: 0.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 179/1000, Loss: 2.5478, Mask Rate: 0.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 180/1000, Loss: 1.0022, Mask Rate: 0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 181/1000, Loss: 2.9509, Mask Rate: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 182/1000, Loss: 2.8374, Mask Rate: 0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Patch 3x3] Epoch 183/1000, Loss: 0.5955, Mask Rate: 0.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 184/1000 (patch 3x3):   5%|▍         | 7/141 [00:00<00:14,  9.09it/s, loss=0.643, mask_rate=0.278]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "\n",
        "# Assume VisionTransformer class is defined elsewhere as in your updated model\n",
        "from transformer_four import VisionTransformer, create_model\n",
        "\n",
        "# Binary Image Dataset\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "# Function to visualize binary images\n",
        "def plot_binary_image(image, title=None):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    return plt\n",
        "\n",
        "# Training Pipeline\n",
        "def train_vision_transformer(\n",
        "    train_dataset,\n",
        "    val_dataset=None,\n",
        "    d_model=512,\n",
        "    num_heads=8,\n",
        "    num_layers=6,\n",
        "    feedforward_dim=2048,\n",
        "    batch_size=32,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=1000,\n",
        "    save_dir=\"checkpoints\",\n",
        "    condition_indices=None,\n",
        "    condition_values=None,\n",
        "    device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    if val_dataset:\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(\n",
        "        train_dataset,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        feedforward_dim=feedforward_dim\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(\n",
        "        project=\"vision-transformer-4x4\",\n",
        "        config={\n",
        "            \"batch_size\": batch_size,\n",
        "            \"d_model\": d_model,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"feedforward_dim\": feedforward_dim,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"num_tokens\": model.num_tokens,\n",
        "            \"num_observed_tokens\": model.num_observed_tokens,\n",
        "            \"dropout\": 0.1,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"num_epochs\": num_epochs,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Track best model\n",
        "    best_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # Masking rate scheduler - linear increase from 1 to max over epochs\n",
        "    max_masks = 256  # All patches for a 64x64 image with 4x4 patches\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Calculate current masking rate (linear schedule from 1 to max_masks)\n",
        "        current_max_masks = min(1 + int((max_masks - 1) * epoch / (num_epochs * 0.8)), max_masks)\n",
        "\n",
        "        # For visualization\n",
        "        first_batch_imgs = []\n",
        "        first_batch_masks = []\n",
        "        first_batch_reconstructions = []\n",
        "\n",
        "        # Training loop\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx, images in progress_bar:\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Tokenize each image - convert to patches and map to token indices\n",
        "            # Shape: (batch_size, 256) for 64x64 images with 4x4 patches\n",
        "            token_indices = torch.stack([\n",
        "                model.tokenize_image(img) for img in images\n",
        "            ]).to(device)\n",
        "\n",
        "            # Create random mask for each image in batch\n",
        "            # Randomly select number of patches to mask for each image (between 1 and current_max_masks)\n",
        "            num_masks_per_image = torch.randint(\n",
        "                1, current_max_masks + 1, (batch_size,)\n",
        "            ).tolist()\n",
        "\n",
        "            # Prepare masked tokens and mask tensor\n",
        "            masked_tokens = token_indices.clone()\n",
        "            mask = torch.zeros_like(token_indices, dtype=torch.bool)\n",
        "\n",
        "            # Create masks and apply partial masking for 30% of masked patches\n",
        "            for b in range(batch_size):\n",
        "                # Select random positions to mask\n",
        "                positions_to_mask = torch.randperm(256)[:num_masks_per_image[b]]\n",
        "\n",
        "                # Determine which positions get partial masks (30%)\n",
        "                num_partial = int(0.3 * len(positions_to_mask))\n",
        "                partial_positions = positions_to_mask[:num_partial]\n",
        "                full_positions = positions_to_mask[num_partial:]\n",
        "\n",
        "                # Apply full masking (replace with mask token)\n",
        "                mask[b, full_positions] = True\n",
        "                masked_tokens[b, full_positions] = model.mask_token_id\n",
        "\n",
        "                # Apply partial masking (replace with partial mask tokens)\n",
        "                for i, pos in enumerate(partial_positions):\n",
        "                    mask[b, pos] = True\n",
        "\n",
        "                    # Get original token and extract one observed cell\n",
        "                    orig_token = token_indices[b, pos].item()\n",
        "                    if orig_token < model.num_observed_tokens:  # Only for non-special tokens\n",
        "                        # Get the original patch\n",
        "                        if orig_token in model.token_to_patch:\n",
        "                            orig_patch = model.token_to_patch[orig_token]\n",
        "\n",
        "                            # Select a random position to observe (0-15 for 4x4 patch)\n",
        "                            observed_pos = torch.randint(0, 16, (1,)).item()\n",
        "                            observed_val = orig_patch[observed_pos].item()\n",
        "\n",
        "                            # Calculate partial mask token id\n",
        "                            partial_idx = observed_pos * 2 + int(observed_val)\n",
        "\n",
        "                            # Assign partial mask token if available\n",
        "                            if partial_idx < len(model.partial_mask_token_ids):\n",
        "                                masked_tokens[b, pos] = model.partial_mask_token_ids[partial_idx]\n",
        "                            else:\n",
        "                                # Fallback to full mask if no suitable partial token\n",
        "                                masked_tokens[b, pos] = model.mask_token_id\n",
        "                        else:\n",
        "                            masked_tokens[b, pos] = model.mask_token_id\n",
        "                    else:\n",
        "                        masked_tokens[b, pos] = model.mask_token_id\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(masked_tokens)\n",
        "\n",
        "            # Extract only the masked positions for loss calculation\n",
        "            masked_positions = mask.nonzero(as_tuple=True)\n",
        "            masked_logits = logits[masked_positions[0], masked_positions[1]]\n",
        "            masked_targets = token_indices[masked_positions[0], masked_positions[1]]\n",
        "\n",
        "            # For partial masks, adjust target probabilities to include only compatible tokens\n",
        "            # For each masked position, we need to filter logits if it's a partial mask\n",
        "            filtered_logits = []\n",
        "            filtered_targets = []\n",
        "\n",
        "            for i in range(len(masked_positions[0])):\n",
        "                b, pos = masked_positions[0][i], masked_positions[1][i]\n",
        "                token_id = masked_tokens[b, pos].item()\n",
        "                target = masked_targets[i].item()\n",
        "\n",
        "                # If it's a partial mask token\n",
        "                if token_id in model.partial_mask_token_ids:\n",
        "                    # Determine which position and value are observed\n",
        "                    partial_idx = model.partial_mask_token_ids.index(token_id)\n",
        "                    observed_pos = partial_idx // 2\n",
        "                    observed_val = partial_idx % 2\n",
        "\n",
        "                    # Create a mask for compatible tokens (those with the same observed value)\n",
        "                    compatible_mask = torch.zeros(model.num_tokens)\n",
        "\n",
        "                    # Find all tokens in vocabulary that have the observed value at observed_pos\n",
        "                    for t_id, patch in model.token_to_patch.items():\n",
        "                        if t_id < model.num_observed_tokens and patch[observed_pos].item() == observed_val:\n",
        "                            compatible_mask[t_id] = 1.0\n",
        "\n",
        "                    # Zero out logits for incompatible tokens\n",
        "                    current_logits = logits[b, pos].clone()\n",
        "                    current_logits = current_logits * compatible_mask.to(device)\n",
        "\n",
        "                    # Replace very negative values for zeros to maintain softmax stability\n",
        "                    current_logits[compatible_mask.to(device) == 0] = -1e9\n",
        "\n",
        "                    filtered_logits.append(current_logits)\n",
        "                    filtered_targets.append(target)\n",
        "                else:\n",
        "                    # For full masks, use all logits\n",
        "                    filtered_logits.append(logits[b, pos])\n",
        "                    filtered_targets.append(target)\n",
        "\n",
        "            # Stack filtered logits and targets\n",
        "            if filtered_logits:\n",
        "                filtered_logits = torch.stack(filtered_logits)\n",
        "                filtered_targets = torch.tensor(filtered_targets).to(device)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(filtered_logits, filtered_targets)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"loss\": loss.item(), \"masks\": f\"{np.mean(num_masks_per_image):.1f}/{current_max_masks}\"})\n",
        "            else:\n",
        "                # Skip if no masked positions (unlikely, but handle it)\n",
        "                print(\"Warning: No masked positions in batch.\")\n",
        "                continue\n",
        "\n",
        "            # Save first batch for visualization\n",
        "            if batch_idx == 0:\n",
        "                with torch.no_grad():\n",
        "                    # Store original images\n",
        "                    first_batch_imgs = images[:4].cpu()\n",
        "\n",
        "                    # Store masked images\n",
        "                    masked_images = []\n",
        "                    for i in range(min(4, batch_size)):\n",
        "                        masked_img = model.tokens_to_image(masked_tokens[i:i+1].cpu())\n",
        "                        masked_images.append(masked_img.squeeze(0))\n",
        "                    first_batch_masks = masked_images\n",
        "\n",
        "                    # Generate reconstructions\n",
        "                    reconstructed_tokens = masked_tokens[:4].cpu().clone()\n",
        "                    mask_cpu = mask[:4].cpu()\n",
        "\n",
        "                    # For each image in mini-batch\n",
        "                    for i in range(min(4, batch_size)):\n",
        "                        # Get positions of masked tokens\n",
        "                        masked_positions_i = mask_cpu[i].nonzero(as_tuple=True)[0]\n",
        "\n",
        "                        # Get predicted token ids\n",
        "                        pred_logits = filtered_logits.reshape(-1, model.num_tokens)\n",
        "                        pred_tokens = torch.argmax(pred_logits, dim=1).cpu()\n",
        "\n",
        "                        # Count masked positions for this image\n",
        "                        count = 0\n",
        "                        for j, pos in enumerate(masked_positions):\n",
        "                            if pos[0] == i and j < len(pred_tokens):\n",
        "                                reconstructed_tokens[i, pos[1]] = pred_tokens[count]\n",
        "                                count += 1\n",
        "\n",
        "                    # Create reconstructed images\n",
        "                    reconstructed_images = []\n",
        "                    for i in range(min(4, batch_size)):\n",
        "                        recon_img = model.tokens_to_image(reconstructed_tokens[i:i+1])\n",
        "                        reconstructed_images.append(recon_img.squeeze(0))\n",
        "                    first_batch_reconstructions = reconstructed_images\n",
        "\n",
        "        # Calculate average loss for epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Log to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"masked_patches\": np.mean(num_masks_per_image),\n",
        "            \"max_masked_patches\": current_max_masks,\n",
        "        })\n",
        "\n",
        "        # Visualization every 50 epochs or on the last epoch\n",
        "        if (epoch + 1) % 50 == 0 or epoch == num_epochs - 1:\n",
        "            # Log images to wandb\n",
        "            if first_batch_imgs:\n",
        "                fig, axes = plt.subplots(3, min(4, batch_size), figsize=(16, 9))\n",
        "                for i in range(min(4, batch_size)):\n",
        "                    # Original\n",
        "                    axes[0, i].imshow(first_batch_imgs[i].squeeze(0), cmap='gray')\n",
        "                    axes[0, i].set_title(f\"Original {i+1}\")\n",
        "                    axes[0, i].axis('off')\n",
        "\n",
        "                    # Masked\n",
        "                    axes[1, i].imshow(first_batch_masks[i].squeeze(0), cmap='gray')\n",
        "                    axes[1, i].set_title(f\"Masked {i+1}\")\n",
        "                    axes[1, i].axis('off')\n",
        "\n",
        "                    # Reconstructed\n",
        "                    axes[2, i].imshow(first_batch_reconstructions[i].squeeze(0), cmap='gray')\n",
        "                    axes[2, i].set_title(f\"Reconstructed {i+1}\")\n",
        "                    axes[2, i].axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                wandb.log({\"sample_images\": wandb.Image(plt)})\n",
        "                plt.close()\n",
        "\n",
        "        # Validation step\n",
        "        if val_dataset:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images in val_loader:\n",
        "                    # Similar tokenization and masking logic as training\n",
        "                    token_indices = torch.stack([\n",
        "                        model.tokenize_image(img) for img in images\n",
        "                    ]).to(device)\n",
        "\n",
        "                    # Use fixed masking rate for validation\n",
        "                    num_masks = min(64, current_max_masks)  # 25% of patches\n",
        "\n",
        "                    # Prepare masked tokens and mask tensor\n",
        "                    masked_tokens = token_indices.clone()\n",
        "                    mask = torch.zeros_like(token_indices, dtype=torch.bool)\n",
        "\n",
        "                    # Apply masking\n",
        "                    for b in range(images.shape[0]):\n",
        "                        positions_to_mask = torch.randperm(256)[:num_masks]\n",
        "                        mask[b, positions_to_mask] = True\n",
        "                        masked_tokens[b, positions_to_mask] = model.mask_token_id\n",
        "\n",
        "                    # Forward pass\n",
        "                    logits = model(masked_tokens)\n",
        "\n",
        "                    # Calculate validation loss\n",
        "                    masked_positions = mask.nonzero(as_tuple=True)\n",
        "                    masked_logits = logits[masked_positions[0], masked_positions[1]]\n",
        "                    masked_targets = token_indices[masked_positions[0], masked_positions[1]]\n",
        "\n",
        "                    # Compute loss\n",
        "                    batch_loss = criterion(masked_logits, masked_targets)\n",
        "                    val_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate average validation loss\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            wandb.log({\"val_loss\": avg_val_loss})\n",
        "\n",
        "            # Save best model\n",
        "            if avg_val_loss < best_loss:\n",
        "                best_loss = avg_val_loss\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # Save checkpoint\n",
        "                checkpoint_path = os.path.join(save_dir, f\"best_model.pth\")\n",
        "                torch.save({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": avg_val_loss,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "                wandb.save(checkpoint_path)\n",
        "                print(f\"New best model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Generate samples with conditioning (requirement G)\n",
        "                if condition_indices is not None and condition_values is not None:\n",
        "                    generate_conditioned_samples(model, condition_indices, condition_values, device)\n",
        "        else:\n",
        "            # If no validation set, save based on training loss\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # Save checkpoint\n",
        "                checkpoint_path = os.path.join(save_dir, f\"best_model.pth\")\n",
        "                torch.save({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": avg_loss,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "                wandb.save(checkpoint_path)\n",
        "                print(f\"New best model saved at epoch {epoch+1} with training loss: {avg_loss:.4f}\")\n",
        "\n",
        "                # Generate samples with conditioning (requirement G)\n",
        "                if condition_indices is not None and condition_values is not None:\n",
        "                    generate_conditioned_samples(model, condition_indices, condition_values, device)\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": avg_loss,\n",
        "            }, checkpoint_path)\n",
        "            wandb.save(checkpoint_path)\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    # Return best model\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_conditioned_samples(model, condition_indices, condition_values, device):\n",
        "    \"\"\"Generate samples with fixed conditioning values at specified indices\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert to tensor\n",
        "    condition_indices = torch.tensor(condition_indices, device=device)\n",
        "    condition_values = torch.tensor(condition_values, device=device)\n",
        "\n",
        "    # Generate multiple samples with the same conditioning\n",
        "    num_samples = 3\n",
        "    samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_samples):\n",
        "            # Start with all masked tokens\n",
        "            tokens = torch.full((1, 256), model.mask_token_id, device=device)\n",
        "\n",
        "            # Set the conditioning values\n",
        "            for idx, val in zip(condition_indices, condition_values):\n",
        "                # Convert flat index to 2D patch index\n",
        "                img_size = 64\n",
        "                patch_size = 4\n",
        "                patches_per_row = img_size // patch_size\n",
        "\n",
        "                pixel_x = idx % img_size\n",
        "                pixel_y = idx // img_size\n",
        "\n",
        "                patch_x = pixel_x // patch_size\n",
        "                patch_y = pixel_y // patch_size\n",
        "\n",
        "                patch_idx = patch_y * patches_per_row + patch_x\n",
        "\n",
        "                # Set the conditioning value in the token\n",
        "                if patch_idx < 256:  # Ensure it's a valid patch index\n",
        "                    # We need to find a token that has the right value at the right position\n",
        "                    within_patch_x = pixel_x % patch_size\n",
        "                    within_patch_y = pixel_y % patch_size\n",
        "                    within_patch_idx = within_patch_y * patch_size + within_patch_x\n",
        "\n",
        "                    # Create a partial mask token or find a matching token\n",
        "                    compatible_tokens = []\n",
        "\n",
        "                    # Check if we have a partial mask token for this position and value\n",
        "                    for partial_idx, token_id in enumerate(model.partial_mask_token_ids):\n",
        "                        if partial_idx // 2 == within_patch_idx and partial_idx % 2 == val.item():\n",
        "                            # Found a matching partial mask token\n",
        "                            tokens[0, patch_idx] = token_id\n",
        "                            break\n",
        "                    else:\n",
        "                        # If no partial mask token, find a token with matching value at position\n",
        "                        for token_id, patch in model.token_to_patch.items():\n",
        "                            if token_id < model.num_observed_tokens and patch[within_patch_idx].item() == val.item():\n",
        "                                compatible_tokens.append(token_id)\n",
        "\n",
        "                        if compatible_tokens:\n",
        "                            # Randomly select a compatible token\n",
        "                            tokens[0, patch_idx] = compatible_tokens[torch.randint(0, len(compatible_tokens), (1,)).item()]\n",
        "\n",
        "            # Now autoregressively generate the rest\n",
        "            image = model.generate_image(batch_size=1, temperature=0.8, device=device)\n",
        "            samples.append(image.cpu())\n",
        "\n",
        "    # Visualize and log the samples\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(samples[i].squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f\"Sample {i+1}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"conditioned_samples\": wandb.Image(plt)})\n",
        "    plt.close()\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Example usage in a notebook:"
      ],
      "metadata": {
        "id": "nNG2It5h7XLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "training_data = arr[:9000]\n",
        "test_data = arr[9000:]\n",
        "training_data = training_data.reshape(-1, 64, 64)\n",
        "val_data = test_data.reshape(-1, 64, 64)"
      ],
      "metadata": {
        "id": "lYPDxTDJ7n7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "feedforward_dim = 512\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 1000\n",
        "\n",
        "# Load data (example)\n",
        "train_images = torch.tensor(training_data, dtype=torch.float32)\n",
        "val_images = torch.tensor(val_data, dtype=torch.float32)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = BinaryImageDataset(train_images)\n",
        "val_dataset = BinaryImageDataset(val_images)\n",
        "\n",
        "# Condition indices and values as specified\n",
        "condition_indices = np.array([876,3825,2122,2892,1556,2683,3667,1767,483,2351,2000,3312,2953,289,2373,2720,872,2713,1206,1341,3541,2226,3423,1904,2882,2540,1497,2524,264,1441])\n",
        "condition_values = np.array([0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,1,0,1,1,0,1])\n",
        "\n",
        "# Train model\n",
        "model = train_vision_transformer(\n",
        "    train_dataset,\n",
        "    val_dataset=None,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    feedforward_dim=feedforward_dim,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    num_epochs=num_epochs,\n",
        "    condition_indices=condition_indices,\n",
        "    condition_values=condition_values\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "f02aa9258027484eba8ae941c9d687a3",
            "4392ce1da1304fb4b481947f1c4edfe9",
            "eec3b9fd794144e5987c900064fac6f2",
            "ba8b29e3e05b4f2795f7a5d6dfa9dc95",
            "c3d070e1c4164e32b8fc83153046afc7",
            "c55e79171b99431a8033d34bc2ce4c04",
            "2fb324f736bb436fbb2e942e2496f06a",
            "d11a1b24bc2d444ea446ddba063ea7da",
            "4c96f375e2494a95ad57e26af5f5ff5e",
            "9241fe190b5841fd81e7b99fef5825d1",
            "ff91dd2d7ad64e318a970afb00574b83",
            "9ba7301a841d4e37bfa697a8049e5211",
            "a059548e81a445248fb88fcbeb8df421",
            "525dcb16c5934d5199f4489595941f24",
            "c131c840e841485a808b1bf2ae3447dc",
            "12b465aa57a742a48c8bf61fbd1a777f",
            "2cf1feae39f14fa49eab43693548fa3f",
            "8a93a11e98dd45c4a01365ac573c3f94",
            "3866e7f8ff884badb674086c995ec8d7",
            "faf4028e255c4dd482187c8086864b04",
            "2056b6345f71469c8afc2104b45865c1",
            "15c6a6e08b884f79ad73a6c06e035067",
            "d5f04d6e0fc64cfeb57fa31ccab5b182",
            "198ee828aa4c4656abf93a1b41b6775b",
            "00b77f1bc92e4a4e94d9bda555844c6f",
            "9fab6075bd74489297ba98b6135505d6",
            "0b0bc1b0d18c4b3c8fca4541103efeb3",
            "5619d93fa6194e5c81723c82c9b8c8bf",
            "c3cb3ed829b44adb984b356a6547cd86",
            "02713b7ff8fe43f6aed5ee47070bf128",
            "e0bcd5f5c1704cfb9ef361654aa5deb0",
            "2134d14a708c421da55f69873c724aab",
            "17899b8df8b74d0aafd477de2f80de48",
            "d0711c0c79d34dda935a3942b3dbda9b",
            "8562f3779a2c4be5b608d85d00116d72"
          ]
        },
        "id": "zZ39GHWc7jtw",
        "outputId": "c2cf0542-3464-46bd-9343-a14557565a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Building vocabulary from training data...\n",
            "Found 30572 unique patches in training data\n",
            "Total token vocabulary size: 30605\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f02aa9258027484eba8ae941c9d687a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 1 with training loss: 7.3368\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ba7301a841d4e37bfa697a8049e5211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 2 with training loss: 6.7637\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5f04d6e0fc64cfeb57fa31ccab5b182",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 3 with training loss: 6.6008\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0711c0c79d34dda935a3942b3dbda9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 4 with training loss: 6.5381\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8562f3779a2c4be5b608d85d00116d72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f02aa9258027484eba8ae941c9d687a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4392ce1da1304fb4b481947f1c4edfe9",
              "IPY_MODEL_eec3b9fd794144e5987c900064fac6f2",
              "IPY_MODEL_ba8b29e3e05b4f2795f7a5d6dfa9dc95"
            ],
            "layout": "IPY_MODEL_c3d070e1c4164e32b8fc83153046afc7"
          }
        },
        "4392ce1da1304fb4b481947f1c4edfe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c55e79171b99431a8033d34bc2ce4c04",
            "placeholder": "​",
            "style": "IPY_MODEL_2fb324f736bb436fbb2e942e2496f06a",
            "value": "Epoch 1/1000: 100%"
          }
        },
        "eec3b9fd794144e5987c900064fac6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11a1b24bc2d444ea446ddba063ea7da",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c96f375e2494a95ad57e26af5f5ff5e",
            "value": 141
          }
        },
        "ba8b29e3e05b4f2795f7a5d6dfa9dc95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9241fe190b5841fd81e7b99fef5825d1",
            "placeholder": "​",
            "style": "IPY_MODEL_ff91dd2d7ad64e318a970afb00574b83",
            "value": " 141/141 [06:23&lt;00:00,  2.27s/it, loss=7.06, masks=1.0/1]"
          }
        },
        "c3d070e1c4164e32b8fc83153046afc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c55e79171b99431a8033d34bc2ce4c04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb324f736bb436fbb2e942e2496f06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11a1b24bc2d444ea446ddba063ea7da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c96f375e2494a95ad57e26af5f5ff5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9241fe190b5841fd81e7b99fef5825d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff91dd2d7ad64e318a970afb00574b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba7301a841d4e37bfa697a8049e5211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a059548e81a445248fb88fcbeb8df421",
              "IPY_MODEL_525dcb16c5934d5199f4489595941f24",
              "IPY_MODEL_c131c840e841485a808b1bf2ae3447dc"
            ],
            "layout": "IPY_MODEL_12b465aa57a742a48c8bf61fbd1a777f"
          }
        },
        "a059548e81a445248fb88fcbeb8df421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf1feae39f14fa49eab43693548fa3f",
            "placeholder": "​",
            "style": "IPY_MODEL_8a93a11e98dd45c4a01365ac573c3f94",
            "value": "Epoch 2/1000: 100%"
          }
        },
        "525dcb16c5934d5199f4489595941f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3866e7f8ff884badb674086c995ec8d7",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faf4028e255c4dd482187c8086864b04",
            "value": 141
          }
        },
        "c131c840e841485a808b1bf2ae3447dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2056b6345f71469c8afc2104b45865c1",
            "placeholder": "​",
            "style": "IPY_MODEL_15c6a6e08b884f79ad73a6c06e035067",
            "value": " 141/141 [06:23&lt;00:00,  2.28s/it, loss=5.94, masks=1.0/1]"
          }
        },
        "12b465aa57a742a48c8bf61fbd1a777f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cf1feae39f14fa49eab43693548fa3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a93a11e98dd45c4a01365ac573c3f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3866e7f8ff884badb674086c995ec8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf4028e255c4dd482187c8086864b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2056b6345f71469c8afc2104b45865c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c6a6e08b884f79ad73a6c06e035067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5f04d6e0fc64cfeb57fa31ccab5b182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_198ee828aa4c4656abf93a1b41b6775b",
              "IPY_MODEL_00b77f1bc92e4a4e94d9bda555844c6f",
              "IPY_MODEL_9fab6075bd74489297ba98b6135505d6"
            ],
            "layout": "IPY_MODEL_0b0bc1b0d18c4b3c8fca4541103efeb3"
          }
        },
        "198ee828aa4c4656abf93a1b41b6775b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5619d93fa6194e5c81723c82c9b8c8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_c3cb3ed829b44adb984b356a6547cd86",
            "value": "Epoch 3/1000:  31%"
          }
        },
        "00b77f1bc92e4a4e94d9bda555844c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02713b7ff8fe43f6aed5ee47070bf128",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0bcd5f5c1704cfb9ef361654aa5deb0",
            "value": 44
          }
        },
        "9fab6075bd74489297ba98b6135505d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2134d14a708c421da55f69873c724aab",
            "placeholder": "​",
            "style": "IPY_MODEL_17899b8df8b74d0aafd477de2f80de48",
            "value": " 44/141 [02:00&lt;04:24,  2.73s/it, loss=5.51, masks=1.0/1]"
          }
        },
        "0b0bc1b0d18c4b3c8fca4541103efeb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5619d93fa6194e5c81723c82c9b8c8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3cb3ed829b44adb984b356a6547cd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02713b7ff8fe43f6aed5ee47070bf128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0bcd5f5c1704cfb9ef361654aa5deb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2134d14a708c421da55f69873c724aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17899b8df8b74d0aafd477de2f80de48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}