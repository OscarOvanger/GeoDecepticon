{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQwb7bUT6z3F",
        "outputId": "f70ecfba-136f-46f2-edea-e933a3804001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OscarOvanger/GeoDecepticon.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZXtl1g-7FeE",
        "outputId": "cd68f6b8-4f1e-4a02-a31b-d01546ac2759"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GeoDecepticon' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/GeoDecepticon')"
      ],
      "metadata": {
        "id": "u5fsIzjR7GZa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "from GeneralViT import VisionTransformer, create_model"
      ],
      "metadata": {
        "id": "SZFtoPVzEsId"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and plot training data"
      ],
      "metadata": {
        "id": "LT8iK4NzEOg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "training_data = arr[:9000]\n",
        "test_data = arr[9000:]\n",
        "# We reshape it to 60x60 dataset\n",
        "training_data = training_data.reshape(-1, 64, 64)\n",
        "test_data = test_data.reshape(-1, 64, 64)\n",
        "training_data = training_data[:,:60,:60]\n",
        "test_data = test_data[:,:60,:60]\n",
        "print(training_data.shape)\n",
        "training_data = torch.tensor(training_data,dtype=torch.float32)\n",
        "test_data = torch.tensor(test_data,dtype=torch.float32)\n",
        "#plot the first data\n",
        "plt.imshow(training_data[0],cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ggDcm9hCEXRB",
        "outputId": "aa1ed790-980e-4a3b-eef8-9f88271e019c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9000, 60, 60)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model hyperparameters\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "ffn_dim = 256\n",
        "hidden_dim = 128\n",
        "patch_size = 2\n",
        "\n",
        "# Initialize the model\n",
        "model = VisionTransformer(num_heads, num_layers, ffn_dim, hidden_dim)\n",
        "\n",
        "# Build the vocabulary with 2x2 patches\n",
        "vocab = model.build_vocabulary(training_data, patch_size=2)"
      ],
      "metadata": {
        "id": "SAh5V-T2G28k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYXrHctXHaIA",
        "outputId": "8cc8afb9-e51f-4d8a-a7e2-92fac68e0c47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [0.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 1.0000, 0.0000, 1.0000],\n",
            "        [0.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 0.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 1.0000, 1.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 1.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 0.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 0.0000, 1.0000],\n",
            "        [0.0000, 0.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.0000, 0.5000, 0.5000, 0.5000],\n",
            "        [1.0000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.0000, 0.5000, 0.5000],\n",
            "        [0.5000, 1.0000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.5000],\n",
            "        [0.5000, 0.5000, 1.0000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.5000, 1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "from GeneralViT import VisionTransformer, create_model\n",
        "\n",
        "# Create Binary Image Dataset\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def image_to_patches(image, patch_size):\n",
        "        n = image.shape[0]\n",
        "        patches = torch.zeros((n // patch_size) ** 2, patch_size ** 2, device=image.device)\n",
        "        idx = 0\n",
        "        for i in range(0, n, patch_size):\n",
        "            for j in range(0, n, patch_size):\n",
        "                patches[idx] = image[i:i+patch_size, j:j+patch_size].reshape(-1)\n",
        "                idx += 1\n",
        "        return patches\n",
        "\n",
        "    @staticmethod\n",
        "    def patches_to_image(patches, patch_size, image_size):\n",
        "        image = torch.zeros(image_size, image_size, device=patches.device)\n",
        "        idx = 0\n",
        "        for i in range(0, image_size, patch_size):\n",
        "            for j in range(0, image_size, patch_size):\n",
        "                image[i:i+patch_size, j:j+patch_size] = patches[idx].reshape(patch_size, patch_size)\n",
        "                idx += 1\n",
        "        return image\n",
        "\n",
        "# Training Pipeline\n",
        "def train_model(training_data):\n",
        "    # Set parameters\n",
        "    num_heads = 4\n",
        "    num_layers = 2\n",
        "    ffn_dim = 256\n",
        "    hidden_dim = 128\n",
        "    patch_size = 2\n",
        "    num_epochs = 1000\n",
        "    batch_size = 32\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    training_data = training_data.to(device)\n",
        "    image_size = training_data.shape[1]\n",
        "\n",
        "    # Initialize model\n",
        "    model = create_model(training_data, num_heads, num_layers, ffn_dim, hidden_dim, patch_size)\n",
        "    model = model.to(device)\n",
        "    # Make sure vocab is also on the correct device\n",
        "    model.vocab = model.vocab.to(device)\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"vision-transformer\", config={\n",
        "        \"num_heads\": num_heads,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"ffn_dim\": ffn_dim,\n",
        "        \"hidden_dim\": hidden_dim,\n",
        "        \"patch_size\": patch_size\n",
        "    })\n",
        "\n",
        "    # Initialize optimizer and loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize dataloader\n",
        "    dataset = BinaryImageDataset(training_data)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Gradually increase masking rate from 15% to 75%\n",
        "        mask_rate = min(0.75, 0.15 + (0.6 * epoch / num_epochs))\n",
        "        partial_mask_rate = 0.3  # 30% of masks will be partial masks\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            batch_size = batch.shape[0]\n",
        "\n",
        "                            # Convert images to patches\n",
        "            patches = torch.stack([BinaryImageDataset.image_to_patches(img, patch_size) for img in batch])\n",
        "            patches = patches.to(device)\n",
        "            num_patches = patches.shape[1]\n",
        "\n",
        "            # Create masked patches\n",
        "            masked_patches = patches.clone()\n",
        "            mask_indices = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
        "\n",
        "            # Apply masking\n",
        "            for b in range(batch_size):\n",
        "                # Determine patches to mask\n",
        "                num_to_mask = max(1, int(num_patches * mask_rate))\n",
        "                mask_idx = torch.randperm(num_patches)[:num_to_mask]\n",
        "                mask_indices[b, mask_idx] = True\n",
        "\n",
        "                # Full masks (mask with 0.5s)\n",
        "                num_partial = int(num_to_mask * partial_mask_rate)\n",
        "                full_indices = mask_idx[num_partial:]\n",
        "                if len(full_indices) > 0:\n",
        "                    masked_patches[b, full_indices] = model.vocab[-1]  # Full mask token\n",
        "\n",
        "                # Partial masks (keep one observed value)\n",
        "                for idx in mask_idx[:num_partial]:\n",
        "                    patch = patches[b, idx].clone()\n",
        "                    pos_to_keep = torch.randint(0, patch_size**2, (1,)).item()\n",
        "                    mask = torch.full_like(patch, 0.5)\n",
        "                    mask[pos_to_keep] = patch[pos_to_keep]\n",
        "                    masked_patches[b, idx] = mask\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(masked_patches)\n",
        "\n",
        "            # Calculate targets (closest vocab token for each original patch)\n",
        "            targets = torch.zeros(batch_size, num_patches, dtype=torch.long, device=device)\n",
        "            for b in range(batch_size):\n",
        "                for p in range(num_patches):\n",
        "                    if mask_indices[b, p]:\n",
        "                        distances = torch.sum((model.vocab - patches[b, p].unsqueeze(0)) ** 2, dim=1)\n",
        "                        targets[b, p] = torch.argmin(distances)\n",
        "\n",
        "            # Calculate loss only on masked patches\n",
        "            masked_logits = logits[mask_indices]\n",
        "            masked_targets = targets[mask_indices]\n",
        "            loss = criterion(masked_logits, masked_targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Log metrics\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": avg_loss, \"mask_rate\": mask_rate})\n",
        "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, Mask Rate: {mask_rate:.2f}\")\n",
        "\n",
        "        # Visualize reconstruction every 20 epochs\n",
        "        if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Get first image from batch for visualization\n",
        "                img = next(iter(dataloader))[0].to(device)\n",
        "                patches = BinaryImageDataset.image_to_patches(img, patch_size).unsqueeze(0).to(device)\n",
        "\n",
        "                # Create masked version\n",
        "                masked = patches.clone()\n",
        "                mask_indices = torch.zeros(1, num_patches, dtype=torch.bool, device=device)\n",
        "\n",
        "                # Apply same masking logic\n",
        "                num_to_mask = int(num_patches * mask_rate)\n",
        "                mask_idx = torch.randperm(num_patches)[:num_to_mask]\n",
        "                mask_indices[0, mask_idx] = True\n",
        "\n",
        "                # Full masks\n",
        "                num_partial = int(num_to_mask * partial_mask_rate)\n",
        "                masked[0, mask_idx[num_partial:]] = model.vocab[-1]\n",
        "\n",
        "                # Partial masks\n",
        "                for idx in mask_idx[:num_partial]:\n",
        "                    patch = patches[0, idx].clone()\n",
        "                    pos_to_keep = torch.randint(0, patch_size**2, (1,)).item()\n",
        "                    mask = torch.full_like(patch, 0.5)\n",
        "                    mask[pos_to_keep] = patch[pos_to_keep]\n",
        "                    masked[0, idx] = mask\n",
        "\n",
        "                # Get predictions\n",
        "                pred_logits = model(masked)\n",
        "\n",
        "                # Reconstruct image\n",
        "                recon = masked.clone()\n",
        "                for p in range(num_patches):\n",
        "                    if mask_indices[0, p]:\n",
        "                        pred_idx = torch.argmax(pred_logits[0, p])\n",
        "                        pred_patch = model.vocab[pred_idx]\n",
        "\n",
        "                        # Preserve observed values in partial masks\n",
        "                        curr = masked[0, p]\n",
        "                        observed = torch.where(curr != 0.5)[0]\n",
        "                        if len(observed) > 0:\n",
        "                            for pos in observed:\n",
        "                                pred_patch = pred_patch.clone()\n",
        "                                pred_patch[pos] = curr[pos]\n",
        "\n",
        "                        recon[0, p] = pred_patch\n",
        "\n",
        "                # Convert patches back to images\n",
        "                masked_img = BinaryImageDataset.patches_to_image(masked[0], patch_size, image_size)\n",
        "                recon_img = BinaryImageDataset.patches_to_image(recon[0], patch_size, image_size)\n",
        "\n",
        "                # Log images\n",
        "                wandb.log({\n",
        "                    \"original\": wandb.Image(img.cpu()),\n",
        "                    \"masked\": wandb.Image(masked_img.cpu()),\n",
        "                    \"reconstructed\": wandb.Image(recon_img.cpu())\n",
        "                }, step=epoch)\n",
        "\n",
        "    wandb.finish()\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming training_data has shape (9000, 64, 64)\n",
        "    arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "    training_data = arr[:9000]\n",
        "    test_data = arr[9000:]\n",
        "    # We reshape it to 60x60 dataset\n",
        "    training_data = training_data.reshape(-1, 64, 64)\n",
        "    test_data = test_data.reshape(-1, 64, 64)\n",
        "    training_data = training_data[:,:60,:60]\n",
        "    test_data = test_data[:,:60,:60]\n",
        "    print(training_data.shape)\n",
        "    training_data = torch.tensor(training_data,dtype=torch.float32)\n",
        "    test_data = torch.tensor(test_data,dtype=torch.float32)\n",
        "    model = train_model(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "AyUEdGeIH8e3",
        "outputId": "0de5a287-881a-48ef-fba2-69ba1ba97b59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9000, 60, 60)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f9a1a6aaabe9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-f9a1a6aaabe9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(training_data)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffn_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Make sure vocab is also on the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GeoDecepticon/GeneralViT.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(train_dataset, num_heads, num_layers, ffn_dim, hidden_dim, patch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisionTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffn_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Build vocabulary from training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GeoDecepticon/GeneralViT.py\u001b[0m in \u001b[0;36mbuild_vocabulary\u001b[0;34m(self, training_data, patch_size, full_mask, one_mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                     \u001b[0munique_patches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "\n",
        "# Assume VisionTransformer class is defined elsewhere as in your updated model\n",
        "from transformer_four import VisionTransformer, create_model\n",
        "\n",
        "# Binary Image Dataset\n",
        "class BinaryImageDataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "# Function to visualize binary images\n",
        "def plot_binary_image(image, title=None):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    return plt\n",
        "\n",
        "# Training Pipeline\n",
        "def train_vision_transformer(\n",
        "    train_dataset,\n",
        "    val_dataset=None,\n",
        "    d_model=512,\n",
        "    num_heads=8,\n",
        "    num_layers=6,\n",
        "    feedforward_dim=2048,\n",
        "    batch_size=32,\n",
        "    learning_rate=1e-4,\n",
        "    num_epochs=1000,\n",
        "    save_dir=\"checkpoints\",\n",
        "    condition_indices=None,\n",
        "    condition_values=None,\n",
        "    device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    if val_dataset:\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(\n",
        "        train_dataset,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        feedforward_dim=feedforward_dim\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(\n",
        "        project=\"vision-transformer-4x4\",\n",
        "        config={\n",
        "            \"batch_size\": batch_size,\n",
        "            \"d_model\": d_model,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"feedforward_dim\": feedforward_dim,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"num_tokens\": model.num_tokens,\n",
        "            \"num_observed_tokens\": model.num_observed_tokens,\n",
        "            \"dropout\": 0.1,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"num_epochs\": num_epochs,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Track best model\n",
        "    best_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # Masking rate scheduler - linear increase from 1 to max over epochs\n",
        "    max_masks = 256  # All patches for a 64x64 image with 4x4 patches\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Calculate current masking rate (linear schedule from 1 to max_masks)\n",
        "        current_max_masks = min(1 + int((max_masks - 1) * epoch / (num_epochs * 0.8)), max_masks)\n",
        "\n",
        "        # For visualization\n",
        "        first_batch_imgs = []\n",
        "        first_batch_masks = []\n",
        "        first_batch_reconstructions = []\n",
        "\n",
        "        # Training loop\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx, images in progress_bar:\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Tokenize each image - convert to patches and map to token indices\n",
        "            # Shape: (batch_size, 256) for 64x64 images with 4x4 patches\n",
        "            token_indices = torch.stack([\n",
        "                model.tokenize_image(img) for img in images\n",
        "            ]).to(device)\n",
        "\n",
        "            # Create random mask for each image in batch\n",
        "            # Randomly select number of patches to mask for each image (between 1 and current_max_masks)\n",
        "            num_masks_per_image = torch.randint(\n",
        "                1, current_max_masks + 1, (batch_size,)\n",
        "            ).tolist()\n",
        "\n",
        "            # Prepare masked tokens and mask tensor\n",
        "            masked_tokens = token_indices.clone()\n",
        "            mask = torch.zeros_like(token_indices, dtype=torch.bool)\n",
        "\n",
        "            # Create masks and apply partial masking for 30% of masked patches\n",
        "            for b in range(batch_size):\n",
        "                # Select random positions to mask\n",
        "                positions_to_mask = torch.randperm(256)[:num_masks_per_image[b]]\n",
        "\n",
        "                # Determine which positions get partial masks (30%)\n",
        "                num_partial = int(0.3 * len(positions_to_mask))\n",
        "                partial_positions = positions_to_mask[:num_partial]\n",
        "                full_positions = positions_to_mask[num_partial:]\n",
        "\n",
        "                # Apply full masking (replace with mask token)\n",
        "                mask[b, full_positions] = True\n",
        "                masked_tokens[b, full_positions] = model.mask_token_id\n",
        "\n",
        "                # Apply partial masking (replace with partial mask tokens)\n",
        "                for i, pos in enumerate(partial_positions):\n",
        "                    mask[b, pos] = True\n",
        "\n",
        "                    # Get original token and extract one observed cell\n",
        "                    orig_token = token_indices[b, pos].item()\n",
        "                    if orig_token < model.num_observed_tokens:  # Only for non-special tokens\n",
        "                        # Get the original patch\n",
        "                        if orig_token in model.token_to_patch:\n",
        "                            orig_patch = model.token_to_patch[orig_token]\n",
        "\n",
        "                            # Select a random position to observe (0-15 for 4x4 patch)\n",
        "                            observed_pos = torch.randint(0, 16, (1,)).item()\n",
        "                            observed_val = orig_patch[observed_pos].item()\n",
        "\n",
        "                            # Calculate partial mask token id\n",
        "                            partial_idx = observed_pos * 2 + int(observed_val)\n",
        "\n",
        "                            # Assign partial mask token if available\n",
        "                            if partial_idx < len(model.partial_mask_token_ids):\n",
        "                                masked_tokens[b, pos] = model.partial_mask_token_ids[partial_idx]\n",
        "                            else:\n",
        "                                # Fallback to full mask if no suitable partial token\n",
        "                                masked_tokens[b, pos] = model.mask_token_id\n",
        "                        else:\n",
        "                            masked_tokens[b, pos] = model.mask_token_id\n",
        "                    else:\n",
        "                        masked_tokens[b, pos] = model.mask_token_id\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(masked_tokens)\n",
        "\n",
        "            # Extract only the masked positions for loss calculation\n",
        "            masked_positions = mask.nonzero(as_tuple=True)\n",
        "            masked_logits = logits[masked_positions[0], masked_positions[1]]\n",
        "            masked_targets = token_indices[masked_positions[0], masked_positions[1]]\n",
        "\n",
        "            # For partial masks, adjust target probabilities to include only compatible tokens\n",
        "            # For each masked position, we need to filter logits if it's a partial mask\n",
        "            filtered_logits = []\n",
        "            filtered_targets = []\n",
        "\n",
        "            for i in range(len(masked_positions[0])):\n",
        "                b, pos = masked_positions[0][i], masked_positions[1][i]\n",
        "                token_id = masked_tokens[b, pos].item()\n",
        "                target = masked_targets[i].item()\n",
        "\n",
        "                # If it's a partial mask token\n",
        "                if token_id in model.partial_mask_token_ids:\n",
        "                    # Determine which position and value are observed\n",
        "                    partial_idx = model.partial_mask_token_ids.index(token_id)\n",
        "                    observed_pos = partial_idx // 2\n",
        "                    observed_val = partial_idx % 2\n",
        "\n",
        "                    # Create a mask for compatible tokens (those with the same observed value)\n",
        "                    compatible_mask = torch.zeros(model.num_tokens)\n",
        "\n",
        "                    # Find all tokens in vocabulary that have the observed value at observed_pos\n",
        "                    for t_id, patch in model.token_to_patch.items():\n",
        "                        if t_id < model.num_observed_tokens and patch[observed_pos].item() == observed_val:\n",
        "                            compatible_mask[t_id] = 1.0\n",
        "\n",
        "                    # Zero out logits for incompatible tokens\n",
        "                    current_logits = logits[b, pos].clone()\n",
        "                    current_logits = current_logits * compatible_mask.to(device)\n",
        "\n",
        "                    # Replace very negative values for zeros to maintain softmax stability\n",
        "                    current_logits[compatible_mask.to(device) == 0] = -1e9\n",
        "\n",
        "                    filtered_logits.append(current_logits)\n",
        "                    filtered_targets.append(target)\n",
        "                else:\n",
        "                    # For full masks, use all logits\n",
        "                    filtered_logits.append(logits[b, pos])\n",
        "                    filtered_targets.append(target)\n",
        "\n",
        "            # Stack filtered logits and targets\n",
        "            if filtered_logits:\n",
        "                filtered_logits = torch.stack(filtered_logits)\n",
        "                filtered_targets = torch.tensor(filtered_targets).to(device)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(filtered_logits, filtered_targets)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"loss\": loss.item(), \"masks\": f\"{np.mean(num_masks_per_image):.1f}/{current_max_masks}\"})\n",
        "            else:\n",
        "                # Skip if no masked positions (unlikely, but handle it)\n",
        "                print(\"Warning: No masked positions in batch.\")\n",
        "                continue\n",
        "\n",
        "            # Save first batch for visualization\n",
        "            if batch_idx == 0:\n",
        "                with torch.no_grad():\n",
        "                    # Store original images\n",
        "                    first_batch_imgs = images[:4].cpu()\n",
        "\n",
        "                    # Store masked images\n",
        "                    masked_images = []\n",
        "                    for i in range(min(4, batch_size)):\n",
        "                        masked_img = model.tokens_to_image(masked_tokens[i:i+1].cpu())\n",
        "                        masked_images.append(masked_img.squeeze(0))\n",
        "                    first_batch_masks = masked_images\n",
        "\n",
        "                    # Generate reconstructions\n",
        "                    reconstructed_tokens = masked_tokens[:4].cpu().clone()\n",
        "                    mask_cpu = mask[:4].cpu()\n",
        "\n",
        "                    # For each image in mini-batch\n",
        "                    for i in range(min(4, batch_size)):\n",
        "                        # Get positions of masked tokens\n",
        "                        masked_positions_i = mask_cpu[i].nonzero(as_tuple=True)[0]\n",
        "\n",
        "                        # Get predicted token ids\n",
        "                        pred_logits = filtered_logits.reshape(-1, model.num_tokens)\n",
        "                        pred_tokens = torch.argmax(pred_logits, dim=1).cpu()\n",
        "\n",
        "                        # Count masked positions for this image\n",
        "                        count = 0\n",
        "                        for j, pos in enumerate(masked_positions):\n",
        "                            if pos[0] == i and j < len(pred_tokens):\n",
        "                                reconstructed_tokens[i, pos[1]] = pred_tokens[count]\n",
        "                                count += 1\n",
        "\n",
        "                    # Create reconstructed images\n",
        "                    reconstructed_images = []\n",
        "                    for i in range(min(4, batch_size)):\n",
        "                        recon_img = model.tokens_to_image(reconstructed_tokens[i:i+1])\n",
        "                        reconstructed_images.append(recon_img.squeeze(0))\n",
        "                    first_batch_reconstructions = reconstructed_images\n",
        "\n",
        "        # Calculate average loss for epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Log to wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"masked_patches\": np.mean(num_masks_per_image),\n",
        "            \"max_masked_patches\": current_max_masks,\n",
        "        })\n",
        "\n",
        "        # Visualization every 50 epochs or on the last epoch\n",
        "        if (epoch + 1) % 50 == 0 or epoch == num_epochs - 1:\n",
        "            # Log images to wandb\n",
        "            if first_batch_imgs:\n",
        "                fig, axes = plt.subplots(3, min(4, batch_size), figsize=(16, 9))\n",
        "                for i in range(min(4, batch_size)):\n",
        "                    # Original\n",
        "                    axes[0, i].imshow(first_batch_imgs[i].squeeze(0), cmap='gray')\n",
        "                    axes[0, i].set_title(f\"Original {i+1}\")\n",
        "                    axes[0, i].axis('off')\n",
        "\n",
        "                    # Masked\n",
        "                    axes[1, i].imshow(first_batch_masks[i].squeeze(0), cmap='gray')\n",
        "                    axes[1, i].set_title(f\"Masked {i+1}\")\n",
        "                    axes[1, i].axis('off')\n",
        "\n",
        "                    # Reconstructed\n",
        "                    axes[2, i].imshow(first_batch_reconstructions[i].squeeze(0), cmap='gray')\n",
        "                    axes[2, i].set_title(f\"Reconstructed {i+1}\")\n",
        "                    axes[2, i].axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                wandb.log({\"sample_images\": wandb.Image(plt)})\n",
        "                plt.close()\n",
        "\n",
        "        # Validation step\n",
        "        if val_dataset:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images in val_loader:\n",
        "                    # Similar tokenization and masking logic as training\n",
        "                    token_indices = torch.stack([\n",
        "                        model.tokenize_image(img) for img in images\n",
        "                    ]).to(device)\n",
        "\n",
        "                    # Use fixed masking rate for validation\n",
        "                    num_masks = min(64, current_max_masks)  # 25% of patches\n",
        "\n",
        "                    # Prepare masked tokens and mask tensor\n",
        "                    masked_tokens = token_indices.clone()\n",
        "                    mask = torch.zeros_like(token_indices, dtype=torch.bool)\n",
        "\n",
        "                    # Apply masking\n",
        "                    for b in range(images.shape[0]):\n",
        "                        positions_to_mask = torch.randperm(256)[:num_masks]\n",
        "                        mask[b, positions_to_mask] = True\n",
        "                        masked_tokens[b, positions_to_mask] = model.mask_token_id\n",
        "\n",
        "                    # Forward pass\n",
        "                    logits = model(masked_tokens)\n",
        "\n",
        "                    # Calculate validation loss\n",
        "                    masked_positions = mask.nonzero(as_tuple=True)\n",
        "                    masked_logits = logits[masked_positions[0], masked_positions[1]]\n",
        "                    masked_targets = token_indices[masked_positions[0], masked_positions[1]]\n",
        "\n",
        "                    # Compute loss\n",
        "                    batch_loss = criterion(masked_logits, masked_targets)\n",
        "                    val_loss += batch_loss.item()\n",
        "\n",
        "            # Calculate average validation loss\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            wandb.log({\"val_loss\": avg_val_loss})\n",
        "\n",
        "            # Save best model\n",
        "            if avg_val_loss < best_loss:\n",
        "                best_loss = avg_val_loss\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # Save checkpoint\n",
        "                checkpoint_path = os.path.join(save_dir, f\"best_model.pth\")\n",
        "                torch.save({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": avg_val_loss,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "                wandb.save(checkpoint_path)\n",
        "                print(f\"New best model saved at epoch {epoch+1} with validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Generate samples with conditioning (requirement G)\n",
        "                if condition_indices is not None and condition_values is not None:\n",
        "                    generate_conditioned_samples(model, condition_indices, condition_values, device)\n",
        "        else:\n",
        "            # If no validation set, save based on training loss\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # Save checkpoint\n",
        "                checkpoint_path = os.path.join(save_dir, f\"best_model.pth\")\n",
        "                torch.save({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": avg_loss,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "                wandb.save(checkpoint_path)\n",
        "                print(f\"New best model saved at epoch {epoch+1} with training loss: {avg_loss:.4f}\")\n",
        "\n",
        "                # Generate samples with conditioning (requirement G)\n",
        "                if condition_indices is not None and condition_values is not None:\n",
        "                    generate_conditioned_samples(model, condition_indices, condition_values, device)\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": avg_loss,\n",
        "            }, checkpoint_path)\n",
        "            wandb.save(checkpoint_path)\n",
        "\n",
        "    # Finish wandb run\n",
        "    wandb.finish()\n",
        "\n",
        "    # Return best model\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_conditioned_samples(model, condition_indices, condition_values, device):\n",
        "    \"\"\"Generate samples with fixed conditioning values at specified indices\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert to tensor\n",
        "    condition_indices = torch.tensor(condition_indices, device=device)\n",
        "    condition_values = torch.tensor(condition_values, device=device)\n",
        "\n",
        "    # Generate multiple samples with the same conditioning\n",
        "    num_samples = 3\n",
        "    samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_samples):\n",
        "            # Start with all masked tokens\n",
        "            tokens = torch.full((1, 256), model.mask_token_id, device=device)\n",
        "\n",
        "            # Set the conditioning values\n",
        "            for idx, val in zip(condition_indices, condition_values):\n",
        "                # Convert flat index to 2D patch index\n",
        "                img_size = 64\n",
        "                patch_size = 4\n",
        "                patches_per_row = img_size // patch_size\n",
        "\n",
        "                pixel_x = idx % img_size\n",
        "                pixel_y = idx // img_size\n",
        "\n",
        "                patch_x = pixel_x // patch_size\n",
        "                patch_y = pixel_y // patch_size\n",
        "\n",
        "                patch_idx = patch_y * patches_per_row + patch_x\n",
        "\n",
        "                # Set the conditioning value in the token\n",
        "                if patch_idx < 256:  # Ensure it's a valid patch index\n",
        "                    # We need to find a token that has the right value at the right position\n",
        "                    within_patch_x = pixel_x % patch_size\n",
        "                    within_patch_y = pixel_y % patch_size\n",
        "                    within_patch_idx = within_patch_y * patch_size + within_patch_x\n",
        "\n",
        "                    # Create a partial mask token or find a matching token\n",
        "                    compatible_tokens = []\n",
        "\n",
        "                    # Check if we have a partial mask token for this position and value\n",
        "                    for partial_idx, token_id in enumerate(model.partial_mask_token_ids):\n",
        "                        if partial_idx // 2 == within_patch_idx and partial_idx % 2 == val.item():\n",
        "                            # Found a matching partial mask token\n",
        "                            tokens[0, patch_idx] = token_id\n",
        "                            break\n",
        "                    else:\n",
        "                        # If no partial mask token, find a token with matching value at position\n",
        "                        for token_id, patch in model.token_to_patch.items():\n",
        "                            if token_id < model.num_observed_tokens and patch[within_patch_idx].item() == val.item():\n",
        "                                compatible_tokens.append(token_id)\n",
        "\n",
        "                        if compatible_tokens:\n",
        "                            # Randomly select a compatible token\n",
        "                            tokens[0, patch_idx] = compatible_tokens[torch.randint(0, len(compatible_tokens), (1,)).item()]\n",
        "\n",
        "            # Now autoregressively generate the rest\n",
        "            image = model.generate_image(batch_size=1, temperature=0.8, device=device)\n",
        "            samples.append(image.cpu())\n",
        "\n",
        "    # Visualize and log the samples\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(samples[i].squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f\"Sample {i+1}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"conditioned_samples\": wandb.Image(plt)})\n",
        "    plt.close()\n",
        "\n",
        "    return samples\n",
        "\n",
        "# Example usage in a notebook:"
      ],
      "metadata": {
        "id": "nNG2It5h7XLu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.load(\"Data/Markov_field/training_data.npz\")[\"arr_0\"]\n",
        "training_data = arr[:9000]\n",
        "test_data = arr[9000:]\n",
        "training_data = training_data.reshape(-1, 64, 64)\n",
        "val_data = test_data.reshape(-1, 64, 64)"
      ],
      "metadata": {
        "id": "lYPDxTDJ7n7A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "feedforward_dim = 512\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 1000\n",
        "\n",
        "# Load data (example)\n",
        "train_images = torch.tensor(training_data, dtype=torch.float32)\n",
        "val_images = torch.tensor(val_data, dtype=torch.float32)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = BinaryImageDataset(train_images)\n",
        "val_dataset = BinaryImageDataset(val_images)\n",
        "\n",
        "# Condition indices and values as specified\n",
        "condition_indices = np.array([876,3825,2122,2892,1556,2683,3667,1767,483,2351,2000,3312,2953,289,2373,2720,872,2713,1206,1341,3541,2226,3423,1904,2882,2540,1497,2524,264,1441])\n",
        "condition_values = np.array([0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,1,0,1,1,0,1])\n",
        "\n",
        "# Train model\n",
        "model = train_vision_transformer(\n",
        "    train_dataset,\n",
        "    val_dataset=None,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    feedforward_dim=feedforward_dim,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    num_epochs=num_epochs,\n",
        "    condition_indices=condition_indices,\n",
        "    condition_values=condition_values\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "f02aa9258027484eba8ae941c9d687a3",
            "4392ce1da1304fb4b481947f1c4edfe9",
            "eec3b9fd794144e5987c900064fac6f2",
            "ba8b29e3e05b4f2795f7a5d6dfa9dc95",
            "c3d070e1c4164e32b8fc83153046afc7",
            "c55e79171b99431a8033d34bc2ce4c04",
            "2fb324f736bb436fbb2e942e2496f06a",
            "d11a1b24bc2d444ea446ddba063ea7da",
            "4c96f375e2494a95ad57e26af5f5ff5e",
            "9241fe190b5841fd81e7b99fef5825d1",
            "ff91dd2d7ad64e318a970afb00574b83",
            "9ba7301a841d4e37bfa697a8049e5211",
            "a059548e81a445248fb88fcbeb8df421",
            "525dcb16c5934d5199f4489595941f24",
            "c131c840e841485a808b1bf2ae3447dc",
            "12b465aa57a742a48c8bf61fbd1a777f",
            "2cf1feae39f14fa49eab43693548fa3f",
            "8a93a11e98dd45c4a01365ac573c3f94",
            "3866e7f8ff884badb674086c995ec8d7",
            "faf4028e255c4dd482187c8086864b04",
            "2056b6345f71469c8afc2104b45865c1",
            "15c6a6e08b884f79ad73a6c06e035067",
            "d5f04d6e0fc64cfeb57fa31ccab5b182",
            "198ee828aa4c4656abf93a1b41b6775b",
            "00b77f1bc92e4a4e94d9bda555844c6f",
            "9fab6075bd74489297ba98b6135505d6",
            "0b0bc1b0d18c4b3c8fca4541103efeb3",
            "5619d93fa6194e5c81723c82c9b8c8bf",
            "c3cb3ed829b44adb984b356a6547cd86",
            "02713b7ff8fe43f6aed5ee47070bf128",
            "e0bcd5f5c1704cfb9ef361654aa5deb0",
            "2134d14a708c421da55f69873c724aab",
            "17899b8df8b74d0aafd477de2f80de48",
            "d0711c0c79d34dda935a3942b3dbda9b",
            "8562f3779a2c4be5b608d85d00116d72"
          ]
        },
        "id": "zZ39GHWc7jtw",
        "outputId": "c2cf0542-3464-46bd-9343-a14557565a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Building vocabulary from training data...\n",
            "Found 30572 unique patches in training data\n",
            "Total token vocabulary size: 30605\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f02aa9258027484eba8ae941c9d687a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 1 with training loss: 7.3368\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ba7301a841d4e37bfa697a8049e5211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 2 with training loss: 6.7637\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5f04d6e0fc64cfeb57fa31ccab5b182",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 3 with training loss: 6.6008\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0711c0c79d34dda935a3942b3dbda9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model saved at epoch 4 with training loss: 6.5381\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8562f3779a2c4be5b608d85d00116d72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/1000:   0%|          | 0/141 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f02aa9258027484eba8ae941c9d687a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4392ce1da1304fb4b481947f1c4edfe9",
              "IPY_MODEL_eec3b9fd794144e5987c900064fac6f2",
              "IPY_MODEL_ba8b29e3e05b4f2795f7a5d6dfa9dc95"
            ],
            "layout": "IPY_MODEL_c3d070e1c4164e32b8fc83153046afc7"
          }
        },
        "4392ce1da1304fb4b481947f1c4edfe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c55e79171b99431a8033d34bc2ce4c04",
            "placeholder": "",
            "style": "IPY_MODEL_2fb324f736bb436fbb2e942e2496f06a",
            "value": "Epoch1/1000:100%"
          }
        },
        "eec3b9fd794144e5987c900064fac6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11a1b24bc2d444ea446ddba063ea7da",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c96f375e2494a95ad57e26af5f5ff5e",
            "value": 141
          }
        },
        "ba8b29e3e05b4f2795f7a5d6dfa9dc95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9241fe190b5841fd81e7b99fef5825d1",
            "placeholder": "",
            "style": "IPY_MODEL_ff91dd2d7ad64e318a970afb00574b83",
            "value": "141/141[06:23&lt;00:00,2.27s/it,loss=7.06,masks=1.0/1]"
          }
        },
        "c3d070e1c4164e32b8fc83153046afc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c55e79171b99431a8033d34bc2ce4c04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb324f736bb436fbb2e942e2496f06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11a1b24bc2d444ea446ddba063ea7da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c96f375e2494a95ad57e26af5f5ff5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9241fe190b5841fd81e7b99fef5825d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff91dd2d7ad64e318a970afb00574b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba7301a841d4e37bfa697a8049e5211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a059548e81a445248fb88fcbeb8df421",
              "IPY_MODEL_525dcb16c5934d5199f4489595941f24",
              "IPY_MODEL_c131c840e841485a808b1bf2ae3447dc"
            ],
            "layout": "IPY_MODEL_12b465aa57a742a48c8bf61fbd1a777f"
          }
        },
        "a059548e81a445248fb88fcbeb8df421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf1feae39f14fa49eab43693548fa3f",
            "placeholder": "",
            "style": "IPY_MODEL_8a93a11e98dd45c4a01365ac573c3f94",
            "value": "Epoch2/1000:100%"
          }
        },
        "525dcb16c5934d5199f4489595941f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3866e7f8ff884badb674086c995ec8d7",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faf4028e255c4dd482187c8086864b04",
            "value": 141
          }
        },
        "c131c840e841485a808b1bf2ae3447dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2056b6345f71469c8afc2104b45865c1",
            "placeholder": "",
            "style": "IPY_MODEL_15c6a6e08b884f79ad73a6c06e035067",
            "value": "141/141[06:23&lt;00:00,2.28s/it,loss=5.94,masks=1.0/1]"
          }
        },
        "12b465aa57a742a48c8bf61fbd1a777f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cf1feae39f14fa49eab43693548fa3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a93a11e98dd45c4a01365ac573c3f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3866e7f8ff884badb674086c995ec8d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faf4028e255c4dd482187c8086864b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2056b6345f71469c8afc2104b45865c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c6a6e08b884f79ad73a6c06e035067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5f04d6e0fc64cfeb57fa31ccab5b182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_198ee828aa4c4656abf93a1b41b6775b",
              "IPY_MODEL_00b77f1bc92e4a4e94d9bda555844c6f",
              "IPY_MODEL_9fab6075bd74489297ba98b6135505d6"
            ],
            "layout": "IPY_MODEL_0b0bc1b0d18c4b3c8fca4541103efeb3"
          }
        },
        "198ee828aa4c4656abf93a1b41b6775b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5619d93fa6194e5c81723c82c9b8c8bf",
            "placeholder": "",
            "style": "IPY_MODEL_c3cb3ed829b44adb984b356a6547cd86",
            "value": "Epoch3/1000:31%"
          }
        },
        "00b77f1bc92e4a4e94d9bda555844c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02713b7ff8fe43f6aed5ee47070bf128",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0bcd5f5c1704cfb9ef361654aa5deb0",
            "value": 44
          }
        },
        "9fab6075bd74489297ba98b6135505d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2134d14a708c421da55f69873c724aab",
            "placeholder": "",
            "style": "IPY_MODEL_17899b8df8b74d0aafd477de2f80de48",
            "value": "44/141[02:00&lt;04:24,2.73s/it,loss=5.51,masks=1.0/1]"
          }
        },
        "0b0bc1b0d18c4b3c8fca4541103efeb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5619d93fa6194e5c81723c82c9b8c8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3cb3ed829b44adb984b356a6547cd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02713b7ff8fe43f6aed5ee47070bf128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0bcd5f5c1704cfb9ef361654aa5deb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2134d14a708c421da55f69873c724aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17899b8df8b74d0aafd477de2f80de48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}